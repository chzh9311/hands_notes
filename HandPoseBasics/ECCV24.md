# ECCVâ€˜24 & CVPR'24 Papers

* <u>**TextDriven**</u>
  
  * NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model
  * **BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics.**
  * **Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction.**
  * AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild

* Weakly-/Semi-supervised
  
  * Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance
  * 3D Reconstruction of Objects in Hands without Real World 3D Supervision

* light-weight / real-time
  
  * MLPHand: Real Time Multi-View 3D Hand Reconstruction via MLP Modeling

* Prediction
  
  * HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning
  
  * Prompting Future Driven Diffusion Model for Hand Motion Prediction

* Image Synthesis
  
  * Controlling the World by Sleight of Hand
  
  * AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild
  
  * **HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data**

* Egocentric
  
  * 3D Hand Pose Estimation in Everyday Egocentric Images
  
  * Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection?
  
  * Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects
  
  * **Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation**

* Blurry Image
  
  * 3D Hand Sequence Recovery from Real Blurry Images and Event Stream

* Representation learning
  
  * Coarse-to-Fine Implicit Representation Learning for 3D Hand-Object Reconstruction from a Single RGB-D Image

* RGB(D) based HO reconstruction
  
  * D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object Reconstruction
  
  * Occlusion Handling in 3D Human Pose Estimation with Perturbed Positional Encoding
  
  * **HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video**
  
  * **Reconstructing Hands in 3D with Transformers**

* Dataset
  
  * Dense Hand-Object (HO) GraspNet with Full Grasping Taxonomy and Dynamics

* <u>**Physical Constraints**</u>
  
  * **Physics-aware Hand-object Interaction Denoising**
  * **MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints**

* Render
  
  * **URHand: Universal Relightable Hands**

* Hand avatar
  
  * **OHTA: One-shot Hand Avatar via Data-driven Implicit Priors.**
  
  * **Authentic Hand Avatar from a Phone Scan via Universal Hand Model.**

* Texture
  
  * **BiTT: Bi-directional Texture Reconstruction of Interacting Two Hands from a Single Image.**

* Event Camera
  
  * **Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction**

* HO Synthesis
  
  * **G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis**
  
  * **Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction.**

* Others
  
  * **HOIST-Former: Hand-held Objects Identification, Segmentation, and Tracking in the Wild**
  * cross-hand grasping
    * Learning Cross-hand Policies of High-DOF Reaching and Grasping
  * action recognition
    * On the Utility of 3D Hand Poses for Action Recognition

# Text Prompt

## Problem

1. The complexity of **cross-modal** modeling from language to contact;

2. A lack of descriptive text for contact patterns

## Egocentric


