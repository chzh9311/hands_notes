---
tags:
  - HandPose
  - DomainAdapt
  - weakly-supervised
Institute:
  - NUS
Corresponding Author:
  - Angela Yao
Year: 2023
Publisher: CVPR
aliases:
  - "@linCrossDomain3DHand2023"
---
## Motivation
* Learning from labelled synthetic data and unlabelled real world data.
* Only RGB synthetic data was considered.
## Method
Input: sync data with RGB image, depth map, and segmentation mask.
### Pre-training with synthetic data
1. Intra-modal contrastive learning: for discriminative feature space in each modalities;
2. Inter-modal contrastive learning: for RGB & depth alignment.
RGB & depth features are processed using the same decoder.
$$L_s = M(\mathbf{y}_R, \mathbf{y}_{gt}) + M(\mathbf{y}_D, \mathbf{y}_{gt}) + M(\mathbf{y}_F, \mathbf{y}_{gt})$$
The three features are: $R$: RGB features; $D$: depth features; $F$ attention-fused features of both RGB and depth.
![[Pasted image 20241119172255.png]]
### Fine-tune with pseudo-labels
![[Pasted image 20241119172521.png]]
Pseudo labels are generated by average of R and F before & after correction.
Use self-distillation between $y_R$ and $y_F$ for better precision.

## Experiments
Datasets: Sync: [[RHD]], Real: [[STB]], [[FreiHand (Zimmermann et al., 2019)]], [[H3D]] & [[MVHand]]
The [[RHD]] is used for pretraining, and the rest are used for testing