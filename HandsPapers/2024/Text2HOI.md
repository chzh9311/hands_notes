---
tags:
  - textDriven
  - Hand-Object
Institute: UNIST, KETI, Adobe
Corresponding Author: Seungryul Baek
Code: https://github.com/JunukCha/Text2HOI
Year: "2024"
Publisher: CVPR
---
# Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction
## Motivation
Text provides the necessary context for HOI generation.
Challenge: lack of data
â†’ Object contact map generation & hand-object motion generation.
![[Text2HOI.png]]
## Method
### Contact map generation

1. compute the radius of smallest bounding sphere of the object mesh $s_{obj}$. The context map is represented by binary values at N sampled points.
2. Supervised by Cross-entropy, dice loss, and KL divergence.
### Motion is generated by a diffusion model
$$x_0 = f^{THOI}(x_t, t, c)$$
The condition $c$ includes the text features, contact map, object scale, and object features.
### Hand Refinement
Considers the **Contact** and **Penetration**.

## Experiments
Use [[H2O]], [[GRAB]], & [[ARCTIC (Fan et al., 2023)]], which collects hand-object mesh seq.
text prompts are generated by action labels for the former two, but ARCTIC is labelled manually.
Metrics are:
* Frechet inception distance (Measures the difference between generated images and real images.)
* Diversity
* multi-modality as in [[IMOS]]
