[
  {"id":"kendallWhatUncertaintiesWe2017","abstract":"There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.","accessed":{"date-parts":[["2024",10,15]]},"author":[{"family":"Kendall","given":"Alex"},{"family":"Gal","given":"Yarin"}],"citation-key":"kendallWhatUncertaintiesWe2017","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2017"]]},"publisher":"Curran Associates, Inc.","source":"Neural Information Processing Systems","title":"What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?","type":"paper-conference","URL":"https://proceedings.neurips.cc/paper/2017/hash/2650d6089a6d640c5e85b2b88265dc2b-Abstract.html","volume":"30"},
  {"id":"liuSwinTransformerHierarchical2021","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Liu","given":"Ze"},{"family":"Lin","given":"Yutong"},{"family":"Cao","given":"Yue"},{"family":"Hu","given":"Han"},{"family":"Wei","given":"Yixuan"},{"family":"Zhang","given":"Zheng"},{"family":"Lin","given":"Stephen"},{"family":"Guo","given":"Baining"}],"citation-key":"liuSwinTransformerHierarchical2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"10012-10022","source":"openaccess.thecvf.com","title":"Swin Transformer: Hierarchical Vision Transformer Using Shifted Windows","title-short":"Swin Transformer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Liu_Swin_Transformer_Hierarchical_Vision_Transformer_Using_Shifted_Windows_ICCV_2021_paper"},
  {"id":"lorensenMarchingCubesHigh1987","abstract":"We present a new algorithm, called marching cubes, that creates triangle models of constant density surfaces from 3D medical data. Using a divide-and-conquer approach to generate inter-slice connectivity, we create a case table that defines triangle topology. The algorithm processes the 3D medical data in scan-line order and calculates triangle vertices using linear interpolation. We find the gradient of the original data, normalize it, and use it as a basis for shading the models. The detail in images produced from the generated surface models is the result of maintaining the inter-slice connectivity, surface data, and gradient information present in the original 3D data. Results from computed tomography (CT), magnetic resonance (MR), and single-photon emission computed tomography (SPECT) illustrate the quality and functionality of marching cubes. We also discuss improvements that decrease processing time and add solid modeling capabilities.","accessed":{"date-parts":[["2024",11,21]]},"author":[{"family":"Lorensen","given":"William E."},{"family":"Cline","given":"Harvey E."}],"citation-key":"lorensenMarchingCubesHigh1987","container-title":"SIGGRAPH Comput. Graph.","DOI":"10.1145/37402.37422","ISSN":"0097-8930","issue":"4","issued":{"date-parts":[["1987",8,1]]},"page":"163â€“169","source":"ACM Digital Library","title":"Marching cubes: A high resolution 3D surface construction algorithm","title-short":"Marching cubes","type":"article-journal","URL":"https://dl.acm.org/doi/10.1145/37402.37422","volume":"21"},
  {"id":"prakashMitigatingPerspectiveDistortioninduced2024","abstract":"Objects undergo varying amounts of perspective distortion as they move across a camera's field of view. Models for predicting 3D from a single image often work with crops around the object of interest and ignore the location of the object in the camera's field of view. We note that ignoring this location information further exaggerates the inherent ambiguity in making 3D inferences from 2D images and can prevent models from even fitting to the training data. To mitigate this ambiguity, we propose Intrinsics-Aware Positional Encoding (KPE), which incorporates information about the location of crops in the image and camera intrinsics. Experiments on three popular 3D-from-a-single-image benchmarks: depth prediction on NYU, 3D object detection on KITTI & nuScenes, and predicting 3D shapes of articulated objects on ARCTIC, show the benefits of KPE.","accessed":{"date-parts":[["2024",10,4]]},"author":[{"family":"Prakash","given":"Aditya"},{"family":"Gupta","given":"Arjun"},{"family":"Gupta","given":"Saurabh"}],"citation-key":"prakashMitigatingPerspectiveDistortioninduced2024","DOI":"10.48550/arXiv.2312.06594","issued":{"date-parts":[["2024",9,23]]},"number":"arXiv:2312.06594","publisher":"arXiv","source":"arXiv.org","title":"Mitigating Perspective Distortion-induced Shape Ambiguity in Image Crops","type":"article","URL":"http://arxiv.org/abs/2312.06594"}
]
