[
  {"id":"agarwalOptimizationBasedFramework2012","abstract":"Human pose estimation using monocular vision is a challenging problem in computer vision. Past work has focused on developing efficient inference algorithms and probabilistic prior models based on captured kinematic/dynamic measurements. However, such algorithms face challenges in generalization beyond the learned dataset.In this work, we propose a model-based generative approach for estimating the human pose solely from uncalibrated monocular video in unconstrained environments without any prior learning on motion capture/image annotation data. We propose a novel Product of Heading Experts (PoHE) based generalized heading estimation framework by probabilistically-merging heading outputs (probabilistic/ non-probabilistic) from time varying number of estimators to bootstrap a synergistically integrated probabilistic-deterministic sequential optimization framework for robustly estimating human pose. Novel pixel-distance based performance measures are developed to penalize false human detections and ensure identity-maintained human tracking. We tested our framework with varied inputs (silhouette and bounding boxes) to evaluate, compare and benchmark it against ground-truth data (collected using our human annotation tool) for 52 video vignettes in the publicly available DARPA Mind’s Eye Year I dataset. Results show robust pose estimates on this challenging dataset of highly diverse activities.","author":[{"family":"Agarwal","given":"Priyanshu"},{"family":"Kumar","given":"Suren"},{"family":"Ryde","given":"Julian"},{"family":"Corso","given":"Jason J."},{"family":"Krovi","given":"Venkat N."}],"citation-key":"agarwalOptimizationBasedFramework2012","collection-title":"Lecture Notes in Computer Science","container-title":"Advances in Visual Computing","DOI":"10.1007/978-3-642-33179-4_55","editor":[{"family":"Bebis","given":"George"},{"family":"Boyle","given":"Richard"},{"family":"Parvin","given":"Bahram"},{"family":"Koracin","given":"Darko"},{"family":"Fowlkes","given":"Charless"},{"family":"Wang","given":"Sen"},{"family":"Choi","given":"Min-Hyung"},{"family":"Mantler","given":"Stephan"},{"family":"Schulze","given":"Jürgen"},{"family":"Acevedo","given":"Daniel"},{"family":"Mueller","given":"Klaus"},{"family":"Papka","given":"Michael"}],"event-place":"Berlin, Heidelberg","ISBN":"978-3-642-33179-4","issued":{"date-parts":[["2012"]]},"language":"en","page":"575-586","publisher":"Springer","publisher-place":"Berlin, Heidelberg","source":"Springer Link","title":"An Optimization Based Framework for Human Pose Estimation in Monocular Videos","type":"paper-conference"},
  {"id":"agarwalRecovering3DHuman2006","abstract":"We describe a learning-based method for recovering 3D human body pose from single images and monocular image sequences. Our approach requires neither an explicit body model nor prior labeling of body parts in the image. Instead, it recovers pose by direct nonlinear regression against shape descriptor vectors extracted automatically from image silhouettes. For robustness against local silhouette segmentation errors, silhouette shape is encoded by histogram-of-shape-contexts descriptors. We evaluate several different regression methods: ridge regression, Relevance Vector Machine (RVM) regression, and Support Vector Machine (SVM) regression over both linear and kernel bases. The RVMs provide much sparser regressors without compromising performance, and kernel bases give a small but worthwhile improvement in performance. The loss of depth and limb labeling information often makes the recovery of 3D pose from single silhouettes ambiguous. To handle this, the method is embedded in a novel regressive tracking framework, using dynamics from the previous state estimate together with a learned regression value to disambiguate the pose. We show that the resulting system tracks long sequences stably. For realism and good generalization over a wide range of viewpoints, we train the regressors on images resynthesized from real human motion capture data. The method is demonstrated for several representations of full body pose, both quantitatively on independent but similar test data and qualitatively on real image sequences. Mean angular errors of 4-6  are obtained for a variety of walking motions.","accessed":{"date-parts":[["2023",3,13]]},"author":[{"family":"Agarwal","given":"A."},{"family":"Triggs","given":"B."}],"citation-key":"agarwalRecovering3DHuman2006","container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","container-title-short":"IEEE Trans. Pattern Anal. Machine Intell.","DOI":"10.1109/TPAMI.2006.21","ISSN":"0162-8828","issue":"1","issued":{"date-parts":[["2006",1]]},"language":"en","page":"44-58","source":"DOI.org (Crossref)","title":"Recovering 3D human pose from monocular images","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/1542030/","volume":"28"},
  {"id":"akhterTrajectorySpaceDual2011","abstract":"Existing approaches to nonrigid structure from motion assume that the instantaneous 3D shape of a deforming object is a linear combination of basis shapes. These bases are object dependent and therefore have to be estimated anew for each video sequence. In contrast, we propose a dual approach to describe the evolving 3D structure in trajectory space by a linear combination of basis trajectories. We describe the dual relationship between the two approaches, showing that they both have equal power for representing 3D structure. We further show that the temporal smoothness in 3D trajectories alone can be used for recovering nonrigid structure from a moving camera. The principal advantage of expressing deforming 3D structure in trajectory space is that we can define an object independent basis. This results in a significant reduction in unknowns and corresponding stability in estimation. We propose the use of the Discrete Cosine Transform (DCT) as the object independent basis and empirically demonstrate that it approaches Principal Component Analysis (PCA) for natural motions. We report the performance of the proposed method, quantitatively using motion capture data, and qualitatively on several video sequences exhibiting nonrigid motions, including piecewise rigid motion, partially nonrigid motion (such as a facial expressions), and highly nonrigid motion (such as a person walking or dancing).","author":[{"family":"Akhter","given":"Ijaz"},{"family":"Sheikh","given":"Yaser"},{"family":"Khan","given":"Sohaib"},{"family":"Kanade","given":"Takeo"}],"citation-key":"akhterTrajectorySpaceDual2011","container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","DOI":"10.1109/TPAMI.2010.201","ISSN":"1939-3539","issue":"7","issued":{"date-parts":[["2011",7]]},"page":"1442-1456","source":"IEEE Xplore","title":"Trajectory Space: A Dual Representation for Nonrigid Structure from Motion","title-short":"Trajectory Space","type":"article-journal","volume":"33"},
  {"id":"amin3DPictorialStructures2014","abstract":"In this work, we address the problem of 3D pose estimation of multiple humans from multiple views. This is a more challenging problem than single human 3D pose estimation due to the much larger state space, partial occlusions as well as across view ambiguities when not knowing the identity of the humans in advance. To address these problems, we ﬁrst create a reduced state space by triangulation of corresponding body joints obtained from part detectors in pairs of camera views. In order to resolve the ambiguities of wrong and mixed body parts of multiple humans after triangulation and also those coming from false positive body part detections, we introduce a novel 3D pictorial structures (3DPS) model. Our model infers 3D human body conﬁgurations from our reduced state space. The 3DPS model is generic and applicable to both single and multiple human pose estimation.","accessed":{"date-parts":[["2021",1,18]]},"author":[{"family":"Amin","given":"Sikandar"},{"family":"Andriluka","given":"Mykhaylo"},{"family":"Schiele","given":"Bernt"},{"family":"Navab","given":"Nassir"},{"family":"Ilic","given":"Slobodan"}],"citation-key":"amin3DPictorialStructures2014","container-title":"2014 IEEE Conference on Computer Vision and Pattern Recognition","DOI":"10.1109/CVPR.2014.216","event-place":"Columbus, OH, USA","event-title":"2014 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-4799-5118-5","issued":{"date-parts":[["2014",6]]},"language":"en","page":"1669-1676","publisher":"IEEE","publisher-place":"Columbus, OH, USA","source":"DOI.org (Crossref)","title":"3D Pictorial Structures for Multiple Human Pose Estimation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/6909612"},
  {"id":"andrilukaDiscriminativeAppearanceModels2012","abstract":"In this paper we consider people detection and articulated pose estimation, two closely related and challenging problems in computer vision. Conceptually, both of these problems can be addressed within the pictorial structures framework (Felzenszwalb and Huttenlocher in Int. J. Comput. Vis. 61(1):55–79, 2005; Fischler and Elschlager in IEEE Trans. Comput. C-22(1):67–92, 1973), even though previous approaches have not shown such generality. A principal difficulty for such a general approach is to model the appearance of body parts. The model has to be discriminative enough to enable reliable detection in cluttered scenes and general enough to capture highly variable appearance. Therefore, as the first important component of our approach, we propose a discriminative appearance model based on densely sampled local descriptors and AdaBoost classifiers. Secondly, we interpret the normalized margin of each classifier as likelihood in a generative model and compute marginal posteriors for each part using belief propagation. Thirdly, non-Gaussian relationships between parts are represented as Gaussians in the coordinate system of the joint between the parts. Additionally, in order to cope with shortcomings of tree-based pictorial structures models, we augment our model with additional repulsive factors in order to discourage overcounting of image evidence. We demonstrate that the combination of these components within the pictorial structures framework results in a generic model that yields state-of-the-art performance for several datasets on a variety of tasks: people detection, upper body pose estimation, and full body pose estimation.","accessed":{"date-parts":[["2021",3,27]]},"author":[{"family":"Andriluka","given":"Mykhaylo"},{"family":"Roth","given":"Stefan"},{"family":"Schiele","given":"Bernt"}],"citation-key":"andrilukaDiscriminativeAppearanceModels2012","container-title":"International Journal of Computer Vision","container-title-short":"Int J Comput Vis","DOI":"10.1007/s11263-011-0498-z","ISSN":"1573-1405","issue":"3","issued":{"date-parts":[["2012",9,1]]},"language":"en","page":"259-280","source":"Springer Link","title":"Discriminative Appearance Models for Pictorial Structures","type":"article-journal","URL":"https://doi.org/10.1007/s11263-011-0498-z","volume":"99"},
  {"id":"andrilukaMPII2014","abstract":"Human pose estimation has made significant progress during the last years. However current datasets are limited in their coverage of the overall pose estimation challenges. Still these serve as the common sources to evaluate, train and compare different models on. In this paper we introduce a novel benchmark \"MPII Human Pose\" that makes a significant advance in terms of diversity and difficulty, a contribution that we feel is required for future developments in human body models. This comprehensive dataset was collected using an established taxonomy of over 800 human activities [1]. The collected images cover a wider variety of human activities than previous datasets including various recreational, occupational and householding activities, and capture people from a wider range of viewpoints. We provide a rich set of labels including positions of body joints, full 3D torso and head orientation, occlusion labels for joints and body parts, and activity labels. For each image we provide adjacent video frames to facilitate the use of motion information. Given these rich annotations we perform a detailed analysis of leading human pose estimation approaches and gaining insights for the success and failures of these methods.","author":[{"family":"Andriluka","given":"Mykhaylo"},{"family":"Pishchulin","given":"Leonid"},{"family":"Gehler","given":"Peter"},{"family":"Schiele","given":"Bernt"}],"citation-key":"andrilukaMPII2014","container-title":"2014 IEEE Conference on Computer Vision and Pattern Recognition","DOI":"10.1109/CVPR.2014.471","event-title":"2014 IEEE Conference on Computer Vision and Pattern Recognition","ISSN":"1063-6919","issued":{"date-parts":[["2014",6]]},"page":"3686-3693","source":"IEEE Xplore","title":"MPII","title-short":"2D Human Pose Estimation","type":"paper-conference"},
  {"id":"AppliedSciencesFree","accessed":{"date-parts":[["2023",9,23]]},"citation-key":"AppliedSciencesFree","title":"Applied Sciences | Free Full-Text | A Comprehensive Study on Deep Learning-Based 3D Hand Pose Estimation Methods","type":"webpage","URL":"https://www.mdpi.com/2076-3417/10/19/6850"},
  {"id":"arnabExploitingTemporalContext2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Arnab","given":"Anurag"},{"family":"Doersch","given":"Carl"},{"family":"Zisserman","given":"Andrew"}],"citation-key":"arnabExploitingTemporalContext2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"3395-3404","source":"openaccess.thecvf.com","title":"Exploiting Temporal Context for 3D Human Pose Estimation in the Wild","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Arnab_Exploiting_Temporal_Context_for_3D_Human_Pose_Estimation_in_the_CVPR_2019_paper.html"},
  {"id":"artachoUniPoseUnifiedHuman2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Artacho","given":"Bruno"},{"family":"Savakis","given":"Andreas"}],"citation-key":"artachoUniPoseUnifiedHuman2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"7035-7044","source":"openaccess.thecvf.com","title":"UniPose: Unified Human Pose Estimation in Single Images and Videos","title-short":"UniPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Artacho_UniPose_Unified_Human_Pose_Estimation_in_Single_Images_and_Videos_CVPR_2020_paper.html"},
  {"id":"azizCoverageStrategiesWireless2009","abstract":"Coverage is one of the main research interests in wireless sensor networks (WSN), it is used to determine the quality of service (QoS) of the networks. Therefore this paper aims to review the common strategies use in solving coverage problem in WSN. The strategies studied are used during deployment phase where the coverage is calculated based on the placement of the sensors on the region of interest (ROI). The strategies reviewed are categorized into three groups based on the approaches used, namely; force based, grid based or computational geometry based approach.","author":[{"family":"Aziz","given":"Nor Azlina Ab"},{"family":"Aziz","given":"Kamarulzaman Ab"},{"family":"Ismail","given":"Wan Zakiah Wan"}],"citation-key":"azizCoverageStrategiesWireless2009","issued":{"date-parts":[["2009"]]},"language":"en","page":"6","source":"Zotero","title":"Coverage Strategies for Wireless Sensor Networks","type":"article-journal"},
  {"id":"azizi3DHumanPose2022","abstract":"3D human pose estimation is fundamental to understanding human behavior. Recently, promising results have been achieved by graph convolutional networks (GCNs), which achieve state-of-the-art performance and provide rather light-weight architectures. However, a major limitation of GCNs is their inability to encode all the transformations between joints explicitly. To address this issue, we propose a novel spectral GCN using the M\\\"obius transformation (M\\\"obiusGCN). In particular, this allows us to directly and explicitly encode the transformation between joints, resulting in a significantly more compact representation. Compared to even the lightest architectures so far, our novel approach requires 90-98% fewer parameters, i.e. our lightest M\\\"obiusGCN uses only 0.042M trainable parameters. Besides the drastic parameter reduction, explicitly encoding the transformation of joints also enables us to achieve state-of-the-art results. We evaluate our approach on the two challenging pose estimation benchmarks, Human3.6M and MPI-INF-3DHP, demonstrating both state-of-the-art results and the generalization capabilities of M\\\"obiusGCN.","accessed":{"date-parts":[["2023",3,14]]},"author":[{"family":"Azizi","given":"Niloofar"},{"family":"Possegger","given":"Horst"},{"family":"Rodolà","given":"Emanuele"},{"family":"Bischof","given":"Horst"}],"citation-key":"azizi3DHumanPose2022","DOI":"10.48550/arXiv.2203.10554","issued":{"date-parts":[["2022",3,20]]},"number":"arXiv:2203.10554","publisher":"arXiv","source":"arXiv.org","title":"3D Human Pose Estimation Using M\\\"obius Graph Convolutional Networks","type":"article","URL":"http://arxiv.org/abs/2203.10554"},
  {"id":"azizi3DHumanPose2022a","abstract":"3D human pose estimation is fundamental to understanding human behavior. Recently, promising results have been achieved by graph convolutional networks (GCNs), which achieve state-of-the-art performance and provide rather light-weight architectures. However, a major limitation of GCNs is their inability to encode all the transformations between joints explicitly. To address this issue, we propose a novel spectral GCN using the M\\\"obius transformation (M\\\"obiusGCN). In particular, this allows us to directly and explicitly encode the transformation between joints, resulting in a significantly more compact representation. Compared to even the lightest architectures so far, our novel approach requires 90-98% fewer parameters, i.e. our lightest M\\\"obiusGCN uses only 0.042M trainable parameters. Besides the drastic parameter reduction, explicitly encoding the transformation of joints also enables us to achieve state-of-the-art results. We evaluate our approach on the two challenging pose estimation benchmarks, Human3.6M and MPI-INF-3DHP, demonstrating both state-of-the-art results and the generalization capabilities of M\\\"obiusGCN.","accessed":{"date-parts":[["2022",11,23]]},"author":[{"family":"Azizi","given":"Niloofar"},{"family":"Possegger","given":"Horst"},{"family":"Rodolà","given":"Emanuele"},{"family":"Bischof","given":"Horst"}],"citation-key":"azizi3DHumanPose2022a","DOI":"10.48550/arXiv.2203.10554","issued":{"date-parts":[["2022",3,20]]},"number":"arXiv:2203.10554","publisher":"arXiv","source":"arXiv.org","title":"3D Human Pose Estimation Using M\\\"obius Graph Convolutional Networks","type":"article","URL":"http://arxiv.org/abs/2203.10554"},
  {"id":"baeMultiViewDepthEstimation2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Bae","given":"Gwangbin"},{"family":"Budvytis","given":"Ignas"},{"family":"Cipolla","given":"Roberto"}],"citation-key":"baeMultiViewDepthEstimation2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"2842-2851","source":"openaccess.thecvf.com","title":"Multi-View Depth Estimation by Fusing Single-View Depth Probability With Multi-View Geometry","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Bae_Multi-View_Depth_Estimation_by_Fusing_Single-View_Depth_Probability_With_Multi-View_CVPR_2022_paper.html"},
  {"id":"baoFusePoseIMUVisionSensor2022","abstract":"There exist challenging problems in 3D human pose estimation mission, such as poor performance caused by occlusion and self-occlusion. Recently, IMU-vision sensor fusion is regarded as valuable for solving these problems. However, previous researches on the fusion of IMU and vision data, which is heterogeneous, fail to adequately utilize either IMU raw data or reliable high-level vision features. To facilitate a more efficient sensor fusion, in this work we propose a framework called FusePose under a parametric human kinematic model. Specifically, we aggregate different information of IMU or vision data and introduce three distinctive sensor fusion approaches: NaiveFuse, KineFuse and AdaDeepFuse. NaiveFuse servers as a basic approach that only fuses simplified IMU data and estimated 3D pose in euclidean space. While in kinematic space, KineFuse is able to integrate the calibrated and aligned IMU raw data with converted 3D pose parameters. AdaDeepFuse further develops this kinematical fusion process to an adaptive and end-to-end trainable manner. Comprehensive experiments with ablation studies demonstrate the rationality and superiority of the proposed framework. The performance of 3D human pose estimation is improved compared to the baseline result. On Total Capture dataset, KineFuse surpasses previous state-of-the-art which uses IMU only for testing by 8.6%. AdaDeepFuse surpasses state-of-the-art which uses IMU for both training and testing by 8.5%. Moreover, we validate the generalization capability of our framework through experiments on Human3.6M dataset.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Bao","given":"Yiming"},{"family":"Zhao","given":"Xu"},{"family":"Qian","given":"Dahong"}],"citation-key":"baoFusePoseIMUVisionSensor2022","container-title":"IEEE Transactions on Multimedia","container-title-short":"IEEE Trans. Multimedia","DOI":"10.1109/TMM.2022.3227472","ISSN":"1520-9210, 1941-0077","issued":{"date-parts":[["2022"]]},"language":"en","note":"ZSCC:00000","page":"1-12","source":"DOI.org (Crossref)","title":"FusePose: IMU-Vision Sensor Fusion in Kinematic Space for Parametric Human Pose Estimation","title-short":"FusePose","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9973327/"},
  {"id":"bartolGeneralizableHumanPose2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Bartol","given":"Kristijan"},{"family":"Bojanić","given":"David"},{"family":"Petković","given":"Tomislav"},{"family":"Pribanić","given":"Tomislav"}],"citation-key":"bartolGeneralizableHumanPose2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"11028-11037","source":"openaccess.thecvf.com","title":"Generalizable Human Pose Triangulation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Bartol_Generalizable_Human_Pose_Triangulation_CVPR_2022_paper.html"},
  {"id":"belagiannisRobustOptimizationDeep2015","abstract":"Convolutional Neural Networks (ConvNets) have successfully contributed to improve the accuracy of regression-based methods for computer vision tasks such as human pose estimation, landmark localization, and object detection. The network optimization has been usually performed with L2 loss and without considering the impact of outliers on the training process, where an outlier in this context is defined by a sample estimation that lies at an abnormal distance from the other training sample estimations in the objective space. In this work, we propose a regression model with ConvNets that achieves robustness to such outliers by minimizing Tukey's biweight function, an M-estimator robust to outliers, as the loss function for the ConvNet. In addition to the robust loss, we introduce a coarse-to-fine model, which processes input images of progressively higher resolutions for improving the accuracy of the regressed values. In our experiments, we demonstrate faster convergence and better generalization of our robust loss function for the tasks of human pose estimation and age estimation from face images. We also show that the combination of the robust loss function with the coarse-to-fine model produces comparable or better results than current state-of-the-art approaches in four publicly available human pose estimation datasets.","author":[{"family":"Belagiannis","given":"Vasileios"},{"family":"Rupprecht","given":"Christian"},{"family":"Carneiro","given":"Gustavo"},{"family":"Navab","given":"Nassir"}],"citation-key":"belagiannisRobustOptimizationDeep2015","container-title":"2015 IEEE International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV.2015.324","event-title":"2015 IEEE International Conference on Computer Vision (ICCV)","ISSN":"2380-7504","issued":{"date-parts":[["2015",12]]},"page":"2830-2838","source":"IEEE Xplore","title":"Robust Optimization for Deep Regression","type":"paper-conference"},
  {"id":"benzinePandaNetAnchorBasedSingleShot2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Benzine","given":"Abdallah"},{"family":"Chabot","given":"Florian"},{"family":"Luvison","given":"Bertrand"},{"family":"Pham","given":"Quoc Cuong"},{"family":"Achard","given":"Catherine"}],"citation-key":"benzinePandaNetAnchorBasedSingleShot2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"6856-6865","source":"openaccess.thecvf.com","title":"PandaNet: Anchor-Based Single-Shot Multi-Person 3D Pose Estimation","title-short":"PandaNet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Benzine_PandaNet_Anchor-Based_Single-Shot_Multi-Person_3D_Pose_Estimation_CVPR_2020_paper.html"},
  {"id":"benzinePandaNetAnchorBasedSingleShot2020a","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Benzine","given":"Abdallah"},{"family":"Chabot","given":"Florian"},{"family":"Luvison","given":"Bertrand"},{"family":"Pham","given":"Quoc Cuong"},{"family":"Achard","given":"Catherine"}],"citation-key":"benzinePandaNetAnchorBasedSingleShot2020a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"6856-6865","source":"openaccess.thecvf.com","title":"PandaNet: Anchor-Based Single-Shot Multi-Person 3D Pose Estimation","title-short":"PandaNet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Benzine_PandaNet_Anchor-Based_Single-Shot_Multi-Person_3D_Pose_Estimation_CVPR_2020_paper.html"},
  {"id":"bianParallelizedAnnealedParticle2012","abstract":"We propose a parallelized Annealed Particle Filter method via heterogeneous computing (P-APF), to implement real-time marker-less motion tracking based on OpenCL framework. The overall computing procedure in P-APF is decomposed into several computational tasks with corresponding granularity. According to the degree of parallelism, the tasks are assigned to standard and attached processors respectively, to fully leverage heterogeneous computing ability. A task latency hidden strategy is used to further reduce time cost. Experiments on different human motion datasets demonstrate that PAPF can achieve real-time tracking performance without losing accuracy. With an average acceleration ratio of 106 compared to serial implementation, the time cost basically remains constant with the growth of particle number and view number in a limited range.","author":[{"family":"Bian","given":"Yatao"},{"family":"Zhao","given":"Xu"},{"family":"Song","given":"Jian"},{"family":"Liu","given":"Yuncai"}],"citation-key":"bianParallelizedAnnealedParticle2012","container-title":"ICPR","issued":{"date-parts":[["2012"]]},"language":"en","note":"ZSCC:00002","source":"Zotero","title":"Parallelized Annealed Particle Filter for Real-Time Marker-Less Motion Tracking Via Heterogeneous Computing","type":"paper-conference"},
  {"id":"bogoKeepItSMPL2016","abstract":"We describe the first method to automatically estimate the 3D pose of the human body as well as its 3D shape from a single unconstrained image. We estimate a full 3D mesh and show that 2D joints alone carry a surprising amount of information about body shape. The problem is challenging because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D. To solve this, we first use a recently published CNN-based method, DeepCut, to predict (bottom-up) the 2D body joint locations. We then fit (top-down) a recently published statistical body shape model, called SMPL, to the 2D joints. We do so by minimizing an objective function that penalizes the error between the projected 3D model joints and detected 2D joints. Because SMPL captures correlations in human shape across the population, we are able to robustly fit it to very little data. We further leverage the 3D model to prevent solutions that cause interpenetration. We evaluate our method, SMPLify, on the Leeds Sports, HumanEva, and Human3.6M datasets, showing superior pose accuracy with respect to the state of the art.","author":[{"family":"Bogo","given":"Federica"},{"family":"Kanazawa","given":"Angjoo"},{"family":"Lassner","given":"Christoph"},{"family":"Gehler","given":"Peter"},{"family":"Romero","given":"Javier"},{"family":"Black","given":"Michael J."}],"citation-key":"bogoKeepItSMPL2016","collection-title":"Lecture Notes in Computer Science","container-title":"Computer Vision – ECCV 2016","DOI":"10.1007/978-3-319-46454-1_34","editor":[{"family":"Leibe","given":"Bastian"},{"family":"Matas","given":"Jiri"},{"family":"Sebe","given":"Nicu"},{"family":"Welling","given":"Max"}],"event-place":"Cham","ISBN":"978-3-319-46454-1","issued":{"date-parts":[["2016"]]},"language":"en","page":"561-578","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"Keep It SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image","title-short":"Keep It SMPL","type":"paper-conference"},
  {"id":"boStructuredNonconvexNonsmooth2019","abstract":"Nonconvex optimization problems are frequently encountered in much of statistics, business, science and engineering, but they are not yet widely recognized as a technology. A reason for this relatively low degree of popularity is the lack of a well developed system of theory and algorithms to support the applications, as is the case for its convex counterpart. This paper aims to take one step in the direction of disciplined nonconvex optimization. In particular, we consider in this paper some constrained nonconvex optimization models in block decision variables, with or without coupled affine constraints. In the case of no coupled constraints, we show a sublinear rate of convergence to an $\\epsilon$-stationary solution in the form of variational inequality for a generalized conditional gradient method, where the convergence rate is shown to be dependent on the H\\\"olderian continuity of the gradient of the smooth part of the objective. For the model with coupled affine constraints, we introduce corresponding $\\epsilon$-stationarity conditions, and propose two proximal-type variants of the ADMM to solve such a model, assuming the proximal ADMM updates can be implemented for all the block variables except for the last block, for which either a gradient step or a majorization-minimization step is implemented. We show an iteration complexity bound of $O(1/\\epsilon^2)$ to reach an $\\epsilon$-stationary solution for both algorithms. Moreover, we show that the same iteration complexity of a proximal BCD method follows immediately. Numerical results are provided to illustrate the efficacy of the proposed algorithms for tensor robust PCA.","accessed":{"date-parts":[["2021",9,2]]},"author":[{"family":"Bo","given":"J."},{"family":"Lin","given":"T."},{"family":"Ma","given":"S."},{"family":"Zhang","given":"S."}],"citation-key":"boStructuredNonconvexNonsmooth2019","container-title":"Computational Optimization and Applications","DOI":"10.1007/s10589-018-0034-y","issue":"3","issued":{"date-parts":[["2019"]]},"source":"Baidu Scholar","title":"Structured Nonconvex and Nonsmooth Optimization: Algorithms and Iteration Complexity Analysis","title-short":"Structured Nonconvex and Nonsmooth Optimization","type":"article-journal","URL":"http://link.springer.com/article/10.1007/s10589-018-0034-y","volume":"72"},
  {"id":"brasoCenterAttentionCenterKeypoint2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Brasó","given":"Guillem"},{"family":"Kister","given":"Nikita"},{"family":"Leal-Taixé","given":"Laura"}],"citation-key":"brasoCenterAttentionCenterKeypoint2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11853-11863","source":"openaccess.thecvf.com","title":"The Center of Attention: Center-Keypoint Grouping via Attention for Multi-Person Pose Estimation","title-short":"The Center of Attention","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Braso_The_Center_of_Attention_Center-Keypoint_Grouping_via_Attention_for_Multi-Person_ICCV_2021_paper.html"},
  {"id":"brauerParticleSwarmOptimization2013","abstract":"Automatic assessment of situations with modern security and surveillance systems requires sophisticated discrimination capabilities. Therefore, action recognition, e.g. in terms of person-person or person-object interactions, is an essential core component of any surveillance system. A subclass of recent action recognition approaches are based on space time volumes, which are generated from trajectories of multiple anatomical landmarks like hands or shoulders. A general prerequisite of these methods is the robust estimation of the body pose, i.e. a simpliﬁed body model consisting of several anatomical landmarks.","accessed":{"date-parts":[["2021",10,11]]},"author":[{"family":"Brauer","given":"Jürgen"},{"family":"Hübner","given":"Wolfgang"},{"family":"Arens","given":"Michael"}],"citation-key":"brauerParticleSwarmOptimization2013","DOI":"10.1117/12.2028702","editor":[{"family":"Zamboni","given":"Roberto"},{"family":"Kajzar","given":"Francois"},{"family":"Szep","given":"Attila A."},{"family":"Burgess","given":"Douglas"},{"family":"Owen","given":"Gari"}],"event-place":"Dresden, Germany","event-title":"SPIE Security + Defence","issued":{"date-parts":[["2013",10,16]]},"language":"en","page":"89010D","publisher-place":"Dresden, Germany","source":"DOI.org (Crossref)","title":"Particle swarm optimization on low dimensional pose manifolds for monocular human pose estimation","type":"paper-conference","URL":"http://proceedings.spiedigitallibrary.org/proceeding.aspx?doi=10.1117/12.2028702"},
  {"id":"bronsteinGeometricDeepLearning2017","abstract":"Many scientific fields study data with an underlying structure that is a non-Euclidean space. Some examples include social networks in computational social sciences, sensor networks in communications, functional networks in brain imaging, regulatory networks in genetics, and meshed surfaces in computer graphics. In many applications, such geometric data are large and complex (in the case of social networks, on the scale of billions), and are natural targets for machine learning techniques. In particular, we would like to use deep neural networks, which have recently proven to be powerful tools for a broad range of problems from computer vision, natural language processing, and audio analysis. However, these tools have been most successful on data with an underlying Euclidean or grid-like structure, and in cases where the invariances of these structures are built into networks used to model them. Geometric deep learning is an umbrella term for emerging techniques attempting to generalize (structured) deep neural models to non-Euclidean domains such as graphs and manifolds. The purpose of this paper is to overview different examples of geometric deep learning problems and present available solutions, key difficulties, applications, and future research directions in this nascent field.","accessed":{"date-parts":[["2023",3,14]]},"author":[{"family":"Bronstein","given":"Michael M."},{"family":"Bruna","given":"Joan"},{"family":"LeCun","given":"Yann"},{"family":"Szlam","given":"Arthur"},{"family":"Vandergheynst","given":"Pierre"}],"citation-key":"bronsteinGeometricDeepLearning2017","container-title":"IEEE Signal Processing Magazine","container-title-short":"IEEE Signal Process. Mag.","DOI":"10.1109/MSP.2017.2693418","ISSN":"1053-5888, 1558-0792","issue":"4","issued":{"date-parts":[["2017",7]]},"page":"18-42","source":"arXiv.org","title":"Geometric deep learning: going beyond Euclidean data","title-short":"Geometric deep learning","type":"article-journal","URL":"http://arxiv.org/abs/1611.08097","volume":"34"},
  {"id":"burenius3DPictorialStructures2013","accessed":{"date-parts":[["2021",3,13]]},"author":[{"family":"Burenius","given":"Magnus"},{"family":"Sullivan","given":"Josephine"},{"family":"Carlsson","given":"Stefan"}],"citation-key":"burenius3DPictorialStructures2013","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2013"]]},"page":"3618-3625","source":"openaccess.thecvf.com","title":"3D Pictorial Structures for Multiple View Articulated Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2013/html/Burenius_3D_Pictorial_Structures_2013_CVPR_paper.html"},
  {"id":"caiExploitingSpatialTemporalRelationships2019","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Cai","given":"Yujun"},{"family":"Ge","given":"Liuhao"},{"family":"Liu","given":"Jun"},{"family":"Cai","given":"Jianfei"},{"family":"Cham","given":"Tat-Jen"},{"family":"Yuan","given":"Junsong"},{"family":"Thalmann","given":"Nadia Magnenat"}],"citation-key":"caiExploitingSpatialTemporalRelationships2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"2272-2281","source":"openaccess.thecvf.com","title":"Exploiting Spatial-Temporal Relationships for 3D Pose Estimation via Graph Convolutional Networks","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Cai_Exploiting_Spatial-Temporal_Relationships_for_3D_Pose_Estimation_via_Graph_Convolutional_ICCV_2019_paper.html"},
  {"id":"Cameraparameters","citation-key":"Cameraparameters","language":"en","page":"35","source":"Zotero","title":"camera-parameters","type":"article-journal"},
  {"id":"caoAnatomyGeometryConstrained2021","abstract":"Although signiﬁcant progress has been achieved in monocular 3D human pose estimation, the correlation between body parts and cross-view geometry consistency have not been well studied. In this work, to fully explore the priors on body structure and view-relationship for 3D human pose estimation, we propose an anatomy and geometry constrained one-stage framework. First of all, we deﬁne a kinematic structure model in deep learning framework which represents the joint positions in a tree-structure model. Then we propose bone-length and bonesymmetry losses based on the anatomy prior, to encode the body structure information. To further explore the cross-view geometry information, we introduce a novel training mechanism for multi-view consistency constraints, which eﬀectively reduces unnatural and implausible estimation results. The proposed approach achieves state-of-the-art results on both Human3.6M and MPI-INF-3DHP data sets.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Cao","given":"Xin"},{"family":"Zhao","given":"Xu"}],"citation-key":"caoAnatomyGeometryConstrained2021","container-title":"Computer Vision – ACCV 2020","DOI":"10.1007/978-3-030-69525-5_14","event-place":"Cham","ISBN":"978-3-030-69524-8 978-3-030-69525-5","issued":{"date-parts":[["2021"]]},"language":"en","note":"ZSCC:00004","page":"227-243","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Anatomy and Geometry Constrained One-Stage Framework for 3D Human Pose Estimation","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-030-69525-5_14","volume":"12622"},
  {"id":"caoOpenPoseRealtimeMultiPerson2019","abstract":"Realtime multi-person 2D pose estimation is a key component in enabling machines to have an understanding of people in images and videos. In this work, we present a realtime approach to detect the 2D pose of multiple people in an image. The proposed method uses a nonparametric representation, which we refer to as Part Affinity Fields (PAFs), to learn to associate body parts with individuals in the image. This bottom-up system achieves high accuracy and realtime performance, regardless of the number of people in the image. In previous work, PAFs and body part location estimation were refined simultaneously across training stages. We demonstrate that a PAF-only refinement rather than both PAF and body part location refinement results in a substantial increase in both runtime performance and accuracy. We also present the first combined body and foot keypoint detector, based on an internal annotated foot dataset that we have publicly released. We show that the combined detector not only reduces the inference time compared to running them sequentially, but also maintains the accuracy of each component individually. This work has culminated in the release of OpenPose, the first open-source realtime system for multi-person 2D pose detection, including body, foot, hand, and facial keypoints.","accessed":{"date-parts":[["2022",9,1]]},"author":[{"family":"Cao","given":"Zhe"},{"family":"Hidalgo","given":"Gines"},{"family":"Simon","given":"Tomas"},{"family":"Wei","given":"Shih-En"},{"family":"Sheikh","given":"Yaser"}],"citation-key":"caoOpenPoseRealtimeMultiPerson2019","DOI":"10.48550/arXiv.1812.08008","issued":{"date-parts":[["2019",5,30]]},"number":"arXiv:1812.08008","publisher":"arXiv","source":"arXiv.org","title":"OpenPose: Realtime Multi-Person 2D Pose Estimation using Part Affinity Fields","title-short":"OpenPose","type":"article","URL":"http://arxiv.org/abs/1812.08008"},
  {"id":"caoRealtimeMultiPerson2D2017","accessed":{"date-parts":[["2022",9,1]]},"author":[{"family":"Cao","given":"Zhe"},{"family":"Simon","given":"Tomas"},{"family":"Wei","given":"Shih-En"},{"family":"Sheikh","given":"Yaser"}],"citation-key":"caoRealtimeMultiPerson2D2017","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2017"]]},"page":"7291-7299","source":"openaccess.thecvf.com","title":"Realtime Multi-Person 2D Pose Estimation Using Part Affinity Fields","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2017/html/Cao_Realtime_Multi-Person_2D_CVPR_2017_paper.html"},
  {"id":"carmonAcceleratedMethodsNonConvex2017","abstract":"We present an accelerated gradient method for non-convex optimization problems with Lipschitz continuous first and second derivatives. The method requires time $O(\\epsilon^{-7/4} \\log(1/ \\epsilon) )$ to find an $\\epsilon$-stationary point, meaning a point $x$ such that $\\|\\nabla f(x)\\| \\le \\epsilon$. The method improves upon the $O(\\epsilon^{-2} )$ complexity of gradient descent and provides the additional second-order guarantee that $\\nabla^2 f(x) \\succeq -O(\\epsilon^{1/2})I$ for the computed $x$. Furthermore, our method is Hessian free, i.e. it only requires gradient computations, and is therefore suitable for large scale applications.","accessed":{"date-parts":[["2021",9,8]]},"author":[{"family":"Carmon","given":"Yair"},{"family":"Duchi","given":"John C."},{"family":"Hinder","given":"Oliver"},{"family":"Sidford","given":"Aaron"}],"citation-key":"carmonAcceleratedMethodsNonConvex2017","container-title":"arXiv:1611.00756 [cs, math]","issued":{"date-parts":[["2017",2,2]]},"source":"arXiv.org","title":"Accelerated Methods for Non-Convex Optimization","type":"article-journal","URL":"http://arxiv.org/abs/1611.00756"},
  {"id":"chaiGlobalAdaptationMeets2023","abstract":"When applying a pre-trained 2D-to-3D human pose lifting model to a target unseen dataset, large performance degradation is commonly encountered due to domain shift issues. We observe that the degradation is caused by two factors: 1) the large distribution gap over global positions of poses between the source and target datasets due to variant camera parameters and settings, and 2) the deficient diversity of local structures of poses in training. To this end, we combine \\textbf{global adaptation} and \\textbf{local generalization} in \\textit{PoseDA}, a simple yet effective framework of unsupervised domain adaptation for 3D human pose estimation. Specifically, global adaptation aims to align global positions of poses from the source domain to the target domain with a proposed global position alignment (GPA) module. And local generalization is designed to enhance the diversity of 2D-3D pose mapping with a local pose augmentation (LPA) module. These modules bring significant performance improvement without introducing additional learnable parameters. In addition, we propose local pose augmentation (LPA) to enhance the diversity of 3D poses following an adversarial training scheme consisting of 1) a augmentation generator that generates the parameters of pre-defined pose transformations and 2) an anchor discriminator to ensure the reality and quality of the augmented data. Our approach can be applicable to almost all 2D-3D lifting models. \\textit{PoseDA} achieves 61.3 mm of MPJPE on MPI-INF-3DHP under a cross-dataset evaluation setup, improving upon the previous state-of-the-art method by 10.2\\%.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Chai","given":"Wenhao"},{"family":"Jiang","given":"Zhongyu"},{"family":"Hwang","given":"Jenq-Neng"},{"family":"Wang","given":"Gaoang"}],"citation-key":"chaiGlobalAdaptationMeets2023","DOI":"10.48550/arXiv.2303.16456","issued":{"date-parts":[["2023",8,17]]},"number":"arXiv:2303.16456","publisher":"arXiv","source":"arXiv.org","title":"Global Adaptation meets Local Generalization: Unsupervised Domain Adaptation for 3D Human Pose Estimation","title-short":"Global Adaptation meets Local Generalization","type":"article","URL":"http://arxiv.org/abs/2303.16456"},
  {"id":"charlesPointNetDeepLearning2017","abstract":"Point cloud is an important type of geometric data structure. Due to its irregular format, most researchers transform such data to regular 3D voxel grids or collections of images. This, however, renders data unnecessarily voluminous and causes issues. In this paper, we design a novel type of neural network that directly consumes point clouds, which well respects the permutation invariance of points in the input. Our network, named PointNet, provides a uniﬁed architecture for applications ranging from object classiﬁcation, part segmentation, to scene semantic parsing. Though simple, PointNet is highly efﬁcient and effective. Empirically, it shows strong performance on par or even better than state of the art. Theoretically, we provide analysis towards understanding of what the network has learnt and why the network is robust with respect to input perturbation and corruption.","accessed":{"date-parts":[["2023",10,10]]},"author":[{"family":"Charles","given":"R. Qi"},{"family":"Su","given":"Hao"},{"family":"Kaichun","given":"Mo"},{"family":"Guibas","given":"Leonidas J."}],"citation-key":"charlesPointNetDeepLearning2017","container-title":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2017.16","event-place":"Honolulu, HI","event-title":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-5386-0457-1","issued":{"date-parts":[["2017",7]]},"language":"en","page":"77-85","publisher":"IEEE","publisher-place":"Honolulu, HI","source":"DOI.org (Crossref)","title":"PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation","title-short":"PointNet","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/8099499/"},
  {"id":"CheLiangJianCeYuShiBieJiYuDPMDeZhengHeJiSuanFangFa2014","citation-key":"CheLiangJianCeYuShiBieJiYuDPMDeZhengHeJiSuanFangFa2014","container-title":"NCIG","issued":{"date-parts":[["2014"]]},"title":"车辆检测与识别：基于DPM的整合计算方法","type":"paper-conference"},
  {"id":"chen3DHumanPose2017","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Chen","given":"Ching-Hang"},{"family":"Ramanan","given":"Deva"}],"citation-key":"chen3DHumanPose2017","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2017"]]},"page":"7035-7043","source":"openaccess.thecvf.com","title":"3D Human Pose Estimation = 2D Pose Estimation + Matching","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2017/html/Chen_3D_Human_Pose_CVPR_2017_paper.html"},
  {"id":"chenCrossViewTrackingMultiHuman2020","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Chen","given":"Long"},{"family":"Ai","given":"Haizhou"},{"family":"Chen","given":"Rui"},{"family":"Zhuang","given":"Zijie"},{"family":"Liu","given":"Shuang"}],"citation-key":"chenCrossViewTrackingMultiHuman2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"3279-3288","source":"openaccess.thecvf.com","title":"Cross-View Tracking for Multi-Human 3D Pose Estimation at Over 100 FPS","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Chen_Cross-View_Tracking_for_Multi-Human_3D_Pose_Estimation_at_Over_100_CVPR_2020_paper.html"},
  {"id":"chengHigherHRNetScaleAwareRepresentation2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Cheng","given":"Bowen"},{"family":"Xiao","given":"Bin"},{"family":"Wang","given":"Jingdong"},{"family":"Shi","given":"Honghui"},{"family":"Huang","given":"Thomas S."},{"family":"Zhang","given":"Lei"}],"citation-key":"chengHigherHRNetScaleAwareRepresentation2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"5386-5395","source":"openaccess.thecvf.com","title":"HigherHRNet: Scale-Aware Representation Learning for Bottom-Up Human Pose Estimation","title-short":"HigherHRNet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Cheng_HigherHRNet_Scale-Aware_Representation_Learning_for_Bottom-Up_Human_Pose_Estimation_CVPR_2020_paper.html"},
  {"id":"chengMonocular3DMultiPerson2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Cheng","given":"Yu"},{"family":"Wang","given":"Bo"},{"family":"Yang","given":"Bo"},{"family":"Tan","given":"Robby T."}],"citation-key":"chengMonocular3DMultiPerson2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"7649-7659","source":"openaccess.thecvf.com","title":"Monocular 3D Multi-Person Pose Estimation by Integrating Top-Down and Bottom-Up Networks","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Cheng_Monocular_3D_Multi-Person_Pose_Estimation_by_Integrating_Top-Down_and_Bottom-Up_CVPR_2021_paper.html"},
  {"id":"chengOcclusionAwareNetworks3D2019","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Cheng","given":"Yu"},{"family":"Yang","given":"Bo"},{"family":"Wang","given":"Bo"},{"family":"Yan","given":"Wending"},{"family":"Tan","given":"Robby T."}],"citation-key":"chengOcclusionAwareNetworks3D2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"723-732","source":"openaccess.thecvf.com","title":"Occlusion-Aware Networks for 3D Human Pose Estimation in Video","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Cheng_Occlusion-Aware_Networks_for_3D_Human_Pose_Estimation_in_Video_ICCV_2019_paper.html"},
  {"id":"chenGSDFGeometryDrivenSigned2023","abstract":"Signed distance functions (SDFs) is an attractive framework that has recently shown promising results for 3D shape reconstruction from images. SDFs seamlessly generalize to different shape resolutions and topologies but lack explicit modelling of the underlying 3D geometry. In this work, we exploit the hand structure and use it as guidance for SDF-based shape reconstruction. In particular, we address reconstruction of hands and manipulated objects from monocular RGB images. To this end, we estimate poses of hands and objects and use them to guide 3D reconstruction. More specifically, we predict kinematic chains of pose transformations and align SDFs with highly-articulated hand poses. We improve the visual features of 3D points with geometry alignment and further leverage temporal information to enhance the robustness to occlusion and motion blurs. We conduct extensive experiments on the challenging ObMan and DexYCB benchmarks and demonstrate significant improvements of the proposed method over the state of the art.","accessed":{"date-parts":[["2023",9,29]]},"author":[{"family":"Chen","given":"Zerui"},{"family":"Chen","given":"Shizhe"},{"family":"Schmid","given":"Cordelia"},{"family":"Laptev","given":"Ivan"}],"citation-key":"chenGSDFGeometryDrivenSigned2023","issued":{"date-parts":[["2023",4,24]]},"number":"arXiv:2304.11970","publisher":"arXiv","source":"arXiv.org","title":"gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction","title-short":"gSDF","type":"article","URL":"http://arxiv.org/abs/2304.11970"},
  {"id":"chenJointHandObject3D2021","abstract":"Accurate 3D reconstruction of the hand and object shape from a hand-object image is important for understanding human-object interaction as well as human daily activities. Different from bare hand pose estimation, hand-object interaction poses a strong constraint on both the hand and its manipulated object, which suggests that hand conﬁguration may be crucial contextual information for the object, and vice versa. However, current approaches address this task by training a two-branch network to reconstruct the hand and object separately with little communication between the two branches. In this work, we propose to consider hand and object jointly in feature space and explore the reciprocity of the two branches. We extensively investigate cross-branch feature fusion architectures with MLP or LSTM units. Among the investigated architectures, a variant with LSTM units that enhances object feature with hand feature shows the best performance gain. Moreover, we employ an auxiliary depth estimation module to augment the input RGB image with the estimated depth map, which further improves the reconstruction accuracy. Experiments conducted on public datasets demonstrate that our approach signiﬁcantly outperforms existing approaches in terms of the reconstruction accuracy of objects.","accessed":{"date-parts":[["2023",10,12]]},"author":[{"family":"Chen","given":"Yujin"},{"family":"Tu","given":"Zhigang"},{"family":"Kang","given":"Di"},{"family":"Chen","given":"Ruizhi"},{"family":"Bao","given":"Linchao"},{"family":"Zhang","given":"Zhengyou"},{"family":"Yuan","given":"Junsong"}],"citation-key":"chenJointHandObject3D2021","container-title":"IEEE Transactions on Image Processing","container-title-short":"IEEE Trans. on Image Process.","DOI":"10.1109/TIP.2021.3068645","ISSN":"1057-7149, 1941-0042","issued":{"date-parts":[["2021"]]},"language":"en","page":"4008-4021","source":"DOI.org (Crossref)","title":"Joint Hand-Object 3D Reconstruction From a Single Image With Cross-Branch Feature Fusion","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9390307/","volume":"30"},
  {"id":"chenJointHandobject3D2021a","abstract":"Accurate 3D reconstruction of the hand and object shape from a hand-object image is important for understanding human-object interaction as well as human daily activities. Different from bare hand pose estimation, hand-object interaction poses a strong constraint on both the hand and its manipulated object, which suggests that hand configuration may be crucial contextual information for the object, and vice versa. However, current approaches address this task by training a two-branch network to reconstruct the hand and object separately with little communication between the two branches. In this work, we propose to consider hand and object jointly in feature space and explore the reciprocity of the two branches. We extensively investigate cross-branch feature fusion architectures with MLP or LSTM units. Among the investigated architectures, a variant with LSTM units that enhances object feature with hand feature shows the best performance gain. Moreover, we employ an auxiliary depth estimation module to augment the input RGB image with the estimated depth map, which further improves the reconstruction accuracy. Experiments conducted on public datasets demonstrate that our approach significantly outperforms existing approaches in terms of the reconstruction accuracy of objects.","accessed":{"date-parts":[["2023",10,3]]},"author":[{"family":"Chen","given":"Yujin"},{"family":"Tu","given":"Zhigang"},{"family":"Kang","given":"Di"},{"family":"Chen","given":"Ruizhi"},{"family":"Bao","given":"Linchao"},{"family":"Zhang","given":"Zhengyou"},{"family":"Yuan","given":"Junsong"}],"citation-key":"chenJointHandobject3D2021a","container-title":"IEEE Transactions on Image Processing","container-title-short":"IEEE Trans. on Image Process.","DOI":"10.1109/TIP.2021.3068645","ISSN":"1057-7149, 1941-0042","issued":{"date-parts":[["2021"]]},"page":"4008-4021","source":"arXiv.org","title":"Joint Hand-object 3D Reconstruction from a Single Image with Cross-branch Feature Fusion","type":"article-journal","URL":"http://arxiv.org/abs/2006.15561","volume":"30"},
  {"id":"chenLearningImplicitFields2019","abstract":"We advocate the use of implicit ﬁelds for learning generative models of shapes and introduce an implicit ﬁeld decoder, called IM-NET, for shape generation, aimed at improving the visual quality of the generated shapes. An implicit ﬁeld assigns a value to each point in 3D space, so that a shape can be extracted as an iso-surface. IM-NET is trained to perform this assignment by means of a binary classiﬁer. Speciﬁcally, it takes a point coordinate, along with a feature vector encoding a shape, and outputs a value which indicates whether the point is outside the shape or not. By replacing conventional decoders by our implicit decoder for representation learning (via IM-AE) and shape generation (via IM-GAN), we demonstrate superior results for tasks such as generative shape modeling, interpolation, and single-view 3D reconstruction, particularly in terms of visual quality. Code and supplementary material are available at https://github.com/czq142857/implicit-decoder.","accessed":{"date-parts":[["2023",10,10]]},"author":[{"family":"Chen","given":"Zhiqin"},{"family":"Zhang","given":"Hao"}],"citation-key":"chenLearningImplicitFields2019","container-title":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2019.00609","event-place":"Long Beach, CA, USA","event-title":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-7281-3293-8","issued":{"date-parts":[["2019",6]]},"language":"en","page":"5932-5941","publisher":"IEEE","publisher-place":"Long Beach, CA, USA","source":"DOI.org (Crossref)","title":"Learning Implicit Fields for Generative Shape Modeling","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8953765/"},
  {"id":"chenleiguoSpatiotemporalSaliencyDetection2008","abstract":"Salient areas in natural scenes are generally regarded as the candidates of attention focus in human eyes, which is the key stage in object detection. In computer vision, many models have been proposed to simulate the behavior of eyes such as SaliencyToolBox (STB), neuromorphic vision toolkit (NVT) and etc., but they demand high computational cost and their remarkable results mostly rely on the choice of parameters. Recently a simple and fast approach based on Fourier transform called spectral residual (SR) was proposed, which used SR of the amplitude spectrum to obtain the saliency map. The results are good, but the reason is questionable.","author":[{"family":"Chenlei Guo","given":""},{"family":"Qi Ma","given":""},{"family":"Liming Zhang","given":""}],"citation-key":"chenleiguoSpatiotemporalSaliencyDetection2008","container-title":"2008 IEEE Conference on Computer Vision and Pattern Recognition","DOI":"10.1109/CVPR.2008.4587715","event-title":"2008 IEEE Conference on Computer Vision and Pattern Recognition","ISSN":"1063-6919","issued":{"date-parts":[["2008",6]]},"page":"1-8","source":"IEEE Xplore","title":"Spatio-temporal Saliency detection using phase spectrum of quaternion fourier transform","type":"paper-conference"},
  {"id":"chenleiguoSpatiotemporalSaliencyDetection2008a","abstract":"Salient areas in natural scenes are generally regarded as the candidates of attention focus in human eyes, which is the key stage in object detection. In computer vision, many models have been proposed to simulate the behavior of eyes such as SaliencyToolBox (STB), neuromorphic vision toolkit (NVT) and etc., but they demand high computational cost and their remarkable results mostly rely on the choice of parameters. Recently a simple and fast approach based on Fourier transform called spectral residual (SR) was proposed, which used SR of the amplitude spectrum to obtain the saliency map. The results are good, but the reason is questionable.","author":[{"family":"Chenlei Guo","given":""},{"family":"Qi Ma","given":""},{"family":"Liming Zhang","given":""}],"citation-key":"chenleiguoSpatiotemporalSaliencyDetection2008a","container-title":"2008 IEEE Conference on Computer Vision and Pattern Recognition","DOI":"10.1109/CVPR.2008.4587715","event-title":"2008 IEEE Conference on Computer Vision and Pattern Recognition","ISSN":"1063-6919","issued":{"date-parts":[["2008",6]]},"page":"1-8","source":"IEEE Xplore","title":"Spatio-temporal Saliency detection using phase spectrum of quaternion fourier transform","type":"paper-conference"},
  {"id":"chenOcclusionObjectTracking2022","abstract":"Visual object tracking is an important issue that has received long-term attention in computer vision. The ability to eﬀectively handle occlusion, especially severe occlusion, is an important aspect of evaluating the performance of object tracking algorithms in long-term tracking, and is of great signiﬁcance to improving the robustness of object tracking algorithms. However, most object tracking algorithms lack a processing mechanism speciﬁcally for occlusion. In the case of occlusion, due to the lack of target information, it is necessary to predict the target position based on the motion trajectory. Kalman ﬁltering and particle ﬁltering can eﬀectively predict the target motion state based on the historical motion information. A single object tracking method, called probabilistic discriminative model prediction (PrDiMP), is based on the spatial attention mechanism in complex scenes and occlusions. In order to improve the performance of PrDiMP, Kalman ﬁltering, particle ﬁltering and linear ﬁltering are introduced. First, for the occlusion situation, Kalman ﬁltering and particle ﬁltering are respectively introduced to predict the object position, thereby replacing the detection result of the original tracking algorithm and stopping recursion of target model. Second, for detection-jump problem of similar objects in complex scenes, a linear ﬁltering window is added. The evaluation results on the three datasets, including GOT-10k, UAV123 and LaSOT, and the visualization results on several videos, show that our algorithms have improved tracking performance under occlusion and the detection-jump is eﬀectively suppressed.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Chen","given":"Kun"},{"family":"Zhao","given":"Xu"},{"family":"Dong","given":"Chunyu"},{"family":"Di","given":"Zichao"},{"family":"Chen","given":"Zongzhi"}],"citation-key":"chenOcclusionObjectTracking2022","container-title":"Journal of Shanghai Jiaotong University (Science)","container-title-short":"J. Shanghai Jiaotong Univ. (Sci.)","DOI":"10.1007/s12204-022-2484-8","ISSN":"1007-1172, 1995-8188","issued":{"date-parts":[["2022",8,19]]},"language":"en","note":"ZSCC:00000","source":"DOI.org (Crossref)","title":"Anti-Occlusion Object Tracking Algorithm Based on Filter Prediction","type":"article-journal","URL":"https://link.springer.com/10.1007/s12204-022-2484-8"},
  {"id":"chenStructuralTriangulationClosedForm2022","abstract":"We propose Structural Triangulation, a closed-form solution for optimal 3D human pose considering multi-view 2D pose estimations, calibrated camera parameters, and bone lengths. To start with, we focus on embedding structural constraints of human body in the process of 2D-to-3D inference using triangulation. Assume bone lengths are known in prior, then the inference process is formulated as a constrained optimization problem. By proper approximation, the closed-form solution to this problem is achieved. Further, we generalize our method with Step Constraint Algorithm to help converge when large error occurs in 2D estimations. In experiment, public datasets (Human3.6M and Total Capture) and synthesized data are used for evaluation. Our method achieves state-of-the-art results on Human3.6M Dataset when bone lengths are known and competitive results when they are not. The generality and eﬃciency of our method are also demonstrated.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Chen","given":"Zhuo"},{"family":"Zhao","given":"Xu"},{"family":"Wan","given":"Xiaoyue"}],"citation-key":"chenStructuralTriangulationClosedForm2022","container-title":"Computer Vision – ECCV 2022","DOI":"10.1007/978-3-031-20065-6_40","event-place":"Cham","ISBN":"978-3-031-20064-9 978-3-031-20065-6","issued":{"date-parts":[["2022"]]},"language":"en","note":"ZSCC:00000","page":"695-711","publisher":"Springer Nature Switzerland","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Structural Triangulation: A Closed-Form Solution to Constrained 3D Human Pose Estimation","title-short":"Structural Triangulation","type":"paper-conference","URL":"https://link.springer.com/10.1007/978-3-031-20065-6_40","volume":"13665"},
  {"id":"chenUnsupervised3DPose2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Chen","given":"Ching-Hang"},{"family":"Tyagi","given":"Ambrish"},{"family":"Agrawal","given":"Amit"},{"family":"Drover","given":"Dylan"},{"family":"Mv","given":"Rohith"},{"family":"Stojanov","given":"Stefan"},{"family":"Rehg","given":"James M."}],"citation-key":"chenUnsupervised3DPose2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"5714-5724","source":"openaccess.thecvf.com","title":"Unsupervised 3D Pose Estimation With Geometric Self-Supervision","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Unsupervised_3D_Pose_Estimation_With_Geometric_Self-Supervision_CVPR_2019_paper.html"},
  {"id":"chenUnsupervised3DPose2019a","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Chen","given":"Ching-Hang"},{"family":"Tyagi","given":"Ambrish"},{"family":"Agrawal","given":"Amit"},{"family":"Drover","given":"Dylan"},{"family":"Mv","given":"Rohith"},{"family":"Stojanov","given":"Stefan"},{"family":"Rehg","given":"James M."}],"citation-key":"chenUnsupervised3DPose2019a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"5714-5724","source":"openaccess.thecvf.com","title":"Unsupervised 3D Pose Estimation With Geometric Self-Supervision","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Unsupervised_3D_Pose_Estimation_With_Geometric_Self-Supervision_CVPR_2019_paper.html"},
  {"id":"chenWeaklySupervisedDiscoveryGeometryAware2019","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Chen","given":"Xipeng"},{"family":"Lin","given":"Kwan-Yee"},{"family":"Liu","given":"Wentao"},{"family":"Qian","given":"Chen"},{"family":"Lin","given":"Liang"}],"citation-key":"chenWeaklySupervisedDiscoveryGeometryAware2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"10895-10904","source":"openaccess.thecvf.com","title":"Weakly-Supervised Discovery of Geometry-Aware Representation for 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Chen_Weakly-Supervised_Discovery_of_Geometry-Aware_Representation_for_3D_Human_Pose_Estimation_CVPR_2019_paper.html"},
  {"id":"choCameraDistortionAware3D2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Cho","given":"Hanbyel"},{"family":"Cho","given":"Yooshin"},{"family":"Yu","given":"Jaemyung"},{"family":"Kim","given":"Junmo"}],"citation-key":"choCameraDistortionAware3D2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11169-11178","source":"openaccess.thecvf.com","title":"Camera Distortion-Aware 3D Human Pose Estimation in Video With Optimization-Based Meta-Learning","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Cho_Camera_Distortion-Aware_3D_Human_Pose_Estimation_in_Video_With_Optimization-Based_ICCV_2021_paper.html"},
  {"id":"choiLearningEstimateRobust2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Choi","given":"Hongsuk"},{"family":"Moon","given":"Gyeongsik"},{"family":"Park","given":"JoonKyu"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"choiLearningEstimateRobust2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"1475-1484","source":"openaccess.thecvf.com","title":"Learning To Estimate Robust 3D Human Mesh From In-the-Wild Crowded Scenes","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Choi_Learning_To_Estimate_Robust_3D_Human_Mesh_From_In-the-Wild_Crowded_CVPR_2022_paper.html"},
  {"id":"chongjingAnalyzingMotionPatterns2013","abstract":"Crowded scene analysis is currently a hot and challenging topic in computer vision field. The ability to analyze motion patterns from videos is a difficult, but critical part of this problem. In this paper, we propose a novel approach for the analysis of motion patterns by clustering the tracklets using an unsupervised hierarchical clustering algorithm, where the similarity between tracklets is measured by the Longest Common Subsequences. The tracklets are obtained by tracking dense points under three effective rules, therefore enabling it to capture the motion patterns in crowded scenes. The analysis of motion patterns is implemented in a completely unsupervised way, and the tracklets are clustered automatically through hierarchical clustering algorithm based on a graphic model. To validate the performance of our approach, we conducted experimental evaluations on two datasets. The results reveal the precise distributions of motion patterns in current crowded videos and demonstrate the effectiveness of our approach.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Chongjing","given":"Wang"},{"family":"Xu","given":"Zhao"},{"family":"Yi","given":"Zou"},{"family":"Yuncai","given":"Liu"}],"citation-key":"chongjingAnalyzingMotionPatterns2013","container-title":"China Communications","container-title-short":"China Commun.","DOI":"10.1109/CC.2013.6506940","ISSN":"1673-5447","issue":"4","issued":{"date-parts":[["2013",4]]},"language":"en","note":"ZSCC:00033","number":"4","page":"144-154","source":"DOI.org (Crossref)","title":"Analyzing motion patterns in crowded scenes via automatic tracklets clustering","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/6506940/","volume":"10"},
  {"id":"choudhuryTEMPOEfficientMultiView2023","abstract":"Existing volumetric methods for predicting 3D human pose estimation are accurate, but computationally expensive and optimized for single time-step prediction. We present TEMPO, an efficient multi-view pose estimation model that learns a robust spatiotemporal representation, improving pose accuracy while also tracking and forecasting human pose. We significantly reduce computation compared to the state-of-the-art by recurrently computing perperson 2D pose features, fusing both spatial and temporal information into a single representation. In doing so, our model is able to use spatiotemporal context to predict more accurate human poses without sacrificing efficiency. We further use this representation to track human poses over time as well as predict future poses. Finally, we demonstrate that our model is able to generalize across datasets without scene-specific fine-tuning. TEMPO achieves 10% better MPJPE with a 33× improvement in FPS compared to TesseTrack on the challenging CMU Panoptic Studio dataset. Our code and demos are available at https: //rccchoudhury.github.io/tempo2023/.","accessed":{"date-parts":[["2024",2,1]]},"author":[{"family":"Choudhury","given":"Rohan"},{"family":"Kitani","given":"Kris M."},{"family":"Jeni","given":"László A."}],"citation-key":"choudhuryTEMPOEfficientMultiView2023","container-title":"2023 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV51070.2023.01355","event-place":"Paris, France","event-title":"2023 IEEE/CVF International Conference on Computer Vision (ICCV)","ISBN":"979-8-3503-0718-4","issued":{"date-parts":[["2023",10,1]]},"language":"en","page":"14704-14714","publisher":"IEEE","publisher-place":"Paris, France","source":"DOI.org (Crossref)","title":"TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting","title-short":"TEMPO","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10377046/"},
  {"id":"choudhuryTEMPOEfficientMultiView2023a","abstract":"Existing volumetric methods for predicting 3D human pose estimation are accurate, but computationally expensive and optimized for single time-step prediction. We present TEMPO, an efficient multi-view pose estimation model that learns a robust spatiotemporal representation, improving pose accuracy while also tracking and forecasting human pose. We significantly reduce computation compared to the state-of-the-art by recurrently computing perperson 2D pose features, fusing both spatial and temporal information into a single representation. In doing so, our model is able to use spatiotemporal context to predict more accurate human poses without sacrificing efficiency. We further use this representation to track human poses over time as well as predict future poses. Finally, we demonstrate that our model is able to generalize across datasets without scene-specific fine-tuning. TEMPO achieves 10% better MPJPE with a 33× improvement in FPS compared to TesseTrack on the challenging CMU Panoptic Studio dataset. Our code and demos are available at https: //rccchoudhury.github.io/tempo2023/.","accessed":{"date-parts":[["2024",2,1]]},"author":[{"family":"Choudhury","given":"Rohan"},{"family":"Kitani","given":"Kris M."},{"family":"Jeni","given":"László A."}],"citation-key":"choudhuryTEMPOEfficientMultiView2023a","container-title":"2023 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV51070.2023.01355","event-place":"Paris, France","event-title":"2023 IEEE/CVF International Conference on Computer Vision (ICCV)","ISBN":"979-8-3503-0718-4","issued":{"date-parts":[["2023",10,1]]},"language":"en","page":"14704-14714","publisher":"IEEE","publisher-place":"Paris, France","source":"DOI.org (Crossref)","title":"TEMPO: Efficient Multi-View Pose Estimation, Tracking, and Forecasting","title-short":"TEMPO","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10377046/"},
  {"id":"ciGFPoseLearning3D2022","abstract":"Learning 3D human pose prior is essential to human-centered AI. Here, we present GFPose, a versatile framework to model plausible 3D human poses for various applications. At the core of GFPose is a time-dependent score network, which estimates the gradient on each body joint and progressively denoises the perturbed 3D human pose to match a given task specification. During the denoising process, GFPose implicitly incorporates pose priors in gradients and unifies various discriminative and generative tasks in an elegant framework. Despite the simplicity, GFPose demonstrates great potential in several downstream tasks. Our experiments empirically show that 1) as a multi-hypothesis pose estimator, GFPose outperforms existing SOTAs by 20% on Human3.6M dataset. 2) as a single-hypothesis pose estimator, GFPose achieves comparable results to deterministic SOTAs, even with a vanilla backbone. 3) GFPose is able to produce diverse and realistic samples in pose denoising, completion and generation tasks. Project page https://sites.google.com/view/gfpose/","accessed":{"date-parts":[["2023",5,15]]},"author":[{"family":"Ci","given":"Hai"},{"family":"Wu","given":"Mingdong"},{"family":"Zhu","given":"Wentao"},{"family":"Ma","given":"Xiaoxuan"},{"family":"Dong","given":"Hao"},{"family":"Zhong","given":"Fangwei"},{"family":"Wang","given":"Yizhou"}],"citation-key":"ciGFPoseLearning3D2022","DOI":"10.48550/arXiv.2212.08641","issued":{"date-parts":[["2022",12,16]]},"number":"arXiv:2212.08641","publisher":"arXiv","source":"arXiv.org","title":"GFPose: Learning 3D Human Pose Prior with Gradient Fields","title-short":"GFPose","type":"article","URL":"http://arxiv.org/abs/2212.08641"},
  {"id":"ciOptimizingNetworkStructure2019","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Ci","given":"Hai"},{"family":"Wang","given":"Chunyu"},{"family":"Ma","given":"Xiaoxuan"},{"family":"Wang","given":"Yizhou"}],"citation-key":"ciOptimizingNetworkStructure2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"2262-2271","source":"openaccess.thecvf.com","title":"Optimizing Network Structure for 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Ci_Optimizing_Network_Structure_for_3D_Human_Pose_Estimation_ICCV_2019_paper.html"},
  {"id":"cleverBodiesRest3D2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Clever","given":"Henry M."},{"family":"Erickson","given":"Zackory"},{"family":"Kapusta","given":"Ariel"},{"family":"Turk","given":"Greg"},{"family":"Liu","given":"Karen"},{"family":"Kemp","given":"Charles C."}],"citation-key":"cleverBodiesRest3D2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"6215-6224","source":"openaccess.thecvf.com","title":"Bodies at Rest: 3D Human Pose and Shape Estimation From a Pressure Image Using Synthetic Data","title-short":"Bodies at Rest","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Clever_Bodies_at_Rest_3D_Human_Pose_and_Shape_Estimation_From_CVPR_2020_paper.html"},
  {"id":"CoverageProblemWireless","abstract":"Explore millions of resources from scholarly journals, books, newspapers, videos and more, on the ProQuest Platform.","accessed":{"date-parts":[["2020",12,19]]},"citation-key":"CoverageProblemWireless","language":"zh","title":"Coverage Problem in Wireless Sensor Network: A Survey - ProQuest","title-short":"Coverage Problem in Wireless Sensor Network","type":"webpage","URL":"https://search.proquest.com/openview/e1a2af724b9f35bfae35f6e99aa7d906/1?cbl=136095&pq-origsite=gscholar"},
  {"id":"CVPR2014Open","accessed":{"date-parts":[["2021",1,18]]},"citation-key":"CVPR2014Open","title":"CVPR 2014 Open Access Repository","type":"webpage","URL":"https://openaccess.thecvf.com/content_cvpr_2014/html/Belagiannis_3D_Pictorial_Structures_2014_CVPR_paper.html"},
  {"id":"CVPR2023Open","accessed":{"date-parts":[["2023",9,4]]},"citation-key":"CVPR2023Open","title":"CVPR 2023 Open Access Repository","type":"webpage","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Petrov_Object_Pop-Up_Can_We_Infer_3D_Objects_and_Their_Poses_CVPR_2023_paper.html"},
  {"id":"daiSLOPER4DSceneAwareDataset2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Dai","given":"Yudi"},{"family":"Lin","given":"Yitai"},{"family":"Lin","given":"Xiping"},{"family":"Wen","given":"Chenglu"},{"family":"Xu","given":"Lan"},{"family":"Yi","given":"Hongwei"},{"family":"Shen","given":"Siqi"},{"family":"Ma","given":"Yuexin"},{"family":"Wang","given":"Cheng"}],"citation-key":"daiSLOPER4DSceneAwareDataset2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"682-692","source":"openaccess.thecvf.com","title":"SLOPER4D: A Scene-Aware Dataset for Global 4D Human Pose Estimation in Urban Environments","title-short":"SLOPER4D","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Dai_SLOPER4D_A_Scene-Aware_Dataset_for_Global_4D_Human_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"davydovAdversarialParametricPose2022","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Davydov","given":"Andrey"},{"family":"Remizova","given":"Anastasia"},{"family":"Constantin","given":"Victor"},{"family":"Honari","given":"Sina"},{"family":"Salzmann","given":"Mathieu"},{"family":"Fua","given":"Pascal"}],"citation-key":"davydovAdversarialParametricPose2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"10997-11005","source":"openaccess.thecvf.com","title":"Adversarial Parametric Pose Prior","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Davydov_Adversarial_Parametric_Pose_Prior_CVPR_2022_paper.html"},
  {"id":"debayleRigidImageRegistration2016","abstract":"This paper aims to propose a new feature and intensity-based image registration method. The proposed approach is based on the block matching algorithm (Ourselin et al., 2000 [1]): a displacement field is locally computed by matching spatially invariant intensity sub-blocks of the images before performing an optimization algorithm from this vector field to estimate the transformation. Our approach proposes a new way to calculate the displacement field by matching spatially variant sub-blocks of the images, called General Adaptive Neighborhoods (GANs) (Debayle and Pinoli, 2006 [2]). These neighborhoods are adaptive with respect to both the intensities and the spatial structures of the image. They represent the patterns within the grayscale images. This paper also presents a consistent shape metric used to match the GANs. The performed qualitative and quantitative experiments show that the proposed GAN matching method provides accurate displacement fields enabling us to perform image rigid registration, even for data from different modalities, that outperforms the classical block matching algorithm with respect to robustness and accuracy criteria.","accessed":{"date-parts":[["2024",1,12]]},"author":[{"family":"Debayle","given":"Johan"},{"family":"Presles","given":"Benoit"}],"citation-key":"debayleRigidImageRegistration2016","container-title":"Pattern Recognition","container-title-short":"Pattern Recognition","DOI":"10.1016/j.patcog.2016.01.024","ISSN":"0031-3203","issued":{"date-parts":[["2016",7,1]]},"page":"45-57","source":"ScienceDirect","title":"Rigid image registration by General Adaptive Neighborhood matching","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0031320316000455","volume":"55"},
  {"id":"DeepLearningBased","abstract":"Human pose estimation has received significant attention recently due to its various applications in the real world. As the performance of the state-of-the-art human pose estimation methods can be improved by deep learning, this paper presents a comprehensive survey of deep learning based human pose estimation methods and analyzes the methodologies employed. We summarize and discuss recent works with a methodology-based taxonomy. Single-person and multi-person pipelines are first reviewed separately. Then, the deep learning techniques applied in these pipelines are compared and analyzed. The datasets and metrics used in this task are also discussed and compared. The aim of this survey is to make every step in the estimation pipelines interpretable and to provide readers a readily comprehensible explanation. Moreover, the unsolved problems and challenges for future research are discussed.","accessed":{"date-parts":[["2023",9,23]]},"citation-key":"DeepLearningBased","language":"en-US","title":"Deep learning based 2D human pose estimation: A survey","title-short":"Deep learning based 2D human pose estimation","type":"webpage","URL":"https://ieeexplore.ieee.org/abstract/document/8727761/"},
  {"id":"dillerForecastingCharacteristic3D2022","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Diller","given":"Christian"},{"family":"Funkhouser","given":"Thomas"},{"family":"Dai","given":"Angela"}],"citation-key":"dillerForecastingCharacteristic3D2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"15914-15923","source":"openaccess.thecvf.com","title":"Forecasting Characteristic 3D Poses of Human Actions","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Diller_Forecasting_Characteristic_3D_Poses_of_Human_Actions_CVPR_2022_paper.html"},
  {"id":"dingGloballyOptimalRelative2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Ding","given":"Yaqing"},{"family":"Barath","given":"Daniel"},{"family":"Yang","given":"Jian"},{"family":"Kong","given":"Hui"},{"family":"Kukelova","given":"Zuzana"}],"citation-key":"dingGloballyOptimalRelative2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"394-403","source":"openaccess.thecvf.com","title":"Globally Optimal Relative Pose Estimation With Gravity Prior","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Ding_Globally_Optimal_Relative_Pose_Estimation_With_Gravity_Prior_CVPR_2021_paper.html"},
  {"id":"dongFastRobustMultiPerson2019","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Dong","given":"Junting"},{"family":"Jiang","given":"Wen"},{"family":"Huang","given":"Qixing"},{"family":"Bao","given":"Hujun"},{"family":"Zhou","given":"Xiaowei"}],"citation-key":"dongFastRobustMultiPerson2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"7792-7801","source":"openaccess.thecvf.com","title":"Fast and Robust Multi-Person 3D Pose Estimation From Multiple Views","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Dong_Fast_and_Robust_Multi-Person_3D_Pose_Estimation_From_Multiple_Views_CVPR_2019_paper.html"},
  {"id":"dongShapeAwareMultiPersonPose2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Dong","given":"Zijian"},{"family":"Song","given":"Jie"},{"family":"Chen","given":"Xu"},{"family":"Guo","given":"Chen"},{"family":"Hilliges","given":"Otmar"}],"citation-key":"dongShapeAwareMultiPersonPose2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11158-11168","source":"openaccess.thecvf.com","title":"Shape-Aware Multi-Person Pose Estimation From Multi-View Images","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Dong_Shape-Aware_Multi-Person_Pose_Estimation_From_Multi-View_Images_ICCV_2021_paper.html"},
  {"id":"doringPoseTrack21DatasetPerson2022","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Döring","given":"Andreas"},{"family":"Chen","given":"Di"},{"family":"Zhang","given":"Shanshan"},{"family":"Schiele","given":"Bernt"},{"family":"Gall","given":"Jürgen"}],"citation-key":"doringPoseTrack21DatasetPerson2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"20963-20972","source":"openaccess.thecvf.com","title":"PoseTrack21: A Dataset for Person Search, Multi-Object Tracking and Multi-Person Pose Tracking","title-short":"PoseTrack21","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Doring_PoseTrack21_A_Dataset_for_Person_Search_Multi-Object_Tracking_and_Multi-Person_CVPR_2022_paper.html"},
  {"id":"duanRevisitingSkeletonbasedAction2022","abstract":"Human skeleton, as a compact representation of human action, has received increasing attention in recent years. Many skeleton-based action recognition methods adopt GCNs to extract features on top of human skeletons. Despite the positive results shown in these attempts, GCN-based methods are subject to limitations in robustness, interoperability, and scalability. In this work, we propose PoseConv3D, a new approach to skeleton-based action recognition. PoseConv3D relies on a 3D heatmap volume instead of a graph sequence as the base representation of human skeletons. Compared to GCN-based methods, PoseConv3D is more effective in learning spatiotemporal features, more robust against pose estimation noises, and generalizes better in cross-dataset settings. Also, PoseConv3D can handle multiple-person scenarios without additional computation costs. The hierarchical features can be easily integrated with other modalities at early fusion stages, providing a great design space to boost the performance. PoseConv3D achieves the state-of-the-art on five of six standard skeleton-based action recognition benchmarks. Once fused with other modalities, it achieves the state-of-the-art on all eight multi-modality action recognition benchmarks. Code has been made available at: https://github.com/kennymckormick/pyskl.","accessed":{"date-parts":[["2023",4,11]]},"author":[{"family":"Duan","given":"Haodong"},{"family":"Zhao","given":"Yue"},{"family":"Chen","given":"Kai"},{"family":"Lin","given":"Dahua"},{"family":"Dai","given":"Bo"}],"citation-key":"duanRevisitingSkeletonbasedAction2022","container-title":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR52688.2022.00298","event-place":"New Orleans, LA, USA","event-title":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-6654-6946-3","issued":{"date-parts":[["2022",6]]},"language":"en","page":"2959-2968","publisher":"IEEE","publisher-place":"New Orleans, LA, USA","source":"DOI.org (Crossref)","title":"Revisiting Skeleton-based Action Recognition","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9879048/"},
  {"id":"duanRevisitingSkeletonBasedAction2022a","accessed":{"date-parts":[["2023",4,11]]},"author":[{"family":"Duan","given":"Haodong"},{"family":"Zhao","given":"Yue"},{"family":"Chen","given":"Kai"},{"family":"Lin","given":"Dahua"},{"family":"Dai","given":"Bo"}],"citation-key":"duanRevisitingSkeletonBasedAction2022a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"2969-2978","source":"openaccess.thecvf.com","title":"Revisiting Skeleton-Based Action Recognition","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Duan_Revisiting_Skeleton-Based_Action_Recognition_CVPR_2022_paper.html"},
  {"id":"ellHypercomplexFourierTransforms2007","abstract":"Fourier transforms are a fundamental tool in signal and image processing, yet, until recently, there was no definition of a Fourier transform applicable to color images in a holistic manner. In this paper, hypercomplex numbers, specifically quaternions, are used to define a Fourier transform applicable to color images. The properties of the transform are developed, and it is shown that the transform may be computed using two standard complex fast Fourier transforms. The resulting spectrum is explained in terms of familiar phase and modulus concepts, and a new concept of hypercomplex axis. A method for visualizing the spectrum using color graphics is also presented. Finally, a convolution operational formula in the spectral domain is discussed","author":[{"family":"Ell","given":"Todd A."},{"family":"Sangwine","given":"Stephen J."}],"citation-key":"ellHypercomplexFourierTransforms2007","container-title":"IEEE Transactions on Image Processing","DOI":"10.1109/TIP.2006.884955","ISSN":"1941-0042","issue":"1","issued":{"date-parts":[["2007",1]]},"page":"22-35","source":"IEEE Xplore","title":"Hypercomplex Fourier Transforms of Color Images","type":"article-journal","volume":"16"},
  {"id":"EulerAngles2020","abstract":"The Euler angles are three angles introduced by Leonhard Euler to describe the orientation of a rigid body with respect to a fixed coordinate system.They can also represent the orientation of a mobile frame of reference in physics or the orientation of a general basis in 3-dimensional linear algebra. Alternative forms were later introduced by Peter Guthrie Tait and George H. Bryan intended for use in aereonautics and engineering.","accessed":{"date-parts":[["2020",7,29]]},"citation-key":"EulerAngles2020","container-title":"Wikipedia","issued":{"date-parts":[["2020",7,19]]},"language":"en","license":"Creative Commons Attribution-ShareAlike License","note":"Page Version ID: 968482090","source":"Wikipedia","title":"Euler angles","type":"entry-encyclopedia","URL":"https://en.wikipedia.org/w/index.php?title=Euler_angles&oldid=968482090"},
  {"id":"fabbriCompressedVolumetricHeatmaps2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Fabbri","given":"Matteo"},{"family":"Lanzi","given":"Fabio"},{"family":"Calderara","given":"Simone"},{"family":"Alletto","given":"Stefano"},{"family":"Cucchiara","given":"Rita"}],"citation-key":"fabbriCompressedVolumetricHeatmaps2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"7204-7213","source":"openaccess.thecvf.com","title":"Compressed Volumetric Heatmaps for Multi-Person 3D Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Fabbri_Compressed_Volumetric_Heatmaps_for_Multi-Person_3D_Pose_Estimation_CVPR_2020_paper.html"},
  {"id":"fanARCTICDatasetDexterous2023","abstract":"Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC – a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. Our code and data are available at https://arctic.is.tue.mpg.de.","accessed":{"date-parts":[["2023",10,12]]},"author":[{"family":"Fan","given":"Zicong"},{"family":"Taheri","given":"Omid"},{"family":"Tzionas","given":"Dimitrios"},{"family":"Kocabas","given":"Muhammed"},{"family":"Kaufmann","given":"Manuel"},{"family":"Black","given":"Michael J."},{"family":"Hilliges","given":"Otmar"}],"citation-key":"fanARCTICDatasetDexterous2023","container-title":"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR52729.2023.01244","event-place":"Vancouver, BC, Canada","event-title":"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"979-8-3503-0129-8","issued":{"date-parts":[["2023",6]]},"language":"en","page":"12943-12954","publisher":"IEEE","publisher-place":"Vancouver, BC, Canada","source":"DOI.org (Crossref)","title":"ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation","title-short":"ARCTIC","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10203858/"},
  {"id":"fanAttentionBasedMultiviewReObservation2019","abstract":"Action recognition is an important and popular area in computer vision. Because of the helpfulness of action recognition of the skeleton and the development of related pose estimation techniques, action recognition based on skeleton data has drawn considerable attention and has been widely studied in recent years. In this paper, we propose an attention-based multiview re-observation fusion model for skeletal action recognition. The proposed model focuses on the factor of observation view of actions, which greatly inﬂuences action recognition. The model utilizes action information from multiple observation views to improve the recognition performance. In this method, we reobserve input skeleton data from several possible viewpoints, process these augmented observation data with a long shortterm memory (LSTM) network separately, and, ﬁnally, fuse the outputs to generate the ﬁnal recognition result. In the multiview fusion process, an attention mechanism is applied to regulate the fusion operation according to the helpfulness for the recognition of all views. In this way, the model can fuse information from multiple viewpoints to recognize actions and can learn to evaluate observation views to improve fusion performance. We also propose a multilayer feature attention method to improve the performance of the LSTM in our model. We utilize an attention mechanism to enhance the feature expression by ﬁnding and focusing on informative feature dimensions according to contextual action information. Moreover, we propose stacking multiple layers of attention operation in a multilayer LSTM network to further improve network performance. The ﬁnal model is integrated into an end-to-end trainable network. Experiments conducted on two popular datasets, NTU RGB+D and SBU Kinect interaction, show that our model achieves state-of-the-art performance.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Fan","given":"Zhaoxuan"},{"family":"Zhao","given":"Xu"},{"family":"Lin","given":"Tianwei"},{"family":"Su","given":"Haisheng"}],"citation-key":"fanAttentionBasedMultiviewReObservation2019","container-title":"IEEE Transactions on Multimedia","container-title-short":"IEEE Trans. Multimedia","DOI":"10.1109/TMM.2018.2859620","ISSN":"1520-9210, 1941-0077","issue":"2","issued":{"date-parts":[["2019",2]]},"language":"en","note":"ZSCC:00060","number":"2","page":"363-374","source":"DOI.org (Crossref)","title":"Attention-Based Multiview Re-Observation Fusion Network for Skeletal Action Recognition","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/8421041/","volume":"21"},
  {"id":"fangDPMBasedApproach2017","abstract":"Object detection and sub-category recognition play important roles in the ﬁeld of computer vision. Most of the existing approaches separate detection and recognition into two sequential parts. We argue, however, detection and recognition could share information of each other to achieve a better performance for both of them. In this paper, a new approach to joint detection and recognition based on Deformable Part Model (DPM) is presented. Our approach extends DPM from pure object detection to simultaneous detection and sub-category recognition. A multi-objective optimization function is formulated. It integrates supervised sub-category recognition into DPM training process, using structural SVM with latent variables. The experiments show that our approach achieves a very exciting result in a challenging vehicle data set.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Fang","given":"Liangji"},{"family":"Lin","given":"Tianwei"},{"family":"Wu","given":"Jun"},{"family":"Zhao","given":"Xu"}],"citation-key":"fangDPMBasedApproach2017","container-title":"2017 IEEE 7th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","DOI":"10.1109/CYBER.2017.8446308","event-place":"Honolulu, HI","event-title":"2017 IEEE 7th Annual International Conference on CYBER Technology in Automation, Control, and Intelligent Systems (CYBER)","ISBN":"978-1-5386-0490-8","issued":{"date-parts":[["2017",7]]},"language":"en","note":"ZSCC:00001","page":"164-168","publisher":"IEEE","publisher-place":"Honolulu, HI","source":"DOI.org (Crossref)","title":"A DPM Based Approach to Joint Object Detection and Sub-category Recognition","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8446308/"},
  {"id":"fangPuttingAnchorsEfficiently2019","abstract":"Anchor box mechanism is of vital importance for deep network based object detection. Current object detectors put anchors uniformly in the entire image, so the false positives along with training and testing costs are increased signiﬁcantly. In this work, we break through the homogeneity limitation in anchor putting by introducing geometric constraints for pedestrian detection. We ﬁrst deduce the relationship between the height of the pedestrian and its location in image plane using geometrical priors, for which we only need to know the rough relative height between camera and pedestrian. As a result, we narrow down the distribution space of anchors with a certain pixel height from 2D to 1D. In implementation, we propose a novel Geometric Constrained Loss, by which the new anchor mechanism is embedded into the deep learning architecture. To further remove false positives in inference, Geometric Constrained Suppression is introduced. Complemented with two eﬀective prediction modules, Dilated Double Shots Detector is designed to achieve better performance. Comprehensive experiments conducted on KITTI and Caltech datasets validate the superior performance and generalization ability of our method for both one-stage and two-stage methods, in both training and inference phases.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Fang","given":"Liangji"},{"family":"Zhao","given":"Xu"},{"family":"Song","given":"Xiao"},{"family":"Zhang","given":"Shiquan"},{"family":"Yang","given":"Ming"}],"citation-key":"fangPuttingAnchorsEfficiently2019","container-title":"Computer Vision – ACCV 2018","DOI":"10.1007/978-3-030-20873-8_25","event-place":"Cham","ISBN":"978-3-030-20872-1 978-3-030-20873-8","issued":{"date-parts":[["2019"]]},"language":"en","note":"ZSCC:00002","page":"387-403","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Putting the Anchors Efficiently: Geometric Constrained Pedestrian Detection","title-short":"Putting the Anchors Efficiently","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-030-20873-8_25","volume":"11365"},
  {"id":"fangReconstructing3DHuman2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Fang","given":"Qi"},{"family":"Shuai","given":"Qing"},{"family":"Dong","given":"Junting"},{"family":"Bao","given":"Hujun"},{"family":"Zhou","given":"Xiaowei"}],"citation-key":"fangReconstructing3DHuman2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"12814-12823","source":"openaccess.thecvf.com","title":"Reconstructing 3D Human Pose by Watching Humans in the Mirror","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Fang_Reconstructing_3D_Human_Pose_by_Watching_Humans_in_the_Mirror_CVPR_2021_paper.html"},
  {"id":"fangSmallobjectnessSensitiveDetection2019","abstract":"We present a small object sensitive method for object detection. Our method is built based on SSD (Single Shot MultiBox Detector (Liu et al. 2016)), a simple but effective deep neural network for image object detection. The discrete nature of anchor mechanism used in SSD, however, may cause misdetection for the small objects located at gaps between the anchor boxes. SSD performs better for small object detection after circular shifts of the input image. Therefore, auxiliary feature maps are generated by conducting circular shifts over lower extra feature maps in SSD for small-object detection, which is equivalent to shifting the objects in order to fit the locations of anchor boxes. We call our proposed system Shifted SSD. Moreover, pinpoint accuracy of localization is of vital importance to small objects detection. Hence, two novel methods called Smooth NMS and IoU-Prediction module are proposed to obtain more precise locations. Then for video sequences, we generate trajectory hypothesis to obtain predicted locations in a new frame for further improved performance. Experiments conducted on PASCAL VOC 2007, along with MS COCO, KITTI and our small object video datasets, validate that both mAP and recall are improved with different degrees and the speed is almost the same as SSD.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Fang","given":"Liangji"},{"family":"Zhao","given":"Xu"},{"family":"Zhang","given":"Shiquan"}],"citation-key":"fangSmallobjectnessSensitiveDetection2019","container-title":"Multimedia Tools and Applications","container-title-short":"Multimed Tools Appl","DOI":"10.1007/s11042-018-6227-7","ISSN":"1380-7501, 1573-7721","issue":"10","issued":{"date-parts":[["2019",5]]},"language":"en","note":"ZSCC:00020","number":"10","page":"13227-13245","source":"DOI.org (Crossref)","title":"Small-objectness sensitive detection based on shifted single shot detector","type":"article-journal","URL":"http://link.springer.com/10.1007/s11042-018-6227-7","volume":"78"},
  {"id":"fanOnlineApproachGesture2017","abstract":"Action recognition is an important research area in computer vision. Recently, the application of deep learning greatly promotes the development of action recognition. Many networks have achieved excellent performances on popular datasets. But there is still a gap between researches and real-world applications. In this paper, we propose an integrated approach for real-time online gesture recognition, trying to bring deep learning based action recognition methods into real-world applications. Our integrated approach mainly consists of three parts. (1) A gesture recognition network simpliﬁed from two-stream CNNs is trained on optical ﬂow images to recognize gestures. (2) To adapt to complicated and changeable real-world environments, target detection and tracking are applied to get a stable target bounding box to eliminate environment disturbances. (3) Improved optical ﬂow is introduced to remove global camera motion and get a better description of human motions, which improves gesture recognition performance signiﬁcantly. The integrated approach is tested on real-world datasets and achieves satisfying recognition performance, while guaranteeing a real-time processing speed.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Fan","given":"Zhaoxuan"},{"family":"Lin","given":"Tianwei"},{"family":"Zhao","given":"Xu"},{"family":"Jiang","given":"Wanli"},{"family":"Xu","given":"Tao"},{"family":"Yang","given":"Ming"}],"citation-key":"fanOnlineApproachGesture2017","container-title":"ICIG","DOI":"10.1007/978-3-319-71607-7_23","event-place":"Cham","ISBN":"978-3-319-71606-0 978-3-319-71607-7","issued":{"date-parts":[["2017"]]},"language":"en","note":"ZSCC:00008","page":"262-272","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"An Online Approach for Gesture Recognition Toward Real-World Applications","type":"paper-conference","URL":"https://link.springer.com/10.1007/978-3-319-71607-7_23"},
  {"id":"fanRevitalizingOptimization3D2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Fan","given":"Taosha"},{"family":"Alwala","given":"Kalyan Vasudev"},{"family":"Xiang","given":"Donglai"},{"family":"Xu","given":"Weipeng"},{"family":"Murphey","given":"Todd"},{"family":"Mukadam","given":"Mustafa"}],"citation-key":"fanRevitalizingOptimization3D2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11457-11466","source":"openaccess.thecvf.com","title":"Revitalizing Optimization for 3D Human Pose and Shape Estimation: A Sparse Constrained Formulation","title-short":"Revitalizing Optimization for 3D Human Pose and Shape Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Fan_Revitalizing_Optimization_for_3D_Human_Pose_and_Shape_Estimation_A_ICCV_2021_paper.html"},
  {"id":"fathyFundamentalMatrixEstimation2011","abstract":"The fundamental matrix (FM) describes the geometric relations that exist between two images of the same scene. Different error criteria are used for estimating FMs from an input set of correspondences. In this paper, the accuracy and efficiency aspects of the different error criteria were studied. We mathematically and experimentally proved that the most popular error criterion, the symmetric epipolar distance, is biased. It was also shown that despite the similarity between the algebraic expressions of the symmetric epipolar distance and Sampson distance, they have different accuracy properties. In addition, a new error criterion, Kanatani distance, was proposed and was proved to be the most effective for use during the outlier removal phase from accuracy and efficiency perspectives. To thoroughly test the accuracy of the different error criteria, we proposed a randomized algorithm for Reprojection Error-based Correspondence Generation (RE-CG). As input, RE-CG takes an FM and a desired reprojection error value $d$. As output, RE-CG generates a random correspondence having that error value. Mathematical analysis of this algorithm revealed that the success probability for any given trial is 1 - (2/3)^2 at best and is 1 - (6/7)^2 at worst while experiments demonstrated that the algorithm often succeeds after only one trial.","accessed":{"date-parts":[["2023",5,3]]},"author":[{"family":"Fathy","given":"Mohammed E."},{"family":"Hussein","given":"Ashraf S."},{"family":"Tolba","given":"Mohammed F."}],"citation-key":"fathyFundamentalMatrixEstimation2011","container-title":"Pattern Recognition Letters","container-title-short":"Pattern Recognition Letters","DOI":"10.1016/j.patrec.2010.09.019","ISSN":"01678655","issue":"2","issued":{"date-parts":[["2011",1]]},"page":"383-391","source":"arXiv.org","title":"Fundamental Matrix Estimation: A Study of Error Criteria","title-short":"Fundamental Matrix Estimation","type":"article-journal","URL":"http://arxiv.org/abs/1706.07886","volume":"32"},
  {"id":"fathyFundamentalMatrixEstimation2011a","abstract":"The fundamental matrix (FM) describes the geometric relations that exist between two images of the same scene. Different error criteria are used for estimating FMs from an input set of correspondences. In this paper, the accuracy and efficiency aspects of the different error criteria were studied. We mathematically and experimentally proved that the most popular error criterion, the symmetric epipolar distance, is biased. It was also shown that despite the similarity between the algebraic expressions of the symmetric epipolar distance and Sampson distance, they have different accuracy properties. In addition, a new error criterion, Kanatani distance, was proposed and was proved to be the most effective for use during the outlier removal phase from accuracy and efficiency perspectives. To thoroughly test the accuracy of the different error criteria, we proposed a randomized algorithm for Reprojection Error-based Correspondence Generation (RE-CG). As input, RE-CG takes an FM and a desired reprojection error value $d$. As output, RE-CG generates a random correspondence having that error value. Mathematical analysis of this algorithm revealed that the success probability for any given trial is 1 - (2/3)^2 at best and is 1 - (6/7)^2 at worst while experiments demonstrated that the algorithm often succeeds after only one trial.","accessed":{"date-parts":[["2023",4,28]]},"author":[{"family":"Fathy","given":"Mohammed E."},{"family":"Hussein","given":"Ashraf S."},{"family":"Tolba","given":"Mohammed F."}],"citation-key":"fathyFundamentalMatrixEstimation2011a","container-title":"Pattern Recognition Letters","container-title-short":"Pattern Recognition Letters","DOI":"10.1016/j.patrec.2010.09.019","ISSN":"01678655","issue":"2","issued":{"date-parts":[["2011",1]]},"page":"383-391","source":"arXiv.org","title":"Fundamental Matrix Estimation: A Study of Error Criteria","title-short":"Fundamental Matrix Estimation","type":"article-journal","URL":"http://arxiv.org/abs/1706.07886","volume":"32"},
  {"id":"felsbergOptimizedFastAlgorithms1999","abstract":"In this article, we deal with fast algorithms for the quaternionic Fourier transform (QFT). Our aim is to give a guideline for choosing algorithms in practical cases. Hence, we are not only interested in the theoretic complexity but in the real execution time of the implementation of an algorithm. This includes oating point multiplications, additions, index computations and the memory accesses. We mainly consider two cases: the QFT of a real signal and the QFT of a quaternionic signal. For both cases it follows that the row-column method yields very fast algorithms. Additionally, these algorithms are easy to implement since one can fall back on standard algorithms for the fast Fourier transform and the fast Hartley transform. The latter is the optimal choice for real signals since there is no redundancy in the transform. We take advantage of the fact that each complete transform can be converted into another complete transform. In the case of the complex Fourier transform, the Hartley transform, and the QFT, the conversions are of low complexity. Hence, the QFT of a real signal is optimally calculated using the Hartley transform.","accessed":{"date-parts":[["2020",8,20]]},"author":[{"family":"Felsberg","given":"Michael"},{"family":"Sommer","given":"Gerald"}],"citation-key":"felsbergOptimizedFastAlgorithms1999","collection-editor":[{"family":"Goos","given":"Gerhard"},{"family":"Hartmanis","given":"Juris"},{"family":"Leeuwen","given":"Jan","non-dropping-particle":"van"}],"container-title":"Computer Analysis of Images and Patterns","DOI":"10.1007/3-540-48375-6_26","editor":[{"family":"Solina","given":"Franc"},{"family":"Leonardis","given":"Alešs"}],"event-place":"Berlin, Heidelberg","ISBN":"978-3-540-66366-9 978-3-540-48375-5","issued":{"date-parts":[["1999"]]},"language":"en","page":"209-216","publisher":"Springer Berlin Heidelberg","publisher-place":"Berlin, Heidelberg","source":"DOI.org (Crossref)","title":"Optimized Fast Algorithms for the Quaternionic Fourier Transform","type":"chapter","URL":"http://link.springer.com/10.1007/3-540-48375-6_26","volume":"1689"},
  {"id":"felzenszwalbPictorialStructuresObject2005","abstract":"In this paper we present a computationally efficient framework for part-based modeling and recognition of objects. Our work is motivated by the pictorial structure models introduced by Fischler and Elschlager. The basic idea is to represent an object by a collection of parts arranged in a deformable configuration. The appearance of each part is modeled separately, and the deformable configuration is represented by spring-like connections between pairs of parts. These models allow for qualitative descriptions of visual appearance, and are suitable for generic recognition problems. We address the problem of using pictorial structure models to find instances of an object in an image as well as the problem of learning an object model from training examples, presenting efficient algorithms in both cases. We demonstrate the techniques by learning models that represent faces and human bodies and using the resulting models to locate the corresponding objects in novel images.","accessed":{"date-parts":[["2021",6,1]]},"author":[{"family":"Felzenszwalb","given":"Pedro F."},{"family":"Huttenlocher","given":"Daniel P."}],"citation-key":"felzenszwalbPictorialStructuresObject2005","container-title":"International Journal of Computer Vision","container-title-short":"International Journal of Computer Vision","DOI":"10.1023/B:VISI.0000042934.15159.49","ISSN":"1573-1405","issue":"1","issued":{"date-parts":[["2005",1,1]]},"language":"en","page":"55-79","source":"Springer Link","title":"Pictorial Structures for Object Recognition","type":"article-journal","URL":"https://doi.org/10.1023/B:VISI.0000042934.15159.49","volume":"61"},
  {"id":"fengDiffPoseSpatioTemporalDiffusion2023","abstract":"Denoising diffusion probabilistic models that were initially proposed for realistic image generation have recently shown success in various perception tasks (e.g., object detection and image segmentation) and are increasingly gaining attention in computer vision. However, extending such models to multi-frame human pose estimation is non-trivial due to the presence of the additional temporal dimension in videos. More importantly, learning representations that focus on keypoint regions is crucial for accurate localization of human joints. Nevertheless, the adaptation of the diffusion-based methods remains unclear on how to achieve such objective. In this paper, we present DiffPose, a novel diffusion architecture that formulates video-based human pose estimation as a conditional heatmap generation problem. First, to better leverage temporal information, we propose SpatioTemporal Representation Learner which aggregates visual evidences across frames and uses the resulting features in each denoising step as a condition. In addition, we present a mechanism called Lookup-based MultiScale Feature Interaction that determines the correlations between local joints and global contexts across multiple scales. This mechanism generates delicate representations that focus on keypoint regions. Altogether, by extending diffusion models, we show two unique characteristics from DiffPose on pose estimation task: (i) the ability to combine multiple sets of pose estimates to improve prediction accuracy, particularly for challenging joints, and (ii) the ability to adjust the number of iterative steps for feature refinement without retraining the model. DiffPose sets new state-of-the-art results on three benchmarks: PoseTrack2017, PoseTrack2018, and PoseTrack21.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Feng","given":"Runyang"},{"family":"Gao","given":"Yixing"},{"family":"Tse","given":"Tze Ho Elden"},{"family":"Ma","given":"Xueqing"},{"family":"Chang","given":"Hyung Jin"}],"citation-key":"fengDiffPoseSpatioTemporalDiffusion2023","DOI":"10.48550/arXiv.2307.16687","issued":{"date-parts":[["2023",8,5]]},"number":"arXiv:2307.16687","publisher":"arXiv","source":"arXiv.org","title":"DiffPose: SpatioTemporal Diffusion Model for Video-Based Human Pose Estimation","title-short":"DiffPose","type":"article","URL":"http://arxiv.org/abs/2307.16687"},
  {"id":"fengMutualInformationBasedTemporal2023","accessed":{"date-parts":[["2023",6,10]]},"author":[{"family":"Feng","given":"Runyang"},{"family":"Gao","given":"Yixing"},{"family":"Ma","given":"Xueqing"},{"family":"Tse","given":"Tze Ho Elden"},{"family":"Chang","given":"Hyung Jin"}],"citation-key":"fengMutualInformationBasedTemporal2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"17131-17141","source":"openaccess.thecvf.com","title":"Mutual Information-Based Temporal Difference Learning for Human Pose Estimation in Video","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Feng_Mutual_Information-Based_Temporal_Difference_Learning_for_Human_Pose_Estimation_in_CVPR_2023_paper.html"},
  {"id":"fischlerRepresentationMatchingPictorial1973","abstract":"The primary problem dealt with in this paper is the following. Given some description of a visual object, find that object in an actual photograph. Part of the solution to this problem is the specification of a descriptive scheme, and a metric on which to base the decision of \"goodness\" of matching or detection.","author":[{"family":"Fischler","given":"M. A."},{"family":"Elschlager","given":"R. A."}],"citation-key":"fischlerRepresentationMatchingPictorial1973","container-title":"IEEE Transactions on Computers","DOI":"10.1109/T-C.1973.223602","ISSN":"1557-9956","issue":"1","issued":{"date-parts":[["1973",1]]},"page":"67-92","source":"IEEE Xplore","title":"The Representation and Matching of Pictorial Structures","type":"article-journal","volume":"C-22"},
  {"id":"fuContextAwareModel2015","abstract":"Simple tree model prevails for 2D pose estimation for its simplicity and efficiency. However, the limited kinetic constraints often lead to double-counting and damage the accuracy of leaf parts, and this is largely ignored in previous work. In this paper, we propose a novel enhanced tree model which incorporates both local kinetic constraints and global contextual constraints among non-adjacent parts. By introducing virtual parts, we are able to model richer constraints within a tree structure and dynamic programming can be utilized for efficient inference. Experiments on public benchmarks show that our method is more effective in tackling double counting problem and can improve the localization accuracy, especially for the challenging lower limbs.","author":[{"family":"Fu","given":"Lianrui"},{"family":"Zhang","given":"Junge"},{"family":"Huang","given":"Kaiqi"}],"citation-key":"fuContextAwareModel2015","container-title":"2015 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2015.7350948","event-title":"2015 IEEE International Conference on Image Processing (ICIP)","issued":{"date-parts":[["2015",9]]},"page":"991-995","source":"IEEE Xplore","title":"Context aware model for articulated human pose estimation","type":"paper-conference"},
  {"id":"gai3DBodyPose2019","abstract":"The estimation of 3D body pose and shape has always been a challenging problem due to various reasons, such as the ambiguity in 2D images and complex articulated structure of the human body. In order to solve the ill-conditioned problems, in this paper, we bring up an end-to-end method to estimate 3D human shape and pose from multi-view RGB images. In the proposed framework, we ﬁrst implement a CNN embedded with attention module to extract the image feature and design the view-pooling layer to combine the features from multiple views. Then we adopt a regression network with a novel geometric constraint of body limbs to estimate 3D human pose and shape. Additionally, during the training process, we employ the idea of adversarial learning in our model to help regress accurate pose and shape parameters. Extensive experiments are conducted on Human3.6M and MPI-INF-3DHP datasets, and our method achieves competitive results in the 3D pose and shape estimation task.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Gai","given":"Zixuan"},{"family":"Zhao","given":"Xu"},{"family":"Cao","given":"Xin"}],"citation-key":"gai3DBodyPose2019","container-title":"2019 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2019.8804203","event-place":"Taipei, Taiwan","event-title":"2019 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-5386-6249-6","issued":{"date-parts":[["2019",9]]},"language":"en","note":"ZSCC:00002","page":"574-578","publisher":"IEEE","publisher-place":"Taipei, Taiwan","source":"DOI.org (Crossref)","title":"3D Body Pose and Shape Estimation from Multi-View Images With Limb Geometric Constraint","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8804203/"},
  {"id":"gallMotionCaptureUsing2009","abstract":"This paper proposes a method for capturing the performance of a human or an animal from a multi-view video sequence. Given an articulated template model and silhouettes from a multi-view image sequence, our approach recovers not only the movement of the skeleton, but also the possibly non-rigid temporal deformation of the 3D surface. While large scale deformations or fast movements are captured by the skeleton pose and approximate surface skinning, true small scale deformations or non-rigid garment motion are captured by fitting the surface to the silhouette. We further propose a novel optimization scheme for skeleton-based pose estimation that exploits the skeleton's tree structure to split the optimization problem into a local one and a lower dimensional global one. We show on various sequences that our approach can capture the 3D motion of animals and humans accurately even in the case of rapid movements and wide apparel like skirts.","author":[{"family":"Gall","given":"Juergen"},{"family":"Stoll","given":"Carsten"},{"family":"Aguiar","given":"Edilson","non-dropping-particle":"de"},{"family":"Theobalt","given":"Christian"},{"family":"Rosenhahn","given":"Bodo"},{"family":"Seidel","given":"Hans-Peter"}],"citation-key":"gallMotionCaptureUsing2009","container-title":"2009 IEEE Conference on Computer Vision and Pattern Recognition","DOI":"10.1109/CVPR.2009.5206755","event-title":"2009 IEEE Conference on Computer Vision and Pattern Recognition","ISSN":"1063-6919","issued":{"date-parts":[["2009",6]]},"page":"1746-1753","source":"IEEE Xplore","title":"Motion capture using joint skeleton tracking and surface estimation","type":"paper-conference"},
  {"id":"ganRobustBinocularPose2015","abstract":"In this paper, an accurate and robust pose estimation algorithm for binocular camera systems is proposed based on pigeon-inspired optimization (PIO), which can be easily generalized to estimation for multiple camera systems. In this method, the information of both cameras is completely used, and the poses of them can be determined accurately at the same time. Pigeon-inspired optimization is a new and efficient evolutionary algorithm, which is employed to select the optimal rotation axis in our method. The robustness and high accuracy of the proposed method has been demonstrated by three cases of experiments over some state-of-the-art pose estimation methods.","author":[{"family":"Gan","given":"L."},{"family":"Duan","given":"H."}],"citation-key":"ganRobustBinocularPose2015","container-title":"2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA)","DOI":"10.1109/ICIEA.2015.7334261","event-title":"2015 IEEE 10th Conference on Industrial Electronics and Applications (ICIEA)","issued":{"date-parts":[["2015",6]]},"page":"1043-1048","source":"IEEE Xplore","title":"Robust binocular pose estimation based on pigeon-inspired optimization","type":"paper-conference"},
  {"id":"GaoSiHunHeMoXingDaoYinXiaDeSanWeiRenTiYunDongGenZong2006","citation-key":"GaoSiHunHeMoXingDaoYinXiaDeSanWeiRenTiYunDongGenZong2006","container-title":"NCIG","issued":{"date-parts":[["2006"]]},"title":"高斯混合模型导引下的三维人体运动跟踪","type":"paper-conference"},
  {"id":"garauDECADeepViewpointEquivariant2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Garau","given":"Nicola"},{"family":"Bisagno","given":"Niccolò"},{"family":"Bródka","given":"Piotr"},{"family":"Conci","given":"Nicola"}],"citation-key":"garauDECADeepViewpointEquivariant2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11677-11686","source":"openaccess.thecvf.com","title":"DECA: Deep Viewpoint-Equivariant Human Pose Estimation Using Capsule Autoencoders","title-short":"DECA","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Garau_DECA_Deep_Viewpoint-Equivariant_Human_Pose_Estimation_Using_Capsule_Autoencoders_ICCV_2021_paper.html"},
  {"id":"gartnerDifferentiableDynamicsArticulated2022","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Gärtner","given":"Erik"},{"family":"Andriluka","given":"Mykhaylo"},{"family":"Coumans","given":"Erwin"},{"family":"Sminchisescu","given":"Cristian"}],"citation-key":"gartnerDifferentiableDynamicsArticulated2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13190-13200","source":"openaccess.thecvf.com","title":"Differentiable Dynamics for Articulated 3D Human Motion Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Gartner_Differentiable_Dynamics_for_Articulated_3D_Human_Motion_Reconstruction_CVPR_2022_paper.html"},
  {"id":"gartnerTrajectoryOptimizationPhysicsBased2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Gärtner","given":"Erik"},{"family":"Andriluka","given":"Mykhaylo"},{"family":"Xu","given":"Hongyi"},{"family":"Sminchisescu","given":"Cristian"}],"citation-key":"gartnerTrajectoryOptimizationPhysicsBased2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13106-13115","source":"openaccess.thecvf.com","title":"Trajectory Optimization for Physics-Based Reconstruction of 3D Human Pose From Monocular Video","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Gartner_Trajectory_Optimization_for_Physics-Based_Reconstruction_of_3D_Human_Pose_From_CVPR_2022_paper.html"},
  {"id":"gengBottomHumanPoseEstimation2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Geng","given":"Zigang"},{"family":"Sun","given":"Ke"},{"family":"Xiao","given":"Bin"},{"family":"Zhang","given":"Zhaoxiang"},{"family":"Wang","given":"Jingdong"}],"citation-key":"gengBottomHumanPoseEstimation2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"14676-14686","source":"openaccess.thecvf.com","title":"Bottom-Up Human Pose Estimation via Disentangled Keypoint Regression","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Geng_Bottom-Up_Human_Pose_Estimation_via_Disentangled_Keypoint_Regression_CVPR_2021_paper.html"},
  {"id":"gengBottomHumanPoseEstimation2021a","accessed":{"date-parts":[["2021",8,25]]},"author":[{"family":"Geng","given":"Zigang"},{"family":"Sun","given":"Ke"},{"family":"Xiao","given":"Bin"},{"family":"Zhang","given":"Zhaoxiang"},{"family":"Wang","given":"Jingdong"}],"citation-key":"gengBottomHumanPoseEstimation2021a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"14676-14686","source":"openaccess.thecvf.com","title":"Bottom-Up Human Pose Estimation via Disentangled Keypoint Regression","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Geng_Bottom-Up_Human_Pose_Estimation_via_Disentangled_Keypoint_Regression_CVPR_2021_paper.html"},
  {"id":"gengHumanPoseCompositional2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Geng","given":"Zigang"},{"family":"Wang","given":"Chunyu"},{"family":"Wei","given":"Yixuan"},{"family":"Liu","given":"Ze"},{"family":"Li","given":"Houqiang"},{"family":"Hu","given":"Han"}],"citation-key":"gengHumanPoseCompositional2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"660-671","source":"openaccess.thecvf.com","title":"Human Pose As Compositional Tokens","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Geng_Human_Pose_As_Compositional_Tokens_CVPR_2023_paper.html"},
  {"id":"ghafoorQuantificationOcclusionHandling2022","abstract":"3D human pose estimation using monocular images is an important yet challenging task. Existing 3D pose detection methods exhibit excellent performance under normal conditions however their performance may degrade due to occlusion. Recently some occlusion aware methods have also been proposed however, the occlusion handling capability of these networks has not yet been thoroughly investigated. In the current work, we propose an occlusion-guided 3D human pose estimation framework and quantify its occlusion handling capability by using different protocols. The proposed method estimates more accurate 3D human poses using 2D skeletons with missing joints as input. Missing joints are handled by introducing occlusion guidance that provides extra information about the absence or presence of a joint. Temporal information has also been exploited to better estimate the missing joints. A large number of experiments are performed for the quantification of occlusion handling capability of the proposed method on three publicly available datasets in various settings including random missing joints, fixed body parts missing, and complete frames missing using mean per joint position error criterion. In addition to that, the quality of the predicted 3D poses is also evaluated using action classification performance as a criterion. 3D poses estimated by the proposed method achieved significantly improved action recognition performance in the presence of missing joints. Our experiments demonstrate the effectiveness of the proposed framework for handling the missing joints as well as quantification of the occlusion handling capability of the deep neural networks.","author":[{"family":"Ghafoor","given":"Mehwish"},{"family":"Mahmood","given":"Arif"}],"citation-key":"ghafoorQuantificationOcclusionHandling2022","container-title":"IEEE Transactions on Multimedia","DOI":"10.1109/TMM.2022.3158068","ISSN":"1941-0077","issued":{"date-parts":[["2022"]]},"page":"1-1","source":"IEEE Xplore","title":"Quantification of Occlusion Handling Capability of 3D Human Pose Estimation Framework","type":"article-journal"},
  {"id":"ghafoorQuantificationOcclusionHandling2023","abstract":"3D human pose estimation using monocular images is an important yet challenging task. Existing 3D pose detection methods exhibit excellent performance under normal conditions however their performance may degrade due to occlusion. Recently some occlusion aware methods have also been proposed, however, the occlusion handling capability of these networks has not yet been thoroughly investigated. In the current work, we propose an occlusion-guided 3D human pose estimation framework and quantify its occlusion handling capability by using different protocols. The proposed method estimates more accurate 3D human poses using 2D skeletons with missing joints as input. Missing joints are handled by introducing occlusion guidance that provides extra information about the absence or presence of a joint. Temporal information has also been exploited to better estimate the missing joints. A large number of experiments are performed for the quantification of occlusion handling capability of the proposed method on three publicly available datasets in various settings including random missing joints, fixed body parts missing, and complete frames missing, using mean per joint position error criterion. In addition to that, the quality of the predicted 3D poses is also evaluated using action classification performance as a criterion. 3D poses estimated by the proposed method achieved significantly improved action recognition performance in the presence of missing joints. Our experiments demonstrate the effectiveness of the proposed framework for handling the missing joints as well as quantification of the occlusion handling capability of the deep neural networks.","accessed":{"date-parts":[["2023",12,4]]},"author":[{"family":"Ghafoor","given":"Mehwish"},{"family":"Mahmood","given":"Arif"}],"citation-key":"ghafoorQuantificationOcclusionHandling2023","container-title":"IEEE Transactions on Multimedia","DOI":"10.1109/TMM.2022.3158068","ISSN":"1941-0077","issued":{"date-parts":[["2023"]]},"page":"3311-3318","source":"IEEE Xplore","title":"Quantification of Occlusion Handling Capability of a 3D Human Pose Estimation Framework","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9732169","volume":"25"},
  {"id":"gholamiAdaptPoseCrossDatasetAdaptation2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Gholami","given":"Mohsen"},{"family":"Wandt","given":"Bastian"},{"family":"Rhodin","given":"Helge"},{"family":"Ward","given":"Rabab"},{"family":"Wang","given":"Z. Jane"}],"citation-key":"gholamiAdaptPoseCrossDatasetAdaptation2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13075-13085","source":"openaccess.thecvf.com","title":"AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation by Learnable Motion Generation","title-short":"AdaptPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Gholami_AdaptPose_Cross-Dataset_Adaptation_for_3D_Human_Pose_Estimation_by_Learnable_CVPR_2022_paper.html"},
  {"id":"gholamiAdaptPoseCrossDatasetAdaptation2022a","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Gholami","given":"Mohsen"},{"family":"Wandt","given":"Bastian"},{"family":"Rhodin","given":"Helge"},{"family":"Ward","given":"Rabab"},{"family":"Wang","given":"Z. Jane"}],"citation-key":"gholamiAdaptPoseCrossDatasetAdaptation2022a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13075-13085","source":"openaccess.thecvf.com","title":"AdaptPose: Cross-Dataset Adaptation for 3D Human Pose Estimation by Learnable Motion Generation","title-short":"AdaptPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Gholami_AdaptPose_Cross-Dataset_Adaptation_for_3D_Human_Pose_Estimation_by_Learnable_CVPR_2022_paper.html"},
  {"id":"goldfarbNumericallyStableDual1983","accessed":{"date-parts":[["2022",4,13]]},"author":[{"family":"Goldfarb","given":"D."},{"family":"Idnani","given":"A."}],"citation-key":"goldfarbNumericallyStableDual1983","container-title":"Mathematical Programming","container-title-short":"Mathematical Programming","DOI":"10.1007/BF02591962","ISSN":"0025-5610, 1436-4646","issue":"1","issued":{"date-parts":[["1983",9]]},"language":"en","page":"1-33","source":"DOI.org (Crossref)","title":"A numerically stable dual method for solving strictly convex quadratic programs","type":"article-journal","URL":"http://link.springer.com/10.1007/BF02591962","volume":"27"},
  {"id":"gongPoseAugDifferentiablePose2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Gong","given":"Kehong"},{"family":"Zhang","given":"Jianfeng"},{"family":"Feng","given":"Jiashi"}],"citation-key":"gongPoseAugDifferentiablePose2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"8575-8584","source":"openaccess.thecvf.com","title":"PoseAug: A Differentiable Pose Augmentation Framework for 3D Human Pose Estimation","title-short":"PoseAug","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Gong_PoseAug_A_Differentiable_Pose_Augmentation_Framework_for_3D_Human_Pose_CVPR_2021_paper.html"},
  {"id":"gongPoseTripletCoEvolving3D2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Gong","given":"Kehong"},{"family":"Li","given":"Bingbing"},{"family":"Zhang","given":"Jianfeng"},{"family":"Wang","given":"Tao"},{"family":"Huang","given":"Jing"},{"family":"Mi","given":"Michael Bi"},{"family":"Feng","given":"Jiashi"},{"family":"Wang","given":"Xinchao"}],"citation-key":"gongPoseTripletCoEvolving3D2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"11017-11027","source":"openaccess.thecvf.com","title":"PoseTriplet: Co-Evolving 3D Human Pose Estimation, Imitation, and Hallucination Under Self-Supervision","title-short":"PoseTriplet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Gong_PoseTriplet_Co-Evolving_3D_Human_Pose_Estimation_Imitation_and_Hallucination_Under_CVPR_2022_paper.html"},
  {"id":"gorryGeneralLeastsquaresSmoothing1990","accessed":{"date-parts":[["2021",3,7]]},"author":[{"family":"Gorry","given":"Peter A."}],"citation-key":"gorryGeneralLeastsquaresSmoothing1990","container-title":"Analytical Chemistry","container-title-short":"Anal. Chem.","DOI":"10.1021/ac00205a007","ISSN":"0003-2700, 1520-6882","issue":"6","issued":{"date-parts":[["1990",3,15]]},"language":"en","page":"570-573","source":"DOI.org (Crossref)","title":"General least-squares smoothing and differentiation by the convolution (Savitzky-Golay) method","type":"article-journal","URL":"https://pubs.acs.org/doi/abs/10.1021/ac00205a007","volume":"62"},
  {"id":"gulerDensePoseDenseHuman2018","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Güler","given":"Rıza Alp"},{"family":"Neverova","given":"Natalia"},{"family":"Kokkinos","given":"Iasonas"}],"citation-key":"gulerDensePoseDenseHuman2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"7297-7306","source":"openaccess.thecvf.com","title":"DensePose: Dense Human Pose Estimation in the Wild","title-short":"DensePose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Guler_DensePose_Dense_Human_CVPR_2018_paper.html"},
  {"id":"guRemovingBiasIntegral2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Gu","given":"Kerui"},{"family":"Yang","given":"Linlin"},{"family":"Yao","given":"Angela"}],"citation-key":"guRemovingBiasIntegral2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11067-11076","source":"openaccess.thecvf.com","title":"Removing the Bias of Integral Pose Regression","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Gu_Removing_the_Bias_of_Integral_Pose_Regression_ICCV_2021_paper.html"},
  {"id":"guzovHumanPOSEitioningSystem2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Guzov","given":"Vladimir"},{"family":"Mir","given":"Aymen"},{"family":"Sattler","given":"Torsten"},{"family":"Pons-Moll","given":"Gerard"}],"citation-key":"guzovHumanPOSEitioningSystem2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"4318-4329","source":"openaccess.thecvf.com","title":"Human POSEitioning System (HPS): 3D Human Pose Estimation and Self-Localization in Large Scenes From Body-Mounted Sensors","title-short":"Human POSEitioning System (HPS)","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Guzov_Human_POSEitioning_System_HPS_3D_Human_Pose_Estimation_and_Self-Localization_CVPR_2021_paper.html"},
  {"id":"habibieWildHumanPose2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Habibie","given":"Ikhsanul"},{"family":"Xu","given":"Weipeng"},{"family":"Mehta","given":"Dushyant"},{"family":"Pons-Moll","given":"Gerard"},{"family":"Theobalt","given":"Christian"}],"citation-key":"habibieWildHumanPose2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"10905-10914","source":"openaccess.thecvf.com","title":"In the Wild Human Pose Estimation Using Explicit 2D Features and Intermediate 3D Representations","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Habibie_In_the_Wild_Human_Pose_Estimation_Using_Explicit_2D_Features_CVPR_2019_paper.html"},
  {"id":"handrich3DHumanPose2018","abstract":"Random Tree Walkers (RTW) are a well-established method for human pose estimation, because they deliver state-of-the-art performance at low computational cost. As the forests capabilities for generalization are limited, the algorithm fails to estimate unlearned poses very quickly. The proposed method pushes this limitation by combining the RTW with optimization methods such as iterative closest point (ICP) and a stochastic search. The RTW is being used to initialize various hypotheses in different ways which are then passed to the optimization stage of the proposed method. The quality of each hypothesis is assessed by a cost function measuring the discrepancy between the data and a human body model generated for each hypothesis. Experimental results show a greater number of correctly estimated poses over a single RTW result.","author":[{"family":"Handrich","given":"Sebastian"},{"family":"Waxweiler","given":"Philipp"},{"family":"Werner","given":"Philipp"},{"family":"Al-Hamadi","given":"Ayoub"}],"citation-key":"handrich3DHumanPose2018","container-title":"2018 25th IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2018.8451427","event-title":"2018 25th IEEE International Conference on Image Processing (ICIP)","ISSN":"2381-8549","issued":{"date-parts":[["2018",10]]},"page":"555-559","source":"IEEE Xplore","title":"3D Human Pose Estimation Using Stochastic Optimization in Real Time","type":"paper-conference"},
  {"id":"hareshArticulated3DHumanObject2022","abstract":"Human-object interactions with articulated objects are common in everyday life. Despite much progress in single-view 3D reconstruction, it is still challenging to infer an articulated 3D object model from an RGB video showing a person manipulating the object. We canonicalize the task of articulated 3D human-object interaction reconstruction from RGB video, and carry out a systematic benchmark of five families of methods for this task: 3D plane estimation, 3D cuboid estimation, CAD model fitting, implicit field fitting, and free-form mesh fitting. Our experiments show that all methods struggle to obtain high accuracy results even when provided ground truth information about the observed objects. We identify key factors which make the task challenging and suggest directions for future work on this challenging 3D computer vision task. Short video summary at https://www.youtube.com/watch?v=5tAlKBojZwc","accessed":{"date-parts":[["2023",8,28]]},"author":[{"family":"Haresh","given":"Sanjay"},{"family":"Sun","given":"Xiaohao"},{"family":"Jiang","given":"Hanxiao"},{"family":"Chang","given":"Angel X."},{"family":"Savva","given":"Manolis"}],"citation-key":"hareshArticulated3DHumanObject2022","issued":{"date-parts":[["2022",9,12]]},"number":"arXiv:2209.05612","publisher":"arXiv","source":"arXiv.org","title":"Articulated 3D Human-Object Interactions from RGB Videos: An Empirical Analysis of Approaches and Challenges","title-short":"Articulated 3D Human-Object Interactions from RGB Videos","type":"article","URL":"http://arxiv.org/abs/2209.05612"},
  {"id":"hartleyMultipleViewGeometry","author":[{"family":"Hartley","given":"Richard"},{"family":"Zisserman","given":"Andrew"}],"citation-key":"hartleyMultipleViewGeometry","language":"en","page":"673","source":"Zotero","title":"Multiple View Geometry in Computer Vision, Second Edition","type":"article-journal"},
  {"id":"hartleyTriangulation1997","abstract":"In this paper, we consider the problem of finding the position of a point in space given its position in two images taken with cameras with known calibration and pose. This process requires the intersection of two known rays in space and is commonly known as triangulation. In the absence of noise, this problem is trivial. When noise is present, the two rays will not generally meet, in which case it is necessary to find the best point of intersection. This problem is especially critical in affine and projective reconstruction in which there is no meaningful metric information about the object space. It is desirable to find a triangulation method that is invariant to projective transformations of space. This paper solves that problem by assuming a Gaussian noise model for perturbation of the image coordinates. The triangulation problem may then be formulated as a least-squares minimization problem. In this paper a noniterative solution is given that finds the global minimum. It is shown that in certain configurations, local minima occur, which are avoided by the new method. Extensive comparisons of the new method with several other methods show that it consistently gives superior results.","author":[{"family":"Hartley","given":"Richard I."},{"family":"Sturm","given":"Peter"}],"citation-key":"hartleyTriangulation1997","container-title":"Computer Vision and Image Understanding","DOI":"https://doi.org/10.1006/cviu.1997.0547","ISSN":"1077-3142","issue":"2","issued":{"date-parts":[["1997"]]},"page":"146-157","title":"Triangulation","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1077314297905476","volume":"68"},
  {"id":"hartleyTriangulation1997a","abstract":"In this paper, we consider the problem of finding the position of a point in space given its position in two images taken with cameras with known calibration and pose. This process requires the intersection of two known rays in space and is commonly known as triangulation. In the absence of noise, this problem is trivial. When noise is present, the two rays will not generally meet, in which case it is necessary to find the best point of intersection. This problem is especially critical in affine and projective reconstruction in which there is no meaningful metric information about the object space. It is desirable to find a triangulation method that is invariant to projective transformations of space. This paper solves that problem by assuming a Gaussian noise model for perturbation of the image coordinates. The triangulation problem may then be formulated as a least-squares minimization problem. In this paper a noniterative solution is given that finds the global minimum. It is shown that in certain configurations, local minima occur, which are avoided by the new method. Extensive comparisons of the new method with several other methods show that it consistently gives superior results.","accessed":{"date-parts":[["2021",1,2]]},"author":[{"family":"Hartley","given":"Richard I."},{"family":"Sturm","given":"Peter"}],"citation-key":"hartleyTriangulation1997a","container-title":"Computer Vision and Image Understanding","container-title-short":"Computer Vision and Image Understanding","DOI":"10.1006/cviu.1997.0547","ISSN":"1077-3142","issue":"2","issued":{"date-parts":[["1997",11,1]]},"language":"en","page":"146-157","source":"ScienceDirect","title":"Triangulation","type":"article-journal","URL":"http://www.sciencedirect.com/science/article/pii/S1077314297905476","volume":"68"},
  {"id":"HarvestingMultipleViews","accessed":{"date-parts":[["2022",1,14]]},"citation-key":"HarvestingMultipleViews","title":"Harvesting Multiple Views for Marker-Less 3D Human Pose Annotations | IEEE Conference Publication | IEEE Xplore","type":"webpage","URL":"https://ieeexplore.ieee.org/document/8099621"},
  {"id":"hassanResolving3DHuman2019","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Hassan","given":"Mohamed"},{"family":"Choutas","given":"Vasileios"},{"family":"Tzionas","given":"Dimitrios"},{"family":"Black","given":"Michael J."}],"citation-key":"hassanResolving3DHuman2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"2282-2292","source":"openaccess.thecvf.com","title":"Resolving 3D Human Pose Ambiguities With 3D Scene Constraints","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Hassan_Resolving_3D_Human_Pose_Ambiguities_With_3D_Scene_Constraints_ICCV_2019_paper.html"},
  {"id":"hassonLearningJointReconstruction2019","abstract":"Estimating hand-object manipulations is essential for interpreting and imitating human actions. Previous work has made signiﬁcant progress towards reconstruction of hand poses and object shapes in isolation. Yet, reconstructing hands and objects during manipulation is a more challenging task due to signiﬁcant occlusions of both the hand and object. While presenting challenges, manipulations may also simplify the problem since the physics of contact restricts the space of valid hand-object conﬁgurations. For example, during manipulation, the hand and object should be in contact but not interpenetrate. In this work, we regularize the joint reconstruction of hands and objects with manipulation constraints. We present an end-to-end learnable model that exploits a novel contact loss that favors physically plausible hand-object constellations. Our approach improves grasp quality metrics over baselines, using RGB images as input. To train and evaluate the model, we also propose a new large-scale synthetic dataset, ObMan, with hand-object manipulations. We demonstrate the transferability of ObMan-trained models to real data.","accessed":{"date-parts":[["2023",9,29]]},"author":[{"family":"Hasson","given":"Yana"},{"family":"Varol","given":"Gul"},{"family":"Tzionas","given":"Dimitrios"},{"family":"Kalevatykh","given":"Igor"},{"family":"Black","given":"Michael J."},{"family":"Laptev","given":"Ivan"},{"family":"Schmid","given":"Cordelia"}],"citation-key":"hassonLearningJointReconstruction2019","container-title":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2019.01208","event-place":"Long Beach, CA, USA","event-title":"2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-7281-3293-8","issued":{"date-parts":[["2019",6]]},"language":"en","page":"11799-11808","publisher":"IEEE","publisher-place":"Long Beach, CA, USA","source":"DOI.org (Crossref)","title":"Learning Joint Reconstruction of Hands and Manipulated Objects","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8954029/"},
  {"id":"heEpipolarTransformers2020","accessed":{"date-parts":[["2021",1,7]]},"author":[{"family":"He","given":"Yihui"},{"family":"Yan","given":"Rui"},{"family":"Fragkiadaki","given":"Katerina"},{"family":"Yu","given":"Shoou-I."}],"citation-key":"heEpipolarTransformers2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"7779-7788","source":"openaccess.thecvf.com","title":"Epipolar Transformers","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/He_Epipolar_Transformers_CVPR_2020_paper.html"},
  {"id":"HierarchicallySelfsupervisedTransformer","accessed":{"date-parts":[["2023",1,31]]},"citation-key":"HierarchicallySelfsupervisedTransformer","title":"Hierarchically Self-supervised Transformer for Human Skeleton Representation Learning | SpringerLink","type":"webpage","URL":"https://link.springer.com/chapter/10.1007/978-3-031-19809-0_11"},
  {"id":"hintonWakeSleepAlgorithmUnsupervised1995","abstract":"An unsupervised learning algorithm for a multilayer network of stochastic neurons is described. Bottom-up “recognition” connections convert the input into representations in successive hidden layers and top-down “generative” connections reconstruct the representation in one layer from the representation in the layer above. In the “wake” phase, neurons are driven by recognition connections, and generative connections are adapted to increase the probability that they would reconstruct the correct activity vector in the layer below. In the “sleep” phase, neurons are driven by generative connections and recognition connections are adapted to increase the probability that they would produce the correct activity vector in the layer above.","accessed":{"date-parts":[["2024",1,25]]},"author":[{"family":"Hinton","given":"Geoffrey E."},{"family":"Dayan","given":"Peter"},{"family":"Frey","given":"Brendan J."},{"family":"Neal","given":"Radford M."}],"citation-key":"hintonWakeSleepAlgorithmUnsupervised1995","container-title":"Science","container-title-short":"Science","DOI":"10.1126/science.7761831","ISSN":"0036-8075, 1095-9203","issue":"5214","issued":{"date-parts":[["1995",5,26]]},"language":"en","page":"1158-1161","source":"DOI.org (Crossref)","title":"The \"Wake-Sleep\" Algorithm for Unsupervised Neural Networks","type":"article-journal","URL":"https://www.science.org/doi/10.1126/science.7761831","volume":"268"},
  {"id":"hirschornNormalizingFlowsHuman2023","abstract":"Video anomaly detection is an ill-posed problem because it relies on many parameters such as appearance, pose, camera angle, background, and more. We distill the problem to anomaly detection of human pose, thus decreasing the risk of nuisance parameters such as appearance affecting the result. Focusing on pose alone also has the side benefit of reducing bias against distinct minority groups. Our model works directly on human pose graph sequences and is exceptionally lightweight (~1K parameters), capable of running on any machine able to run the pose estimation with negligible additional resources. We leverage the highly compact pose representation in a normalizing flows framework, which we extend to tackle the unique characteristics of spatio-temporal pose data and show its advantages in this use case. The algorithm is quite general and can handle training data of only normal examples as well as a supervised setting that consists of labeled normal and abnormal examples. We report state-of-the-art results on two anomaly detection benchmarks - the unsupervised ShanghaiTech dataset and the recent supervised UBnormal dataset.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Hirschorn","given":"Or"},{"family":"Avidan","given":"Shai"}],"citation-key":"hirschornNormalizingFlowsHuman2023","DOI":"10.48550/arXiv.2211.10946","issued":{"date-parts":[["2023",8,16]]},"number":"arXiv:2211.10946","publisher":"arXiv","source":"arXiv.org","title":"Normalizing Flows for Human Pose Anomaly Detection","type":"article","URL":"http://arxiv.org/abs/2211.10946"},
  {"id":"holmquistDiffPoseMultihypothesisHuman2022","abstract":"Traditionally, monocular 3D human pose estimation employs a machine learning model to predict the most likely 3D pose for a given input image. However, a single image can be highly ambiguous and induces multiple plausible solutions for the 2D-3D lifting step which results in overly confident 3D pose predictors. To this end, we propose \\emph{DiffPose}, a conditional diffusion model, that predicts multiple hypotheses for a given input image. In comparison to similar approaches, our diffusion model is straightforward and avoids intensive hyperparameter tuning, complex network structures, mode collapse, and unstable training. Moreover, we tackle a problem of the common two-step approach that first estimates a distribution of 2D joint locations via joint-wise heatmaps and consecutively approximates them based on first- or second-moment statistics. Since such a simplification of the heatmaps removes valid information about possibly correct, though labeled unlikely, joint locations, we propose to represent the heatmaps as a set of 2D joint candidate samples. To extract information about the original distribution from these samples we introduce our \\emph{embedding transformer} that conditions the diffusion model. Experimentally, we show that DiffPose slightly improves upon the state of the art for multi-hypothesis pose estimation for simple poses and outperforms it by a large margin for highly ambiguous poses.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Holmquist","given":"Karl"},{"family":"Wandt","given":"Bastian"}],"citation-key":"holmquistDiffPoseMultihypothesisHuman2022","DOI":"10.48550/arXiv.2211.16487","issued":{"date-parts":[["2022",11,29]]},"number":"arXiv:2211.16487","publisher":"arXiv","source":"arXiv.org","title":"DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models","title-short":"DiffPose","type":"article","URL":"http://arxiv.org/abs/2211.16487"},
  {"id":"holmquistDiffPoseMultihypothesisHuman2022a","abstract":"Traditionally, monocular 3D human pose estimation employs a machine learning model to predict the most likely 3D pose for a given input image. However, a single image can be highly ambiguous and induces multiple plausible solutions for the 2D-3D lifting step which results in overly confident 3D pose predictors. To this end, we propose \\emph{DiffPose}, a conditional diffusion model, that predicts multiple hypotheses for a given input image. In comparison to similar approaches, our diffusion model is straightforward and avoids intensive hyperparameter tuning, complex network structures, mode collapse, and unstable training. Moreover, we tackle a problem of the common two-step approach that first estimates a distribution of 2D joint locations via joint-wise heatmaps and consecutively approximates them based on first- or second-moment statistics. Since such a simplification of the heatmaps removes valid information about possibly correct, though labeled unlikely, joint locations, we propose to represent the heatmaps as a set of 2D joint candidate samples. To extract information about the original distribution from these samples we introduce our \\emph{embedding transformer} that conditions the diffusion model. Experimentally, we show that DiffPose slightly improves upon the state of the art for multi-hypothesis pose estimation for simple poses and outperforms it by a large margin for highly ambiguous poses.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Holmquist","given":"Karl"},{"family":"Wandt","given":"Bastian"}],"citation-key":"holmquistDiffPoseMultihypothesisHuman2022a","DOI":"10.48550/arXiv.2211.16487","issued":{"date-parts":[["2022",11,29]]},"number":"arXiv:2211.16487","publisher":"arXiv","source":"arXiv.org","title":"DiffPose: Multi-hypothesis Human Pose Estimation using Diffusion models","title-short":"DiffPose","type":"article","URL":"http://arxiv.org/abs/2211.16487"},
  {"id":"hongPOSEPseudoObject2018","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Hong","given":"Je Hyeong"},{"family":"Zach","given":"Christopher"}],"citation-key":"hongPOSEPseudoObject2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"1876-1885","source":"openaccess.thecvf.com","title":"pOSE: Pseudo Object Space Error for Initialization-Free Bundle Adjustment","title-short":"pOSE","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Hong_pOSE_Pseudo_Object_CVPR_2018_paper.html"},
  {"id":"hrubyLearningSolveHard2022","accessed":{"date-parts":[["2022",8,31]]},"author":[{"family":"Hruby","given":"Petr"},{"family":"Duff","given":"Timothy"},{"family":"Leykin","given":"Anton"},{"family":"Pajdla","given":"Tomas"}],"citation-key":"hrubyLearningSolveHard2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"5532-5542","source":"openaccess.thecvf.com","title":"Learning To Solve Hard Minimal Problems","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Hruby_Learning_To_Solve_Hard_Minimal_Problems_CVPR_2022_paper.html"},
  {"id":"huangNeuralVotingField2023","accessed":{"date-parts":[["2023",10,11]]},"author":[{"family":"Huang","given":"Lin"},{"family":"Lin","given":"Chung-Ching"},{"family":"Lin","given":"Kevin"},{"family":"Liang","given":"Lin"},{"family":"Wang","given":"Lijuan"},{"family":"Yuan","given":"Junsong"},{"family":"Liu","given":"Zicheng"}],"citation-key":"huangNeuralVotingField2023","container-title":"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR52729.2023.00866","event-place":"Vancouver, BC, Canada","event-title":"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"979-8-3503-0129-8","issued":{"date-parts":[["2023",6]]},"language":"en","page":"8969-8978","publisher":"IEEE","publisher-place":"Vancouver, BC, Canada","source":"DOI.org (Crossref)","title":"Neural Voting Field for Camera-Space 3D Hand Pose Estimation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10203288/"},
  {"id":"huangPixelwiseVarifocalCamera2017","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Huang","given":"Longxiang"},{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"huangPixelwiseVarifocalCamera2017","container-title":"Electronics Letters","container-title-short":"Electron. lett.","DOI":"10.1049/el.2017.1648","ISSN":"0013-5194, 1350-911X","issue":"15","issued":{"date-parts":[["2017",7]]},"language":"en","note":"ZSCC:00000","number":"15","page":"1044-1046","source":"DOI.org (Crossref)","title":"Pixel‐wise varifocal camera model for handling multilayer refractions","type":"article-journal","URL":"https://onlinelibrary.wiley.com/doi/10.1049/el.2017.1648","volume":"53"},
  {"id":"huangPlateRefractiveCamera2017","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Huang","given":"Longxiang"},{"family":"Zhao","given":"Xu"},{"family":"Cai","given":"Shen"},{"family":"Liu","given":"Yuncai"}],"citation-key":"huangPlateRefractiveCamera2017","container-title":"Journal of Electronic Imaging","container-title-short":"J. Electron. Imaging","DOI":"10.1117/1.JEI.26.2.023020","ISSN":"1017-9909","issue":"2","issued":{"date-parts":[["2017",4,14]]},"language":"en","note":"ZSCC:00005","number":"2","page":"023020","source":"DOI.org (Crossref)","title":"Plate refractive camera model and its applications","type":"article-journal","URL":"http://electronicimaging.spiedigitallibrary.org/article.aspx?doi=10.1117/1.JEI.26.2.023020","volume":"26"},
  {"id":"huangPropertiesMatrixTwoview1989","abstract":"In the eight-point linear algorithm for determining 3D motion/structure from two perspective views using point correspondences, the E matrix plays a central role. The E matrix is defined as a skew-symmetrical matrix (containing the translation components) postmultiplied by a rotation matrix. The authors show that a necessary and sufficient condition for a 3*3 matrix to be so decomposable is that one of its singular values is zero and the other two are equal. Several other forms of this property are presented. Some applications are briefly described.<>","author":[{"family":"Huang","given":"T. S."},{"family":"Faugeras","given":"O. D."}],"citation-key":"huangPropertiesMatrixTwoview1989","container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","DOI":"10.1109/34.41368","ISSN":"1939-3539","issue":"12","issued":{"date-parts":[["1989",12]]},"page":"1310-1312","source":"IEEE Xplore","title":"Some properties of the E matrix in two-view motion estimation","type":"article-journal","volume":"11"},
  {"id":"huangSemiSupervised2DHuman2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Huang","given":"Linzhi"},{"family":"Li","given":"Yulong"},{"family":"Tian","given":"Hongbo"},{"family":"Yang","given":"Yue"},{"family":"Li","given":"Xiangang"},{"family":"Deng","given":"Weihong"},{"family":"Ye","given":"Jieping"}],"citation-key":"huangSemiSupervised2DHuman2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"693-703","source":"openaccess.thecvf.com","title":"Semi-Supervised 2D Human Pose Estimation Driven by Position Inconsistency Pseudo Label Correction Module","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Semi-Supervised_2D_Human_Pose_Estimation_Driven_by_Position_Inconsistency_Pseudo_CVPR_2023_paper.html"},
  {"id":"huangUnderwaterCameraModel2015","abstract":"The widespread use of the underwater camera provides an effective nondestructive means for underwater measurement in various scenarios. The underwater camera captures objects through at least one refraction by the interface between the water and the protecting house. It is a nonsingle viewpoint (non-SVP) imaging system and the assumption of single viewpoint (SVP) camera model is invalid. Always, we must calibrate the camera system to quantify the image deformation caused by refraction and to measure the object accuracy. However, it is sometimes difﬁcult to be done. In this paper, we propose a novel ﬂexible underwater camera model. Then, we present an underwater camera calibration method based on the proposed camera model to calibrate the underwater camera. Both synthetic and real experiments validate the proposed method.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Huang","given":"Longxiang"},{"family":"Zhao","given":"Xu"},{"family":"Huang","given":"Xingling"},{"family":"Liu","given":"Yuncai"}],"citation-key":"huangUnderwaterCameraModel2015","container-title":"2015 IEEE International Conference on Information and Automation","DOI":"10.1109/ICInfA.2015.7279526","event-place":"Lijiang, China","event-title":"2015 IEEE International Conference on Information and Automation (ICIA)","ISBN":"978-1-4673-9104-7","issued":{"date-parts":[["2015",8]]},"language":"en","note":"ZSCC:00010","page":"1519-1523","publisher":"IEEE","publisher-place":"Lijiang, China","source":"DOI.org (Crossref)","title":"Underwater camera model and its use in calibration","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7279526/"},
  {"id":"huaWeaklySupervised3DHuman2023","accessed":{"date-parts":[["2023",12,5]]},"author":[{"family":"Hua","given":"Guoliang"},{"family":"Liu","given":"Hong"},{"family":"Li","given":"Wenhao"},{"family":"Zhang","given":"Qian"},{"family":"Ding","given":"Runwei"},{"family":"Xu","given":"Xin"}],"citation-key":"huaWeaklySupervised3DHuman2023","container-title":"IEEE Transactions on Multimedia","container-title-short":"IEEE Trans. Multimedia","DOI":"10.1109/TMM.2022.3171102","ISSN":"1520-9210, 1941-0077","issued":{"date-parts":[["2023"]]},"page":"1832-1843","source":"DOI.org (Crossref)","title":"Weakly-Supervised 3D Human Pose Estimation With Cross-View U-Shaped Graph Convolutional Network","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9765377/","volume":"25"},
  {"id":"huConditionalDirectedGraph2021","abstract":"Graph convolutional networks have significantly improved 3D human pose estimation by representing the human skeleton as an undirected graph. However, this representation fails to reflect the articulated characteristic of human skeletons as the hierarchical orders among the joints are not explicitly presented. In this paper, we propose to represent the human skeleton as a directed graph with the joints as nodes and bones as edges that are directed from parent joints to child joints. By so doing, the directions of edges can explicitly reflect the hierarchical relationships among the nodes. Based on this representation, we further propose a spatial-temporal conditional directed graph convolution to leverage varying non-local dependence for different poses by conditioning the graph topology on input poses. Altogether, we form a U-shaped network, named U-shaped Conditional Directed Graph Convolutional Network, for 3D human pose estimation from monocular videos. To evaluate the effectiveness of our method, we conducted extensive experiments on two challenging large-scale benchmarks: Human3.6M and MPI-INF-3DHP. Both quantitative and qualitative results show that our method achieves top performance. Also, ablation studies show that directed graphs can better exploit the hierarchy of articulated human skeletons than undirected graphs, and the conditional connections can yield adaptive graph topologies for different poses.","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Hu","given":"Wenbo"},{"family":"Zhang","given":"Changgong"},{"family":"Zhan","given":"Fangneng"},{"family":"Zhang","given":"Lei"},{"family":"Wong","given":"Tien-Tsin"}],"citation-key":"huConditionalDirectedGraph2021","DOI":"10.48550/arXiv.2107.07797","issued":{"date-parts":[["2021",8,4]]},"number":"arXiv:2107.07797","publisher":"arXiv","source":"arXiv.org","title":"Conditional Directed Graph Convolution for 3D Human Pose Estimation","type":"article","URL":"http://arxiv.org/abs/2107.07797"},
  {"id":"huHandwrittenNumberRecognition2021","abstract":"In recent years, Artificial Neural Network (ANN) has been widely used in digital handwriting recognition by virtue of its strong fault-tolerant ability and classification ability. However, in traditional recognition methods, taking the number of image pixels as the input number of neurons will cause problems such as long learning and training time and low efficiency. This paper combines principal component analysis (PCA) algorithm with BP algorithm for handwritten digit recognition. After image preprocessing, PCA algorithm is used to reduce dimension of original data. The first 10 principal components, the first 30 principal components, the first 45 principal components, and the first 60 principal components are sent to the neural network as input neurons for training. Then, the test data set was used for testing. Finally, the simulation was analyzed from three dimensions: training efficiency, learning time and identification accuracy. The results show that when the number of input neurons is 60 and the number of hidden layer neurons is 30, the highest recognition rate is only 0.13% lower than that of the number of input neurons is 784, but the training time of the neural network is reduced by 94 seconds, and the efficiency is improved by 32%. When the number of hidden layer neurons is 50, the highest recognition rate is only 0.25% lower than that of input neurons is 784, but the time is reduced by 237 seconds and the efficiency is improved by 63%. This design solves the problem of low learning efficiency in digital recognition well.","author":[{"family":"Hu","given":"Zhihong"},{"family":"Tian","given":"Qian"},{"family":"Wang","given":"Hongbo"},{"family":"He","given":"Zhenyu"}],"citation-key":"huHandwrittenNumberRecognition2021","container-title":"2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","DOI":"10.1109/IMCEC51613.2021.9482278","event-title":"2021 IEEE 4th Advanced Information Management, Communicates, Electronic and Automation Control Conference (IMCEC)","ISSN":"2693-2776","issued":{"date-parts":[["2021",6]]},"page":"1570-1574","source":"IEEE Xplore","title":"Handwritten number recognition based on PCA and neural network","type":"paper-conference","volume":"4"},
  {"id":"huSemanticSegmentationStreet2019","abstract":"In this work, we address the task of semantic segmentation in street scenes. Recent approaches based on convolutional neural networks have shown excellent results on several semantic segmentation benchmarks. Most of them, however, only exploit RGB information. Due to the development of stereo matching algorithms, disparity maps can be more easily acquired. Structural information encoded in disparity can be treated as supplementary information of RGB images, which is expected to boost performance. Therefore, in this work we propose to fuse disparity information in street scene understanding task. And we design four methods to incorporate disparity information into semantic segmentation framework. They are summation, multiplication, concatenation and channel concatenation. Besides, disparity map can be utilized as ground truth of a regression task, guiding the learning of semantic segmentation as a loss term. Comprehensive experiments on KITTI and Cityscapes datasets show that each method can achieve performance improvement. The experimental results validate the eﬀectiveness of disparity information to street scene semantic segmentation tasks.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Hu","given":"Hanwen"},{"family":"Zhao","given":"Xu"}],"citation-key":"huSemanticSegmentationStreet2019","container-title":"ICIG","DOI":"10.1007/978-3-030-34120-6_14","event-place":"Cham","event-title":"Image and Graphics","ISBN":"978-3-030-34119-0 978-3-030-34120-6","issued":{"date-parts":[["2019"]]},"language":"en","note":"ZSCC:00001","page":"169-181","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Semantic Segmentation of Street Scenes Using Disparity Information","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-030-34120-6_14"},
  {"id":"HybridRefinementCorrectionHeatmaps","accessed":{"date-parts":[["2023",12,5]]},"citation-key":"HybridRefinementCorrectionHeatmaps","title":"Hybrid Refinement-Correction Heatmaps for Human Pose Estimation | IEEE Journals & Magazine | IEEE Xplore","type":"webpage","URL":"https://ieeexplore.ieee.org/document/9107502"},
  {"id":"ionescuHuman36M2014","abstract":"We introduce a new dataset, Human3.6M, of 3.6 Million accurate 3D Human poses, acquired by recording the performance of 5 female and 6 male subjects, under 4 different viewpoints, for training realistic human sensing systems and for evaluating the next generation of human pose estimation models and algorithms. Besides increasing the size of the datasets in the current state-of-the-art by several orders of magnitude, we also aim to complement such datasets with a diverse set of motions and poses encountered as part of typical human activities (taking photos, talking on the phone, posing, greeting, eating, etc.), with additional synchronized image, human motion capture, and time of flight (depth) data, and with accurate 3D body scans of all the subject actors involved. We also provide controlled mixed reality evaluation scenarios where 3D human models are animated using motion capture and inserted using correct 3D geometry, in complex real environments, viewed with moving cameras, and under occlusion. Finally, we provide a set of large-scale statistical models and detailed evaluation baselines for the dataset illustrating its diversity and the scope for improvement by future work in the research community. Our experiments show that our best large-scale model can leverage our full training set to obtain a 20% improvement in performance compared to a training set of the scale of the largest existing public dataset for this problem. Yet the potential for improvement by leveraging higher capacity, more complex models with our large dataset, is substantially vaster and should stimulate future research. The dataset together with code for the associated large-scale learning models, features, visualization tools, as well as the evaluation server, is available online at http://vision.imar.ro/human3.6m.","author":[{"family":"Ionescu","given":"C."},{"family":"Papava","given":"D."},{"family":"Olaru","given":"V."},{"family":"Sminchisescu","given":"C."}],"citation-key":"ionescuHuman36M2014","container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","DOI":"10.1109/TPAMI.2013.248","ISSN":"1939-3539","issue":"7","issued":{"date-parts":[["2014",7]]},"page":"1325-1339","source":"IEEE Xplore","title":"Human3.6M","title-short":"Human3.6M","type":"article-journal","volume":"36"},
  {"id":"iqbalWeaklySupervised3DHuman2020","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Iqbal","given":"Umar"},{"family":"Molchanov","given":"Pavlo"},{"family":"Kautz","given":"Jan"}],"citation-key":"iqbalWeaklySupervised3DHuman2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"5243-5252","source":"openaccess.thecvf.com","title":"Weakly-Supervised 3D Human Pose Learning via Multi-View Images in the Wild","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Iqbal_Weakly-Supervised_3D_Human_Pose_Learning_via_Multi-View_Images_in_the_CVPR_2020_paper.html"},
  {"id":"iskakovLearnableTriangulationHuman2019","abstract":"We present two novel solutions for multi-view 3D human pose estimation based on new learnable triangulation methods that combine 3D information from multiple 2D views. The ﬁrst (baseline) solution is a basic differentiable algebraic triangulation with an addition of conﬁdence weights estimated from the input images. The second solution is based on a novel method of volumetric aggregation from intermediate 2D backbone feature maps. The aggregated volume is then reﬁned via 3D convolutions that produce ﬁnal 3D joint heatmaps and allow implicit modelling a human pose prior. Crucially, both approaches are end-toend differentiable, which allows us to directly optimize the target metric. We demonstrate transferability of the solutions across datasets and considerably improve the multiview state of the art on the Human3.6M dataset. Video demonstration, annotations and additional materials will be posted on our project page1.","accessed":{"date-parts":[["2022",10,20]]},"author":[{"family":"Iskakov","given":"Karim"},{"family":"Burkov","given":"Egor"},{"family":"Lempitsky","given":"Victor"},{"family":"Malkov","given":"Yury"}],"citation-key":"iskakovLearnableTriangulationHuman2019","container-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV.2019.00781","event-place":"Seoul, Korea (South)","event-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","ISBN":"978-1-7281-4803-8","issued":{"date-parts":[["2019",10]]},"language":"en","page":"7717-7726","publisher":"IEEE","publisher-place":"Seoul, Korea (South)","source":"DOI.org (Crossref)","title":"Learnable Triangulation of Human Pose","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9010013/"},
  {"id":"ivekovicHumanBodyPose2006","abstract":"In this paper we describe the application of Particle Swarm Optimisation to the problem of human body pose estimation from multiple view video sequences. We use a subdivision body model with an underlying skeleton layer to estimate and illustrate the body pose. The optimisation looks for the best match between the silhouettes extracted from the original video sequence and the silhouettes generated by the projection of the model in a pose suggested by the PSO. The original PSO algorithm is applied hierarchically and combined with the full overall optimisation to decrease the effects of error propagation. Results demonstrate the ability of PSO to reliably recover the correct body pose from 4-viewpoint video sequences.","author":[{"family":"Ivekovic","given":"S."},{"family":"Trucco","given":"E."}],"citation-key":"ivekovicHumanBodyPose2006","container-title":"2006 IEEE International Conference on Evolutionary Computation","DOI":"10.1109/CEC.2006.1688453","event-title":"2006 IEEE International Conference on Evolutionary Computation","ISSN":"1941-0026","issued":{"date-parts":[["2006",7]]},"page":"1256-1263","source":"IEEE Xplore","title":"Human Body Pose Estimation with PSO","type":"paper-conference"},
  {"id":"jackAdversariallyParameterizedOptimization2017","abstract":"We propose Adversarially Parameterized Optimization, a framework for learning low-dimensional feasible parameterizations of human poses and inferring 3D poses from 2D input. We train a Generative Adversarial Network to `imagine' feasible poses, and search this imagination space for a solution that is consistent with observations. The framework requires no scene/observation correspondences and enforces known geometric invariances without dataset augmentation. The algorithm can be configured at run time to take advantage of known values such as intrinsic/extrinsic camera parameters or target height when available without additional training. We demonstrate the framework by inferring 3D human poses from projected joint positions for both single frames and sequences. We show competitive results with extremely simple shallow network architectures and make the code publicly available.","author":[{"family":"Jack","given":"Dominic"},{"family":"Maire","given":"Frederic"},{"family":"Eriksson","given":"Anders"},{"family":"Shirazi","given":"Sareh"}],"citation-key":"jackAdversariallyParameterizedOptimization2017","container-title":"2017 International Conference on 3D Vision (3DV)","DOI":"10.1109/3DV.2017.00026","event-title":"2017 International Conference on 3D Vision (3DV)","ISSN":"2475-7888","issued":{"date-parts":[["2017",10]]},"page":"145-154","source":"IEEE Xplore","title":"Adversarially Parameterized Optimization for 3D Human Pose Estimation","type":"paper-conference"},
  {"id":"jackIGENetInverseGraphics2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Jack","given":"Dominic"},{"family":"Maire","given":"Frederic"},{"family":"Shirazi","given":"Sareh"},{"family":"Eriksson","given":"Anders"}],"citation-key":"jackIGENetInverseGraphics2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"7075-7084","source":"openaccess.thecvf.com","title":"IGE-Net: Inverse Graphics Energy Networks for Human Pose Estimation and Single-View Reconstruction","title-short":"IGE-Net","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Jack_IGE-Net_Inverse_Graphics_Energy_Networks_for_Human_Pose_Estimation_and_CVPR_2019_paper.html"},
  {"id":"jiangA2JTransformerAnchorJointTransformer2023","abstract":"3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar appearance patterns between 2 hands, ill-posed joint position mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture interacting hands' local fine details and global articulated clues among joints jointly. To this end, A2J is evolved under Transformer's non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages over A2J. First, self-attention across local anchor points is built to make them global spatial context aware to better capture joints' articulation clues for resisting occlusion. Secondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advancement in 2-hand case) and can also be applied to depth domain with strong generalization.","accessed":{"date-parts":[["2023",12,9]]},"author":[{"family":"Jiang","given":"Changlong"},{"family":"Xiao","given":"Yang"},{"family":"Wu","given":"Cunlin"},{"family":"Zhang","given":"Mingyang"},{"family":"Zheng","given":"Jinghong"},{"family":"Cao","given":"Zhiguo"},{"family":"Zhou","given":"Joey Tianyi"}],"citation-key":"jiangA2JTransformerAnchorJointTransformer2023","issued":{"date-parts":[["2023",4,7]]},"number":"arXiv:2304.03635","publisher":"arXiv","source":"arXiv.org","title":"A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation from a Single RGB Image","title-short":"A2J-Transformer","type":"article","URL":"http://arxiv.org/abs/2304.03635"},
  {"id":"jiangEgocentricPoseEstimation2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Jiang","given":"Hao"},{"family":"Ithapu","given":"Vamsi Krishna"}],"citation-key":"jiangEgocentricPoseEstimation2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11006-11014","source":"openaccess.thecvf.com","title":"Egocentric Pose Estimation From Human Vision Span","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Jiang_Egocentric_Pose_Estimation_From_Human_Vision_Span_ICCV_2021_paper.html"},
  {"id":"jiangH4DHuman4D2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Jiang","given":"Boyan"},{"family":"Zhang","given":"Yinda"},{"family":"Wei","given":"Xingkui"},{"family":"Xue","given":"Xiangyang"},{"family":"Fu","given":"Yanwei"}],"citation-key":"jiangH4DHuman4D2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"19355-19365","source":"openaccess.thecvf.com","title":"H4D: Human 4D Modeling by Learning Neural Compositional Representation","title-short":"H4D","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Jiang_H4D_Human_4D_Modeling_by_Learning_Neural_Compositional_Representation_CVPR_2022_paper.html"},
  {"id":"jiangInstantNVRInstantNeural2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Jiang","given":"Yuheng"},{"family":"Yao","given":"Kaixin"},{"family":"Su","given":"Zhuo"},{"family":"Shen","given":"Zhehao"},{"family":"Luo","given":"Haimin"},{"family":"Xu","given":"Lan"}],"citation-key":"jiangInstantNVRInstantNeural2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"595-605","source":"openaccess.thecvf.com","title":"Instant-NVR: Instant Neural Volumetric Rendering for Human-Object Interactions From Monocular RGBD Stream","title-short":"Instant-NVR","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_Instant-NVR_Instant_Neural_Volumetric_Rendering_for_Human-Object_Interactions_From_Monocular_CVPR_2023_paper.html"},
  {"id":"jiangLocalImplicitGrid2020","abstract":"Shape priors learned from data are commonly used to reconstruct 3D objects from partial or noisy data. Yet no such shape priors are available for indoor scenes, since typical 3D autoencoders cannot handle their scale, complexity, or diversity. In this paper, we introduce Local Implicit Grid Representations, a new 3D shape representation designed for scalability and generality. The motivating idea is that most 3D surfaces share geometric details at some scale –i.e., at a scale smaller than an entire object and larger than a small patch. We train an autoencoder to learn an embedding of local crops of 3D shapes at that size. Then, we use the decoder as a component in a shape optimization that solves for a set of latent codes on a regular grid of overlapping crops such that an interpolation of the decoded local shapes matches a partial or noisy observation. We demonstrate the value of this proposed approach for 3D surface reconstruction from sparse point observations, showing signiﬁcantly better results than alternative approaches.","accessed":{"date-parts":[["2023",10,10]]},"author":[{"family":"Jiang","given":"Chiyu"},{"family":"Sud","given":"Avneesh"},{"family":"Makadia","given":"Ameesh"},{"family":"Huang","given":"Jingwei"},{"family":"NieBner","given":"Matthias"},{"family":"Funkhouser","given":"Thomas"}],"citation-key":"jiangLocalImplicitGrid2020","container-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR42600.2020.00604","event-place":"Seattle, WA, USA","event-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-7281-7168-5","issued":{"date-parts":[["2020",6]]},"language":"en","page":"6000-6009","publisher":"IEEE","publisher-place":"Seattle, WA, USA","source":"DOI.org (Crossref)","title":"Local Implicit Grid Representations for 3D Scenes","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9156855/"},
  {"id":"jiangOPDSingleview3D2022","abstract":"We address the task of predicting what parts of an object can open and how they move when they do so. The input is a single image of an object, and as output we detect what parts of the object can open, and the motion parameters describing the articulation of each openable part. To tackle this task, we create two datasets of 3D objects: OPDSynth based on existing synthetic objects, and OPDReal based on RGBD reconstructions of real objects. We then design OPDRCNN, a neural architecture that detects openable parts and predicts their motion parameters. Our experiments show that this is a challenging task especially when considering generalization across object categories, and the limited amount of information in a single image. Our architecture outperforms baselines and prior work especially for RGB image inputs. Short video summary at https://www.youtube.com/watch?v=P85iCaD0rfc","accessed":{"date-parts":[["2023",8,28]]},"author":[{"family":"Jiang","given":"Hanxiao"},{"family":"Mao","given":"Yongsen"},{"family":"Savva","given":"Manolis"},{"family":"Chang","given":"Angel X."}],"citation-key":"jiangOPDSingleview3D2022","issued":{"date-parts":[["2022",3,30]]},"number":"arXiv:2203.16421","publisher":"arXiv","source":"arXiv.org","title":"OPD: Single-view 3D Openable Part Detection","title-short":"OPD","type":"article","URL":"http://arxiv.org/abs/2203.16421"},
  {"id":"jiangProbabilisticAttentionModel2023","abstract":"Recently, deep learning based approaches have shown promising results in 3D hand reconstruction from a single RGB image. These approaches can be roughly divided into model-based approaches, which are heavily dependent on the model's parameter space, and model-free approaches, which require large numbers of 3D ground truths to reduce depth ambiguity and struggle in weakly-supervised scenarios. To overcome these issues, we propose a novel probabilistic model to achieve the robustness of model-based approaches and reduced dependence on the model's parameter space of model-free approaches. The proposed probabilistic model incorporates a model-based network as a prior-net to estimate the prior probability distribution of joints and vertices. An Attention-based Mesh Vertices Uncertainty Regression (AMVUR) model is proposed to capture dependencies among vertices and the correlation between joints and mesh vertices to improve their feature representation. We further propose a learning based occlusion-aware Hand Texture Regression model to achieve high-fidelity texture reconstruction. We demonstrate the flexibility of the proposed probabilistic model to be trained in both supervised and weakly-supervised scenarios. The experimental results demonstrate our probabilistic model's state-of-the-art accuracy in 3D hand and texture reconstruction from a single image in both training schemes, including in the presence of severe occlusions.","accessed":{"date-parts":[["2023",9,24]]},"author":[{"family":"Jiang","given":"Zheheng"},{"family":"Rahmani","given":"Hossein"},{"family":"Black","given":"Sue"},{"family":"Williams","given":"Bryan M."}],"citation-key":"jiangProbabilisticAttentionModel2023","DOI":"10.48550/arXiv.2304.14299","issued":{"date-parts":[["2023",4,27]]},"number":"arXiv:2304.14299","publisher":"arXiv","source":"arXiv.org","title":"A Probabilistic Attention Model with Occlusion-aware Texture Regression for 3D Hand Reconstruction from a Single RGB Image","type":"article","URL":"http://arxiv.org/abs/2304.14299"},
  {"id":"jiangSeeingInvisiblePoses2017","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Jiang","given":"Hao"},{"family":"Grauman","given":"Kristen"}],"citation-key":"jiangSeeingInvisiblePoses2017","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2017"]]},"page":"3976-3984","source":"openaccess.thecvf.com","title":"Seeing Invisible Poses: Estimating 3D Body Pose From Egocentric Video","title-short":"Seeing Invisible Poses","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2017/html/Jiang_Seeing_Invisible_Poses_CVPR_2017_paper.html"},
  {"id":"jinSingleStageEnoughMultiPerson2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Jin","given":"Lei"},{"family":"Xu","given":"Chenyang"},{"family":"Wang","given":"Xiaojuan"},{"family":"Xiao","given":"Yabo"},{"family":"Guo","given":"Yandong"},{"family":"Nie","given":"Xuecheng"},{"family":"Zhao","given":"Jian"}],"citation-key":"jinSingleStageEnoughMultiPerson2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13086-13095","source":"openaccess.thecvf.com","title":"Single-Stage Is Enough: Multi-Person Absolute 3D Pose Estimation","title-short":"Single-Stage Is Enough","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Jin_Single-Stage_Is_Enough_Multi-Person_Absolute_3D_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"JiYuDiZhiLiangSanWeiDianYunDeRenTiZiTaiHeTiTaiJingXiChongJianpdf2016","citation-key":"JiYuDiZhiLiangSanWeiDianYunDeRenTiZiTaiHeTiTaiJingXiChongJianpdf2016","container-title":"NCIG","issued":{"date-parts":[["2016"]]},"title":"基于低质量三维点云的人体姿态和体态精细重建.pdf","type":"paper-conference"},
  {"id":"JiYuDuoFenZhiFasterRCNN2016","citation-key":"JiYuDuoFenZhiFasterRCNN2016","container-title":"NCIG","issued":{"date-parts":[["2016"]]},"title":"基于多分支 Faster RCNN 的人体姿态估计.pdf","type":"paper-conference"},
  {"id":"JiYuXiShuBianMaHeJuBuShiKongTeZhengDeRenTiDongZuoShiBiepdf2010","citation-key":"JiYuXiShuBianMaHeJuBuShiKongTeZhengDeRenTiDongZuoShiBiepdf2010","container-title":"NCIG","issued":{"date-parts":[["2010"]]},"title":"基于稀疏编码和局部时空特征的人体动作识别.pdf","type":"paper-conference"},
  {"id":"juMaskBasedAttentionParallel2022","abstract":"Facial expression recognition suffers big pose and occlusion in real world and attention mechanism is deployed widely to cope with these challenges. But most previous attentionbased methods are inadequate in locating crucial expressionrelated regions precisely and capturing useful facial expression features comprehensively. For these reasons, we present a novel mask-based attention parallel network (MAPNet). Firstly, mask-based attention module that locates expressionrelated regions is constructed from binary mask extracted by key landmark detection. Secondly, the designed parallel network embeds mask-based attention modules into its different layers to acquire comprehensive facial expression features. Thirdly, the extracted parallel features are divided into several detached blocks from spatial dimension to predict facial expression independently. Finally, the expression label is acquired by combining two predictions of the parallel network and a new loss function is designed to weigh unbalanced facial expression distribution. We validate our method on three popular in-the-wild datasets and the results demonstrate that our MANPnet outperforms previous state-of-the-art methods among RAFDB, AffectNet and FEDRO.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Ju","given":"Lingzhao"},{"family":"Zhao","given":"Xu"}],"citation-key":"juMaskBasedAttentionParallel2022","container-title":"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","DOI":"10.1109/ICASSP43922.2022.9747717","event-place":"Singapore, Singapore","event-title":"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","ISBN":"978-1-6654-0540-9","issued":{"date-parts":[["2022",5,23]]},"language":"en","note":"ZSCC:00003","page":"2410-2414","publisher":"IEEE","publisher-place":"Singapore, Singapore","source":"DOI.org (Crossref)","title":"Mask-Based Attention Parallel Network for in-the-Wild Facial Expression Recognition","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9747717/"},
  {"id":"kamelHybridRefinementCorrectionHeatmaps2021","abstract":"In this paper, we present a method (Hybrid-Pose) to improve human pose estimation in images. We adopt Stacked Hourglass Networks to design two convolutional neural network models, RNet for pose refinement and CNet for pose correction. The CNet (Correction Network) guides the pose refinement RNet (Refinement Network) to correct the joint location before generating the final pose. Each of the two models is composed of four hourglasses, and each hourglass generates a group of detection heatmaps for the joints. The RNet model hourglasses have the same structure. However, the CNet model is designed with hourglasses of different structures for pose guidance. Since the pose estimation in RGB images is very sensitive to the image scene, our proposed approach generates multiple outputs of detection heatmaps to broaden the searching scope for the correct joints locations. We use the RNet model to refine the joints locations in each hourglass stage horizontally, then the heatmaps of each stage are fused with the heatmaps of all the CNet model hourglasses vertically in a hybrid manner. Our method shows competitive results with the existing state-of-the-art approaches on MPII and FLIC benchmark datasets. Although our proposed method focuses on improving single-person pose estimation, we also show the influence of this improvement on multi-person pose estimation by detecting multiple people using SSD detector, then estimating the pose of each person individually.","author":[{"family":"Kamel","given":"Aouaidjia"},{"family":"Sheng","given":"Bin"},{"family":"Li","given":"Ping"},{"family":"Kim","given":"Jinman"},{"family":"Feng","given":"David Dagan"}],"citation-key":"kamelHybridRefinementCorrectionHeatmaps2021","container-title":"IEEE Transactions on Multimedia","DOI":"10.1109/TMM.2020.2999181","ISSN":"1941-0077","issued":{"date-parts":[["2021"]]},"page":"1330-1342","source":"IEEE Xplore","title":"Hybrid Refinement-Correction Heatmaps for Human Pose Estimation","type":"article-journal","volume":"23"},
  {"id":"kamelHybridRefinementCorrectionHeatmaps2021a","abstract":"In this paper, we present a method (Hybrid-Pose) to improve human pose estimation in images. We adopt Stacked Hourglass Networks to design two convolutional neural network models, RNet for pose refinement and CNet for pose correction. The CNet (Correction Network) guides the pose refinement RNet (Refinement Network) to correct the joint location before generating the final pose. Each of the two models is composed of four hourglasses, and each hourglass generates a group of detection heatmaps for the joints. The RNet model hourglasses have the same structure. However, the CNet model is designed with hourglasses of different structures for pose guidance. Since the pose estimation in RGB images is very sensitive to the image scene, our proposed approach generates multiple outputs of detection heatmaps to broaden the searching scope for the correct joints locations. We use the RNet model to refine the joints locations in each hourglass stage horizontally, then the heatmaps of each stage are fused with the heatmaps of all the CNet model hourglasses vertically in a hybrid manner. Our method shows competitive results with the existing state-of-the-art approaches on MPII and FLIC benchmark datasets. Although our proposed method focuses on improving single-person pose estimation, we also show the influence of this improvement on multi-person pose estimation by detecting multiple people using SSD detector, then estimating the pose of each person individually.","author":[{"family":"Kamel","given":"Aouaidjia"},{"family":"Sheng","given":"Bin"},{"family":"Li","given":"Ping"},{"family":"Kim","given":"Jinman"},{"family":"Feng","given":"David Dagan"}],"citation-key":"kamelHybridRefinementCorrectionHeatmaps2021a","container-title":"IEEE Transactions on Multimedia","DOI":"10.1109/TMM.2020.2999181","ISSN":"1941-0077","issued":{"date-parts":[["2021"]]},"page":"1330-1342","source":"IEEE Xplore","title":"Hybrid Refinement-Correction Heatmaps for Human Pose Estimation","type":"article-journal","volume":"23"},
  {"id":"kamelHybridRefinementCorrectionHeatmaps2021b","abstract":"In this paper, we present a method (Hybrid-Pose) to improve human pose estimation in images. We adopt Stacked Hourglass Networks to design two convolutional neural network models, RNet for pose reﬁnement and CNet for pose correction. The CNet (Correction Network) guides the pose reﬁnement RNet (Reﬁnement Network) to correct the joint location before generating the ﬁnal pose. Each of the two models is composed of four hourglasses, and each hourglass generates a group of detection heatmaps for the joints. The RNet model hourglasses have the same structure. However, the CNet model is designed with hourglasses of different structures for pose guidance. Since the pose estimation in RGB images is very sensitive to the image scene, our proposed approach generates multiple outputs of detection heatmaps to broaden the searching scope for the correct joints locations. We use the RNet model to reﬁne the joints locations in each hourglass stage horizontally, then the heatmaps of each stage are fused with the heatmaps of all the CNet model hourglasses vertically in a hybrid manner. Our method shows competitive results with the existing state-of-the-art approaches on MPII and FLIC benchmark datasets. Although our proposed method focuses on improving single-person pose estimation, we also show the inﬂuence of this improvement on multi-person pose estimation by detecting multiple people using SSD detector, then estimating the pose of each person individually.","accessed":{"date-parts":[["2023",12,5]]},"author":[{"family":"Kamel","given":"Aouaidjia"},{"family":"Sheng","given":"Bin"},{"family":"Li","given":"Ping"},{"family":"Kim","given":"Jinman"},{"family":"Feng","given":"David Dagan"}],"citation-key":"kamelHybridRefinementCorrectionHeatmaps2021b","container-title":"IEEE Transactions on Multimedia","container-title-short":"IEEE Trans. Multimedia","DOI":"10.1109/TMM.2020.2999181","ISSN":"1520-9210, 1941-0077","issued":{"date-parts":[["2021"]]},"language":"en","page":"1330-1342","source":"DOI.org (Crossref)","title":"Hybrid Refinement-Correction Heatmaps for Human Pose Estimation","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9107502/","volume":"23"},
  {"id":"kanazawaEndEndRecoveryHuman2018","abstract":"We describe Human Mesh Recovery (HMR), an end-to-end framework for reconstructing a full 3D mesh of a human body from a single RGB image. In contrast to most current methods that compute 2D or 3D joint locations, we produce a richer and more useful mesh representation that is parameterized by shape and 3D joint angles. The main objective is to minimize the reprojection loss of keypoints, which allows our model to be trained using in-the-wild images that only have ground truth 2D annotations. However, the reprojection loss alone is highly underconstrained. In this work we address this problem by introducing an adversary trained to tell whether human body shape and pose parameters are real or not using a large database of 3D human meshes. We show that HMR can be trained with and without using any paired 2D-to-3D supervision. We do not rely on intermediate 2D keypoint detections and infer 3D pose and shape parameters directly from image pixels. Our model runs in real-time given a bounding box containing the person. We demonstrate our approach on various images in-the-wild and out-perform previous optimization-based methods that output 3D meshes and show competitive results on tasks such as 3D joint location estimation and part segmentation.","author":[{"family":"Kanazawa","given":"Angjoo"},{"family":"Black","given":"Michael J."},{"family":"Jacobs","given":"David W."},{"family":"Malik","given":"Jitendra"}],"citation-key":"kanazawaEndEndRecoveryHuman2018","container-title":"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","DOI":"10.1109/CVPR.2018.00744","event-title":"2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition","ISSN":"2575-7075","issued":{"date-parts":[["2018",6]]},"page":"7122-7131","source":"IEEE Xplore","title":"End-to-End Recovery of Human Shape and Pose","type":"paper-conference"},
  {"id":"kanazawaEndEndRecoveryHuman2018a","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Kanazawa","given":"Angjoo"},{"family":"Black","given":"Michael J."},{"family":"Jacobs","given":"David W."},{"family":"Malik","given":"Jitendra"}],"citation-key":"kanazawaEndEndRecoveryHuman2018a","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"7122-7131","source":"openaccess.thecvf.com","title":"End-to-End Recovery of Human Shape and Pose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Kanazawa_End-to-End_Recovery_of_CVPR_2018_paper.html"},
  {"id":"KeShiMeiTiZhongDeRenTiYunDongFenXipdf2009","citation-key":"KeShiMeiTiZhongDeRenTiYunDongFenXipdf2009","container-title":"中国计算机学会通讯","issued":{"date-parts":[["2009"]]},"title":"可视媒体中的人体运动分析.pdf","type":"article-journal"},
  {"id":"kimSamplingMatterPointGuided2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Kim","given":"Jeonghwan"},{"family":"Gwon","given":"Mi-Gyeong"},{"family":"Park","given":"Hyunwoo"},{"family":"Kwon","given":"Hyukmin"},{"family":"Um","given":"Gi-Mun"},{"family":"Kim","given":"Wonjun"}],"citation-key":"kimSamplingMatterPointGuided2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12880-12889","source":"openaccess.thecvf.com","title":"Sampling Is Matter: Point-Guided 3D Human Mesh Reconstruction","title-short":"Sampling Is Matter","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Kim_Sampling_Is_Matter_Point-Guided_3D_Human_Mesh_Reconstruction_CVPR_2023_paper.html"},
  {"id":"kingmaAdamMethodStochastic2017","abstract":"We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.","accessed":{"date-parts":[["2021",6,30]]},"author":[{"family":"Kingma","given":"Diederik P."},{"family":"Ba","given":"Jimmy"}],"citation-key":"kingmaAdamMethodStochastic2017","container-title":"arXiv:1412.6980 [cs]","issued":{"date-parts":[["2017",1,29]]},"source":"arXiv.org","title":"Adam: A Method for Stochastic Optimization","title-short":"Adam","type":"article-journal","URL":"http://arxiv.org/abs/1412.6980"},
  {"id":"kipfSemiSupervisedClassificationGraph2017","abstract":"We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin.","accessed":{"date-parts":[["2023",3,17]]},"author":[{"family":"Kipf","given":"Thomas N."},{"family":"Welling","given":"Max"}],"citation-key":"kipfSemiSupervisedClassificationGraph2017","issued":{"date-parts":[["2017",2,22]]},"number":"arXiv:1609.02907","publisher":"arXiv","source":"arXiv.org","title":"Semi-Supervised Classification with Graph Convolutional Networks","type":"article","URL":"http://arxiv.org/abs/1609.02907"},
  {"id":"kocabasPAREPartAttention2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Kocabas","given":"Muhammed"},{"family":"Huang","given":"Chun-Hao P."},{"family":"Hilliges","given":"Otmar"},{"family":"Black","given":"Michael J."}],"citation-key":"kocabasPAREPartAttention2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11127-11137","source":"openaccess.thecvf.com","title":"PARE: Part Attention Regressor for 3D Human Body Estimation","title-short":"PARE","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Kocabas_PARE_Part_Attention_Regressor_for_3D_Human_Body_Estimation_ICCV_2021_paper.html"},
  {"id":"kocabasSelfSupervisedLearning3D2019","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Kocabas","given":"Muhammed"},{"family":"Karagoz","given":"Salih"},{"family":"Akbas","given":"Emre"}],"citation-key":"kocabasSelfSupervisedLearning3D2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"1077-1086","source":"openaccess.thecvf.com","title":"Self-Supervised Learning of 3D Human Pose Using Multi-View Geometry","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Kocabas_Self-Supervised_Learning_of_3D_Human_Pose_Using_Multi-View_Geometry_CVPR_2019_paper.html"},
  {"id":"kocabasSPECSeeingPeople2021","accessed":{"date-parts":[["2022",11,23]]},"author":[{"family":"Kocabas","given":"Muhammed"},{"family":"Huang","given":"Chun-Hao P."},{"family":"Tesch","given":"Joachim"},{"family":"Müller","given":"Lea"},{"family":"Hilliges","given":"Otmar"},{"family":"Black","given":"Michael J."}],"citation-key":"kocabasSPECSeeingPeople2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11035-11045","source":"openaccess.thecvf.com","title":"SPEC: Seeing People in the Wild With an Estimated Camera","title-short":"SPEC","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Kocabas_SPEC_Seeing_People_in_the_Wild_With_an_Estimated_Camera_ICCV_2021_paper.html"},
  {"id":"kocabasVIBEVideoInference2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Kocabas","given":"Muhammed"},{"family":"Athanasiou","given":"Nikos"},{"family":"Black","given":"Michael J."}],"citation-key":"kocabasVIBEVideoInference2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"5253-5263","source":"openaccess.thecvf.com","title":"VIBE: Video Inference for Human Body Pose and Shape Estimation","title-short":"VIBE","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Kocabas_VIBE_Video_Inference_for_Human_Body_Pose_and_Shape_Estimation_CVPR_2020_paper.html"},
  {"id":"kolotourosLearningReconstruct3D2019","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Kolotouros","given":"Nikos"},{"family":"Pavlakos","given":"Georgios"},{"family":"Black","given":"Michael J."},{"family":"Daniilidis","given":"Kostas"}],"citation-key":"kolotourosLearningReconstruct3D2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"2252-2261","source":"openaccess.thecvf.com","title":"Learning to Reconstruct 3D Human Pose and Shape via Model-Fitting in the Loop","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Kolotouros_Learning_to_Reconstruct_3D_Human_Pose_and_Shape_via_Model-Fitting_ICCV_2019_paper.html"},
  {"id":"kolotourosProbabilisticModelingHuman2021","abstract":"This paper focuses on the problem of 3D human reconstruction from 2D evidence. Although this is an inherently ambiguous problem, the majority of recent works avoid the uncertainty modeling and typically regress a single estimate for a given input. In contrast to that, in this work, we propose to embrace the reconstruction ambiguity and we recast the problem as learning a mapping from the input to a distribution of plausible 3D poses. Our approach is based on the normalizing flows model and offers a series of advantages. For conventional applications, where a single 3D estimate is required, our formulation allows for efficient mode computation. Using the mode leads to performance that is comparable with the state of the art among deterministic unimodal regression models. Simultaneously, since we have access to the likelihood of each sample, we demonstrate that our model is useful in a series of downstream tasks, where we leverage the probabilistic nature of the prediction as a tool for more accurate estimation. These tasks include reconstruction from multiple uncalibrated views, as well as human model fitting, where our model acts as a powerful image-based prior for mesh recovery. Our results validate the importance of probabilistic modeling, and indicate state-of-the-art performance across a variety of settings. Code and models are available at: https://www.seas.upenn.edu/~nkolot/projects/prohmr.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Kolotouros","given":"Nikos"},{"family":"Pavlakos","given":"Georgios"},{"family":"Jayaraman","given":"Dinesh"},{"family":"Daniilidis","given":"Kostas"}],"citation-key":"kolotourosProbabilisticModelingHuman2021","DOI":"10.48550/arXiv.2108.11944","issued":{"date-parts":[["2021",8,26]]},"number":"arXiv:2108.11944","publisher":"arXiv","source":"arXiv.org","title":"Probabilistic Modeling for Human Mesh Recovery","type":"article","URL":"http://arxiv.org/abs/2108.11944"},
  {"id":"kongImprovedMethodWSN2019","abstract":"Aimed at the unreasonable distribution of sensors' random deployment, an improved method of WSN coverage based on enhanced PSO algorithm is presented. The dynamic adjustment of inertia coefficient and mutation operator are introduced to improve the standard PSO algorithm, which can improve the global convergence speed and particle diversity, and can avoid falling into local convergence. The enhanced PSO algorithm is applied to the optimization of WSN coverage, which can increase the maximum coverage. According to the simulation results, compared with PSO in coverage, the enhanced algorithm is superior.","author":[{"family":"Kong","given":"H."},{"family":"Yu","given":"B."}],"citation-key":"kongImprovedMethodWSN2019","container-title":"2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","DOI":"10.1109/ITAIC.2019.8785849","event-title":"2019 IEEE 8th Joint International Information Technology and Artificial Intelligence Conference (ITAIC)","issued":{"date-parts":[["2019",5]]},"page":"1294-1297","source":"IEEE Xplore","title":"An Improved Method of WSN Coverage Based on Enhanced PSO Algorithm","type":"paper-conference"},
  {"id":"kunduSelfSupervised3DHuman2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Kundu","given":"Jogendra Nath"},{"family":"Seth","given":"Siddharth"},{"family":"Jampani","given":"Varun"},{"family":"Rakesh","given":"Mugalodi"},{"family":"Babu","given":"R. Venkatesh"},{"family":"Chakraborty","given":"Anirban"}],"citation-key":"kunduSelfSupervised3DHuman2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"6152-6162","source":"openaccess.thecvf.com","title":"Self-Supervised 3D Human Pose Estimation via Part Guided Novel Image Synthesis","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Kundu_Self-Supervised_3D_Human_Pose_Estimation_via_Part_Guided_Novel_Image_CVPR_2020_paper.html"},
  {"id":"kunduUncertaintyAwareAdaptationSelfSupervised2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Kundu","given":"Jogendra Nath"},{"family":"Seth","given":"Siddharth"},{"family":"Ym","given":"Pradyumna"},{"family":"Jampani","given":"Varun"},{"family":"Chakraborty","given":"Anirban"},{"family":"Babu","given":"R. Venkatesh"}],"citation-key":"kunduUncertaintyAwareAdaptationSelfSupervised2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"20448-20459","source":"openaccess.thecvf.com","title":"Uncertainty-Aware Adaptation for Self-Supervised 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Kundu_Uncertainty-Aware_Adaptation_for_Self-Supervised_3D_Human_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"laffertyConditionalRandomFields2001","author":[{"family":"Lafferty","given":"John"},{"family":"McCallum","given":"Andrew"},{"family":"Pereira","given":"Fernando"}],"citation-key":"laffertyConditionalRandomFields2001","container-title":"Departmental Papers (CIS)","issued":{"date-parts":[["2001",6,28]]},"title":"Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data","title-short":"Conditional Random Fields","type":"article-journal","URL":"https://repository.upenn.edu/cis_papers/159"},
  {"id":"lainaDeeperDepthPrediction2016","abstract":"This paper addresses the problem of estimating the depth map of a scene given a single RGB image. We propose a fully convolutional architecture, encompassing residual learning, to model the ambiguous mapping between monocular images and depth maps. In order to improve the output resolution, we present a novel way to efficiently learn feature map up-sampling within the network. For optimization, we introduce the reverse Huber loss that is particularly suited for the task at hand and driven by the value distributions commonly present in depth maps. Our model is composed of a single architecture that is trained end-to-end and does not rely on post-processing techniques, such as CRFs or other additional refinement steps. As a result, it runs in real-time on images or videos. In the evaluation, we show that the proposed model contains fewer parameters and requires fewer training data than the current state of the art, while outperforming all approaches on depth estimation. Code and models are publicly available.","author":[{"family":"Laina","given":"Iro"},{"family":"Rupprecht","given":"Christian"},{"family":"Belagiannis","given":"Vasileios"},{"family":"Tombari","given":"Federico"},{"family":"Navab","given":"Nassir"}],"citation-key":"lainaDeeperDepthPrediction2016","container-title":"2016 Fourth International Conference on 3D Vision (3DV)","DOI":"10.1109/3DV.2016.32","event-title":"2016 Fourth International Conference on 3D Vision (3DV)","issued":{"date-parts":[["2016",10]]},"page":"239-248","source":"IEEE Xplore","title":"Deeper Depth Prediction with Fully Convolutional Residual Networks","type":"paper-conference"},
  {"id":"leeBAAMMonocular3D2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Lee","given":"Hyo-Jun"},{"family":"Kim","given":"Hanul"},{"family":"Choi","given":"Su-Min"},{"family":"Jeong","given":"Seong-Gyun"},{"family":"Koh","given":"Yeong Jun"}],"citation-key":"leeBAAMMonocular3D2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"9011-9020","source":"openaccess.thecvf.com","title":"BAAM: Monocular 3D Pose and Shape Reconstruction With Bi-Contextual Attention Module and Attention-Guided Modeling","title-short":"BAAM","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Lee_BAAM_Monocular_3D_Pose_and_Shape_Reconstruction_With_Bi-Contextual_Attention_CVPR_2023_paper.html"},
  {"id":"leeDetermination3DHuman1985","abstract":"In this paper a method is proposed to recover and interpret the 3D body structures of a person from a single view, provided that (1) at least six feature points on the head and a set of body joints are available on the image plane, and (2) the geometry of head and lengths of body segments formed by joints are known. First of all, the feature points on the head in the head-centered coordinate system and their image projections are used to determine a transformation matrix. Then, the camera position and orientations are extracted from the matrix. Finally, the 3D coordinates of the head points expressed in the camera-centered coordinate system are obtained. Starting from the coordinates of the neck, which is a head feature point, the 3D coordinates of other joints one-by-one are determined under the assumption of the fixed lengths of the body segments. A binary interpretation tree is used to represent the 2 n  1 possible body structures, if a human body has n joints. To determine the final feasible body structures, physical and motion constraints are used to prune the interpretation tree. Formulas and rules required for the tree pruning are formulated. Experiments are used to illustrate the pruning powers of these constraints. In the two cases of input data chosen, a unique or nearly unique solution of the body structure is obtained.","accessed":{"date-parts":[["2021",9,2]]},"author":[{"family":"Lee","given":"H. J."},{"family":"Chen","given":"Z."}],"citation-key":"leeDetermination3DHuman1985","container-title":"Computer Vision, Graphics, and Image Processing","DOI":"10.1016/0734-189X(85)90094-5","issue":"2","issued":{"date-parts":[["1985"]]},"page":"148-168","source":"Baidu Scholar","title":"Determination of 3D human body postures from a single view","type":"article-journal","URL":"http://www.sciencedirect.com/science/article/pii/0734189X85900945","volume":"30"},
  {"id":"leeDetermination3DHuman1985a","abstract":"In this paper a method is proposed to recover and interpret the 3D body structures of a person from a single view, provided that (1) at least six feature points on the head and a set of body joints are available on the image plane, and (2) the geometry of head and lengths of body segments formed by joints are known. First of all, the feature points on the head in the head-centered coordinate system and their image projections are used to determine a transformation matrix. Then, the camera position and orientations are extracted from the matrix. Finally, the 3D coordinates of the head points expressed in the camera-centered coordinate system are obtained. Starting from the coordinates of the neck, which is a head feature point, the 3D coordinates of other joints one-by-one are determined under the assumption of the fixed lengths of the body segments. A binary interpretation tree is used to represent the 2n − 1 possible body structures, if a human body has n joints. To determine the final feasible body structures, physical and motion constraints are used to prune the interpretation tree. Formulas and rules required for the tree pruning are formulated. Experiments are used to illustrate the pruning powers of these constraints. In the two cases of input data chosen, a unique or nearly unique solution of the body structure is obtained.","accessed":{"date-parts":[["2021",9,2]]},"author":[{"family":"Lee","given":"Hsi-Jian"},{"family":"Chen","given":"Zen"}],"citation-key":"leeDetermination3DHuman1985a","container-title":"Computer Vision, Graphics, and Image Processing","container-title-short":"Computer Vision, Graphics, and Image Processing","DOI":"10.1016/0734-189X(85)90094-5","ISSN":"0734-189X","issue":"2","issued":{"date-parts":[["1985",5,1]]},"language":"en","page":"148-168","source":"ScienceDirect","title":"Determination of 3D human body postures from a single view","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/0734189X85900945","volume":"30"},
  {"id":"leeHumanPoseEstimation2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Lee","given":"Sohyun"},{"family":"Rim","given":"Jaesung"},{"family":"Jeong","given":"Boseung"},{"family":"Kim","given":"Geonu"},{"family":"Woo","given":"Byungju"},{"family":"Lee","given":"Haechan"},{"family":"Cho","given":"Sunghyun"},{"family":"Kwak","given":"Suha"}],"citation-key":"leeHumanPoseEstimation2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"704-714","source":"openaccess.thecvf.com","title":"Human Pose Estimation in Extremely Low-Light Conditions","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Lee_Human_Pose_Estimation_in_Extremely_Low-Light_Conditions_CVPR_2023_paper.html"},
  {"id":"LeiBieMinGanDeQuanJuShiXuGuanLianShiPinDongZuoJianCepdf2022","citation-key":"LeiBieMinGanDeQuanJuShiXuGuanLianShiPinDongZuoJianCepdf2022","container-title":"中国图象图形学报","issued":{"date-parts":[["2022"]]},"title":"类别敏感的全局时序关联视频动作检测.pdf","type":"article-journal"},
  {"id":"leiwangRecognitionDetectionPerceiving2014","abstract":"Visually perceiving human motion at semantic level is an important however challenging problem in multimedia area. In this work, we propose a novel approach to map the low-level responses from visual detection to semantically sensitive description to human actions. The feature map is triggered by the output of deformable part model detection, in which the critical information about body parts conﬁguration is contained implicitly under the speciﬁc human actions. We map the ﬁlter responses of the detectors to an effective feature description, which encodes the position and appearance information of the root and every body parts simultaneously. Statistically, the obtained feature map captures the signiﬁcance of relative conﬁguration of body parts, therefore is robust to the false detections occurred in the individual part detectors. We conduct comprehensive experiments and the results show that the method generates discriminative action features and achieves remarkable performance in most of the cases.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"literal":"Lei Wang"},{"literal":"Jun Wu"},{"literal":"Zhimin Zhou"},{"family":"Liu","given":"Yuncai"},{"family":"Zhao","given":"Xu"}],"citation-key":"leiwangRecognitionDetectionPerceiving2014","container-title":"2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","DOI":"10.1109/ICMEW.2014.6890599","event-place":"Chengdu, China","event-title":"2014 IEEE International Conference on Multimedia and Expo Workshops (ICMEW)","ISBN":"978-1-4799-4717-1","issued":{"date-parts":[["2014",7]]},"language":"en","note":"ZSCC:00001","page":"1-6","publisher":"IEEE","publisher-place":"Chengdu, China","source":"DOI.org (Crossref)","title":"Recognition by detection: Perceiving human motion through part-configured feature maps","title-short":"Recognition by detection","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/6890599/"},
  {"id":"lengDynamicHyperbolicAttention","abstract":"Reconstructing both objects and hands in 3D from a single RGB image is complex. Existing methods rely on manually defined hand-object constraints in Euclidean space, leading to suboptimal feature learning. Compared with Euclidean space, hyperbolic space better preserves the geometric properties of meshes thanks to its exponentiallygrowing space distance, which amplifies the differences between the features based on similarity. In this work, we propose the first precise hand-object reconstruction method in hyperbolic space, namely Dynamic Hyperbolic Attention Network (DHANet), which leverages intrinsic properties of hyperbolic space to learn representative features. Our method that projects mesh and image features into a unified hyperbolic space includes two modules, i.e. dynamic hyperbolic graph convolution and image-attention hyperbolic graph convolution. With these two modules, our method learns mesh features with rich geometry-image multi-modal information and models better hand-object interaction. Our method provides a promising alternative for fine handobject reconstruction in hyperbolic space. Extensive experiments on three public datasets demonstrate that our method outperforms most state-of-the-art methods.","author":[{"family":"Leng","given":"Zhiying"},{"family":"Wu","given":"Shun-Cheng"},{"family":"Saleh","given":"Mahdi"},{"family":"Montanaro","given":"Antonio"},{"family":"Yu","given":"Hao"},{"family":"Wang","given":"Yin"},{"family":"Navab","given":"Nassir"},{"family":"Liang","given":"Xiaohui"},{"family":"Tombari","given":"Federico"}],"citation-key":"lengDynamicHyperbolicAttention","language":"en","source":"Zotero","title":"Dynamic Hyperbolic Attention Network for Fine Hand-object Reconstruction","type":"article-journal"},
  {"id":"li3DPoseDetection2019","abstract":"We propose a method to automatically detect 3D poses of closely interactive humans from sparse multi-view images at one time instance. It is a challenging problem due to the strong partial occlusion and truncation between humans and no tracking process to provide priori poses information. To solve this problem, we first obtain 2D joints in every image using OpenPose and human semantic segmentation results from Mask R-CNN. With the 3D joints triangulated from multi-view 2D joints, a two-stage assembling method is proposed to select the correct 3D pose from thousands of pose seeds combined by joint semantic meanings. We further present a novel approach to minimize the interpenetration between human shapes with close interactions. Finally, we test our method on multi-view human-human interaction (MHHI) datasets. Experimental results demonstrate that our method achieves high visualized correct rate and outperforms the existing method in accuracy and real-time capability.","accessed":{"date-parts":[["2022",2,23]]},"author":[{"family":"Li","given":"Xiu"},{"family":"Fan","given":"Zhen"},{"family":"Liu","given":"Yebin"},{"family":"Li","given":"Yipeng"},{"family":"Dai","given":"Qionghai"}],"citation-key":"li3DPoseDetection2019","container-title":"Sensors","DOI":"10.3390/s19122831","ISSN":"1424-8220","issue":"12","issued":{"date-parts":[["2019",1]]},"language":"en","license":"http://creativecommons.org/licenses/by/3.0/","number":"12","page":"2831","publisher":"Multidisciplinary Digital Publishing Institute","source":"www.mdpi.com","title":"3D Pose Detection of Closely Interactive Humans Using Multi-View Cameras","type":"article-journal","URL":"https://www.mdpi.com/1424-8220/19/12/2831","volume":"19"},
  {"id":"liBimodalGenderRecognition2010","abstract":"This paper focuses on multimodal gender recognition. To achieve a robust and discriminative performance for gender recognition, visual observations from both face and corresponding ﬁngerprints are fused to serve for the task. The bag-of-words model is employed to structure the image representation. We propose a novel supervised method to construct the visual words, by which the redundant feature dimensions are discarded and the important dimensions for gender classiﬁcation are highlighted. The dimension rearrangement is achieved by aligning the feature dimensions to a common normal vector of the hyperplane between categories. The Latent Dirichlet Allocation (LDA) model is extended to incorporate discriminative clues for supervised classiﬁcation. We build the novel Discriminative LDA (DLDA) model by maximizing the inter-class margins, which can signiﬁcantly enhance the discriminative power of the whole model. Experiments on a large face and ﬁngerprint database demonstrate the effectiveness of the proposed new feature and model. Complementary advantages beneﬁted from face-ﬁngerprint fusion to a robust gender recognition framework also get validated.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Li","given":"Xiong"},{"family":"Zhao","given":"Xu"},{"family":"Fu","given":"Yun"},{"family":"Liu","given":"Yuncai"}],"citation-key":"liBimodalGenderRecognition2010","container-title":"2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition","DOI":"10.1109/CVPR.2010.5539969","event-place":"San Francisco, CA, USA","event-title":"2010 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-4244-6984-0","issued":{"date-parts":[["2010",6]]},"language":"en","note":"ZSCC:00038","page":"2590-2597","publisher":"IEEE","publisher-place":"San Francisco, CA, USA","source":"DOI.org (Crossref)","title":"Bimodal gender recognition from face and fingerprint","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/5539969/"},
  {"id":"liBoostingSingleFrame3D2019","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Li","given":"Zhi"},{"family":"Wang","given":"Xuan"},{"family":"Wang","given":"Fei"},{"family":"Jiang","given":"Peilin"}],"citation-key":"liBoostingSingleFrame3D2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"2192-2201","source":"openaccess.thecvf.com","title":"On Boosting Single-Frame 3D Human Pose Estimation via Monocular Videos","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Li_On_Boosting_Single-Frame_3D_Human_Pose_Estimation_via_Monocular_Videos_ICCV_2019_paper.html"},
  {"id":"liCascadedDeepMonocular2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Li","given":"Shichao"},{"family":"Ke","given":"Lei"},{"family":"Pratama","given":"Kevin"},{"family":"Tai","given":"Yu-Wing"},{"family":"Tang","given":"Chi-Keung"},{"family":"Cheng","given":"Kwang-Ting"}],"citation-key":"liCascadedDeepMonocular2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"6173-6183","source":"openaccess.thecvf.com","title":"Cascaded Deep Monocular 3D Human Pose Estimation With Evolutionary Training Data","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Li_Cascaded_Deep_Monocular_3D_Human_Pose_Estimation_With_Evolutionary_Training_CVPR_2020_paper.html"},
  {"id":"liCHORDCategorylevelHandheld","author":[{"family":"Li","given":"Kailin"},{"family":"Yang","given":"Lixin"},{"family":"Zhen","given":"Haoyu"},{"family":"Lin","given":"Zenan"},{"family":"Zhan","given":"Xinyu"},{"family":"Zhong","given":"Licheng"},{"family":"Xu","given":"Jian"},{"family":"Wu","given":"Kejian"},{"family":"Lu","given":"Cewu"}],"citation-key":"liCHORDCategorylevelHandheld","language":"en","source":"Zotero","title":"CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation","type":"article-journal"},
  {"id":"liCLIFFCarryingLocation2022","abstract":"Top-down methods dominate the field of 3D human pose and shape estimation, because they are decoupled from human detection and allow researchers to focus on the core problem. However, cropping, their first step, discards the location information from the very beginning, which makes themselves unable to accurately predict the global rotation in the original camera coordinate system. To address this problem, we propose to Carry Location Information in Full Frames (CLIFF) into this task. Specifically, we feed more holistic features to CLIFF by concatenating the cropped-image feature with its bounding box information. We calculate the 2D reprojection loss with a broader view of the full frame, taking a projection process similar to that of the person projected in the image. Fed and supervised by global-location-aware information, CLIFF directly predicts the global rotation along with more accurate articulated poses. Besides, we propose a pseudo-ground-truth annotator based on CLIFF, which provides high-quality 3D annotations for in-the-wild 2D datasets and offers crucial full supervision for regression-based methods. Extensive experiments on popular benchmarks show that CLIFF outperforms prior arts by a significant margin, and reaches the first place on the AGORA leaderboard (the SMPL-Algorithms track). The code and data are available at https://github.com/huawei-noah/noah-research/tree/master/CLIFF.","author":[{"family":"Li","given":"Zhihao"},{"family":"Liu","given":"Jianzhuang"},{"family":"Zhang","given":"Zhensong"},{"family":"Xu","given":"Songcen"},{"family":"Yan","given":"Youliang"}],"citation-key":"liCLIFFCarryingLocation2022","collection-title":"Lecture Notes in Computer Science","container-title":"Computer Vision – ECCV 2022","DOI":"10.1007/978-3-031-20065-6_34","editor":[{"family":"Avidan","given":"Shai"},{"family":"Brostow","given":"Gabriel"},{"family":"Cissé","given":"Moustapha"},{"family":"Farinella","given":"Giovanni Maria"},{"family":"Hassner","given":"Tal"}],"event-place":"Cham","ISBN":"978-3-031-20065-6","issued":{"date-parts":[["2022"]]},"language":"en","page":"590-606","publisher":"Springer Nature Switzerland","publisher-place":"Cham","source":"Springer Link","title":"CLIFF: Carrying Location Information in Full Frames into Human Pose and Shape Estimation","title-short":"CLIFF","type":"paper-conference"},
  {"id":"liCollectingPuzzlePieces2023","abstract":"Human pose transfer synthesizes new view(s) of a person for a given pose. Recent work achieves this via self-reconstruction, which disentangles a person's pose and texture information by breaking the person down into parts, then recombines them for reconstruction. However, part-level disentanglement preserves some pose information that can create unwanted artifacts. In this paper, we propose Pose Transfer by Permuting Textures (PT$^2$), an approach for self-driven human pose transfer that disentangles pose from texture at the patch-level. Specifically, we remove pose from an input image by permuting image patches so only texture information remains. Then we reconstruct the input image by sampling from the permuted textures for patch-level disentanglement. To reduce noise and recover clothing shape information from the permuted patches, we employ encoders with multiple kernel sizes in a triple branch network. On DeepFashion and Market-1501, PT$^2$ reports significant gains on automatic metrics over other self-driven methods, and even outperforms some fully-supervised methods. A user study also reports images generated by our method are preferred in 68% of cases over self-driven approaches from prior work. Code is available at https://github.com/NannanLi999/pt_square.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Li","given":"Nannan"},{"family":"Shih","given":"Kevin J."},{"family":"Plummer","given":"Bryan A."}],"citation-key":"liCollectingPuzzlePieces2023","DOI":"10.48550/arXiv.2210.01887","issued":{"date-parts":[["2023",8,30]]},"number":"arXiv:2210.01887","publisher":"arXiv","source":"arXiv.org","title":"Collecting The Puzzle Pieces: Disentangled Self-Driven Human Pose Transfer by Permuting Textures","title-short":"Collecting The Puzzle Pieces","type":"article","URL":"http://arxiv.org/abs/2210.01887"},
  {"id":"liCostEfficientSubcategoryaware2017","abstract":"In this paper, we propose an accurate and cost efﬁcient deep CNN network for object detection. In contrast to the previous region-based methods like Sub-CNN [1], our detector is almost fully convolutional so that the computation cost can be reduced efﬁciently. By introducing position-sensitive score maps and exploiting subcategory information, our method is less time consuming while maintaining competitive performance on detecting objects with various scales. In addition, we remove image pyramid used in Sub-CNN to achieve further acceleration. The experimental results show that our approach is 1.3 times faster than Sub-CNN with only 14% number of parameters and archives comparable results on the challenging KITTI dataset. Compared with the state-of-theart methods for object detection, our approach provides an efﬁcient solution that takes into account both accuracy and speed.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Li","given":"Tingfeng"},{"family":"Zhao","given":"Xu"}],"citation-key":"liCostEfficientSubcategoryaware2017","container-title":"2017 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2017.8297074","event-place":"Beijing","event-title":"2017 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-5090-2175-8","issued":{"date-parts":[["2017",9]]},"language":"en","note":"ZSCC:00000","page":"4202-4206","publisher":"IEEE","publisher-place":"Beijing","source":"DOI.org (Crossref)","title":"Cost efficient subcategory-aware CNN for object detection","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/8297074/"},
  {"id":"liDiteHRNetDynamicLightweight2022","abstract":"A high-resolution network exhibits remarkable capability in extracting multi-scale features for human pose estimation, but fails to capture long-range interactions between joints and has high computational complexity. To address these problems, we present a Dynamic lightweight High-Resolution Network (Dite-HRNet), which can efficiently extract multi-scale contextual information and model long-range spatial dependency for human pose estimation. Specifically, we propose two methods, dynamic split convolution and adaptive context modeling, and embed them into two novel lightweight blocks, which are named dynamic multi-scale context block and dynamic global context block. These two blocks, as the basic component units of our Dite-HRNet, are specially designed for the high-resolution networks to make full use of the parallel multi-resolution architecture. Experimental results show that the proposed network achieves superior performance on both COCO and MPII human pose estimation datasets, surpassing the state-of-the-art lightweight networks. Code is available at: https://github.com/ZiyiZhang27/Dite-HRNet.","accessed":{"date-parts":[["2024",4,1]]},"author":[{"family":"Li","given":"Qun"},{"family":"Zhang","given":"Ziyi"},{"family":"Xiao","given":"Fu"},{"family":"Zhang","given":"Feng"},{"family":"Bhanu","given":"Bir"}],"citation-key":"liDiteHRNetDynamicLightweight2022","issued":{"date-parts":[["2022",5,24]]},"number":"arXiv:2204.10762","publisher":"arXiv","source":"arXiv.org","title":"Dite-HRNet: Dynamic Lightweight High-Resolution Network for Human Pose Estimation","title-short":"Dite-HRNet","type":"article","URL":"http://arxiv.org/abs/2204.10762","version":"3"},
  {"id":"liEgoBodyPoseEstimation2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Li","given":"Jiaman"},{"family":"Liu","given":"Karen"},{"family":"Wu","given":"Jiajun"}],"citation-key":"liEgoBodyPoseEstimation2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"17142-17151","source":"openaccess.thecvf.com","title":"Ego-Body Pose Estimation via Ego-Head Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Li_Ego-Body_Pose_Estimation_via_Ego-Head_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"liGeneratingMultipleHypotheses2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Li","given":"Chen"},{"family":"Lee","given":"Gim Hee"}],"citation-key":"liGeneratingMultipleHypotheses2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"9887-9895","source":"openaccess.thecvf.com","title":"Generating Multiple Hypotheses for 3D Human Pose Estimation With Mixture Density Network","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Li_Generating_Multiple_Hypotheses_for_3D_Human_Pose_Estimation_With_Mixture_CVPR_2019_paper.html"},
  {"id":"liHRNeXtHighResolutionContext2023","abstract":"Occlusion handling in crowded scenes is an intractable challenge for human pose estimation. To address this problem, we propose two novel feed-forward network structures named Global Feed-Forward Network (GFFN) and Dynamic Feed-Forward Network (DFFN), which are specifically designed for image-based tasks to capture both local and global contextual information within intermediate features and update feature representations with high adaptability for occlusions. By exploiting the context modeling ability of the proposed GFFN and DFFN, we present a novel backbone network, namely High-Resolution Context Network (HRNeXt), which learns high-resolution representations with abundant contextual information to better estimate poses of occluded human bodies. Compared to state-of-the-art pose estimation networks, our HRNeXt absorbs advantages of convolution operation and attention mechanism, and it is more efficient in terms of training data sizes, network parameters and computational costs. Experimental results show that our HRNeXt significantly outperforms state-of-the-art backbone networks on challenging pose estimation datasets with high occurrence of crowds and occlusions.","accessed":{"date-parts":[["2023",12,4]]},"author":[{"family":"Li","given":"Qun"},{"family":"Zhang","given":"Ziyi"},{"family":"Zhang","given":"Feng"},{"family":"Xiao","given":"Fu"}],"citation-key":"liHRNeXtHighResolutionContext2023","container-title":"IEEE Transactions on Multimedia","DOI":"10.1109/TMM.2023.3248144","ISSN":"1941-0077","issued":{"date-parts":[["2023"]]},"page":"1521-1528","source":"IEEE Xplore","title":"HRNeXt: High-Resolution Context Network for Crowd Pose Estimation","title-short":"HRNeXt","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/10050764","volume":"25"},
  {"id":"liHumanCarvingParsingBased2021","abstract":"Human-centric computer vision tasks often beneﬁt from each other. In this paper, we propose a novel framework called Human Carving to explore the relationships between human parsing and multi-view 3D human reconstruction, which is the ﬁrst method to consider the two related tasks. It consists of three modules: 1) Pose-aware Multi-view Human Parsing, 2) Semantic Visual Hull Carving and 3) Hierarchical Human Model Fitting. Taking the sparse multi-view images as input, the framework automatically generates a Part-Aware Visual Hull (PAVH) of human body parts and then estimates the human shape and pose simultaneously. Experimental results on real scenes demonstrate the effectiveness of our framework.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Li","given":"Baoxing"},{"family":"Zhao","given":"Xu"}],"citation-key":"liHumanCarvingParsingBased2021","container-title":"2021 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP42928.2021.9506068","event-place":"Anchorage, AK, USA","event-title":"2021 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-6654-4115-5","issued":{"date-parts":[["2021",9,19]]},"language":"en","note":"ZSCC:00000","page":"3238-3242","publisher":"IEEE","publisher-place":"Anchorage, AK, USA","source":"DOI.org (Crossref)","title":"Human Carving: A Parsing-Based Framework For 3d Human Reconstruction","title-short":"Human Carving","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9506068/"},
  {"id":"liHumanPoseRegression2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Li","given":"Jiefeng"},{"family":"Bian","given":"Siyuan"},{"family":"Zeng","given":"Ailing"},{"family":"Wang","given":"Can"},{"family":"Pang","given":"Bo"},{"family":"Liu","given":"Wentao"},{"family":"Lu","given":"Cewu"}],"citation-key":"liHumanPoseRegression2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11025-11034","source":"openaccess.thecvf.com","title":"Human Pose Regression With Residual Log-Likelihood Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Li_Human_Pose_Regression_With_Residual_Log-Likelihood_Estimation_ICCV_2021_paper.html"},
  {"id":"liHybrIKHybridAnalyticalNeural2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Li","given":"Jiefeng"},{"family":"Xu","given":"Chao"},{"family":"Chen","given":"Zhicun"},{"family":"Bian","given":"Siyuan"},{"family":"Yang","given":"Lixin"},{"family":"Lu","given":"Cewu"}],"citation-key":"liHybrIKHybridAnalyticalNeural2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"3383-3393","source":"openaccess.thecvf.com","title":"HybrIK: A Hybrid Analytical-Neural Inverse Kinematics Solution for 3D Human Pose and Shape Estimation","title-short":"HybrIK","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Li_HybrIK_A_Hybrid_Analytical-Neural_Inverse_Kinematics_Solution_for_3D_Human_CVPR_2021_paper.html"},
  {"id":"liLiDARCapLongRangeMarkerLess2022","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Li","given":"Jialian"},{"family":"Zhang","given":"Jingyi"},{"family":"Wang","given":"Zhiyong"},{"family":"Shen","given":"Siqi"},{"family":"Wen","given":"Chenglu"},{"family":"Ma","given":"Yuexin"},{"family":"Xu","given":"Lan"},{"family":"Yu","given":"Jingyi"},{"family":"Wang","given":"Cheng"}],"citation-key":"liLiDARCapLongRangeMarkerLess2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"20502-20512","source":"openaccess.thecvf.com","title":"LiDARCap: Long-Range Marker-Less 3D Human Motion Capture With LiDAR Point Clouds","title-short":"LiDARCap","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Li_LiDARCap_Long-Range_Marker-Less_3D_Human_Motion_Capture_With_LiDAR_Point_CVPR_2022_paper.html"},
  {"id":"liMegaDepthLearningSingleView2018","accessed":{"date-parts":[["2022",11,24]]},"author":[{"family":"Li","given":"Zhengqi"},{"family":"Snavely","given":"Noah"}],"citation-key":"liMegaDepthLearningSingleView2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"2041-2050","source":"openaccess.thecvf.com","title":"MegaDepth: Learning Single-View Depth Prediction From Internet Photos","title-short":"MegaDepth","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Li_MegaDepth_Learning_Single-View_CVPR_2018_paper.html"},
  {"id":"liMHFormerMultiHypothesisTransformer2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Li","given":"Wenhao"},{"family":"Liu","given":"Hong"},{"family":"Tang","given":"Hao"},{"family":"Wang","given":"Pichao"},{"family":"Van Gool","given":"Luc"}],"citation-key":"liMHFormerMultiHypothesisTransformer2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13147-13156","source":"openaccess.thecvf.com","title":"MHFormer: Multi-Hypothesis Transformer for 3D Human Pose Estimation","title-short":"MHFormer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Li_MHFormer_Multi-Hypothesis_Transformer_for_3D_Human_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"liMultimodalityGenderEstimation2010","abstract":"We propose to estimate human gender from corresponding ﬁngerprint and face information with the Bayesian hierarchical model. Different from previous works on ﬁngerprint based gender estimation with specially designed features, our method extends to use general local image features. Furthermore, a novel word representation called latent word is designed to work with the Bayesian hierarchical model. The feature representation is embedded to our multimodality model, within which the information from ﬁngerprint and face is fused at the decision level for gender estimation. Experiments on our internal database show the promising performance.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Li","given":"Xiong"},{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Huanxi"},{"family":"Fu","given":"Yun"},{"family":"Liu","given":"Yuncai"}],"citation-key":"liMultimodalityGenderEstimation2010","container-title":"2010 IEEE International Conference on Acoustics, Speech and Signal Processing","DOI":"10.1109/ICASSP.2010.5495242","event-place":"Dallas, TX, USA","event-title":"2010 IEEE International Conference on Acoustics, Speech and Signal Processing","ISBN":"978-1-4244-4295-9","issued":{"date-parts":[["2010"]]},"language":"en","note":"ZSCC:00004","page":"5590-5593","publisher":"IEEE","publisher-place":"Dallas, TX, USA","source":"DOI.org (Crossref)","title":"Multimodality gender estimation using Bayesian hierarchical model","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/5495242/"},
  {"id":"linBSNBoundarySensitive2018","abstract":"Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover truth action instances with high recall and high overlap using relatively fewer proposals. To address these diﬃculties, we introduce an eﬀective proposal generation method, named Boundary-Sensitive Network (BSN), which adopts “local to global ” fashion. Locally , BSN ﬁrst locates temporal boundaries with high probabilities, then directly combines these boundaries as proposals. Globally , with Boundary-Sensitive Proposal feature, BSN retrieves proposals by evaluating the conﬁdence of whether a proposal contains an action within its region. We conduct experiments on two challenging datasets: ActivityNet-1.3 and THUMOS14, where BSN outperforms other state-of-the-art temporal action proposal generation methods with high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classiﬁers, our method signiﬁcantly improves the state-of-the-art temporal action detection performance.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Lin","given":"Tianwei"},{"family":"Zhao","given":"Xu"},{"family":"Su","given":"Haisheng"},{"family":"Wang","given":"Chongjing"},{"family":"Yang","given":"Ming"}],"citation-key":"linBSNBoundarySensitive2018","container-title":"Computer Vision – ECCV 2018","DOI":"10.1007/978-3-030-01225-0_1","event-place":"Cham","ISBN":"978-3-030-01224-3 978-3-030-01225-0","issued":{"date-parts":[["2018"]]},"language":"en","note":"ZSCC:00585","page":"3-21","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"BSN: Boundary Sensitive Network for Temporal Action Proposal Generation","title-short":"BSN","type":"paper-conference","URL":"https://link.springer.com/10.1007/978-3-030-01225-0_1","volume":"11208"},
  {"id":"linCrossDomain3DHand2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Lin","given":"Qiuxia"},{"family":"Yang","given":"Linlin"},{"family":"Yao","given":"Angela"}],"citation-key":"linCrossDomain3DHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"17184-17193","source":"openaccess.thecvf.com","title":"Cross-Domain 3D Hand Pose Estimation With Dual Modalities","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Cross-Domain_3D_Hand_Pose_Estimation_With_Dual_Modalities_CVPR_2023_paper.html"},
  {"id":"linEndEndHumanPose2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Lin","given":"Kevin"},{"family":"Wang","given":"Lijuan"},{"family":"Liu","given":"Zicheng"}],"citation-key":"linEndEndHumanPose2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"1954-1963","source":"openaccess.thecvf.com","title":"End-to-End Human Pose and Mesh Reconstruction with Transformers","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Lin_End-to-End_Human_Pose_and_Mesh_Reconstruction_with_Transformers_CVPR_2021_paper.html"},
  {"id":"liNIKINeuralInverse2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Li","given":"Jiefeng"},{"family":"Bian","given":"Siyuan"},{"family":"Liu","given":"Qi"},{"family":"Tang","given":"Jiasheng"},{"family":"Wang","given":"Fan"},{"family":"Lu","given":"Cewu"}],"citation-key":"liNIKINeuralInverse2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12933-12942","source":"openaccess.thecvf.com","title":"NIKI: Neural Inverse Kinematics With Invertible Neural Networks for 3D Human Pose and Shape Estimation","title-short":"NIKI","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Li_NIKI_Neural_Inverse_Kinematics_With_Invertible_Neural_Networks_for_3D_CVPR_2023_paper.html"},
  {"id":"linJointLearningLocal2020","abstract":"Temporal action proposal generation is an important yet challenging problem, since temporal proposals with rich action content are indispensable for analysing real-world videos with long duration and high proportion irrelevant content. This problem requires methods not only generating proposals with precise temporal boundaries, but also retrieving proposals to cover ground truth action instances with high recall and high overlap using relatively fewer proposals. To address these difﬁculties, we introduce an effective and efﬁcient proposal generation method, named Local-Global Network (LGN), by which local and global contexts are jointly learned to generate high quality proposals. Locally, LGN ﬁrst locates temporal boundaries with high starting and ending probabilities separately, then directly combines these boundaries as proposals. Globally, LGN evaluates the actionness probability of multiple-durations temporal regions simultaneously using temporal convolutional layers and anchor mechanism. Finally, we combine the boundary probabilities of each proposal with actionness probability of matched temporal regions as the conﬁdence score, which is used for retrieving proposals. We conduct experiments on two datasets: ActivityNet-1.3 and THUMOS-14, where LGN outperforms other state-of-theart methods with both high recall and high temporal precision. Finally, further experiments demonstrate that by combining existing action classiﬁers, our method signiﬁcantly improves the state-of-the-art temporal action detection performance.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Lin","given":"Tianwei"},{"family":"Zhao","given":"Xu"},{"family":"Su","given":"Haisheng"}],"citation-key":"linJointLearningLocal2020","container-title":"IEEE Transactions on Circuits and Systems for Video Technology","container-title-short":"IEEE Trans. Circuits Syst. Video Technol.","DOI":"10.1109/TCSVT.2019.2962063","ISSN":"1051-8215, 1558-2205","issue":"12","issued":{"date-parts":[["2020",12]]},"language":"en","note":"ZSCC:00010","number":"12","page":"4899-4912","source":"DOI.org (Crossref)","title":"Joint Learning of Local and Global Context for Temporal Action Proposal Generation","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/8941024/","volume":"30"},
  {"id":"linMultiViewMultiPerson3D2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Lin","given":"Jiahao"},{"family":"Lee","given":"Gim Hee"}],"citation-key":"linMultiViewMultiPerson3D2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"11886-11895","source":"openaccess.thecvf.com","title":"Multi-View Multi-Person 3D Pose Estimation With Plane Sweep Stereo","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Lin_Multi-View_Multi-Person_3D_Pose_Estimation_With_Plane_Sweep_Stereo_CVPR_2021_paper.html"},
  {"id":"linRecurrent3DPose2017","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Lin","given":"Mude"},{"family":"Lin","given":"Liang"},{"family":"Liang","given":"Xiaodan"},{"family":"Wang","given":"Keze"},{"family":"Cheng","given":"Hui"}],"citation-key":"linRecurrent3DPose2017","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2017"]]},"page":"810-819","source":"openaccess.thecvf.com","title":"Recurrent 3D Pose Sequence Machines","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2017/html/Lin_Recurrent_3D_Pose_CVPR_2017_paper.html"},
  {"id":"linSingleShotTemporal2017","abstract":"Temporal action detection is a very important yet challenging problem, since videos in real applications are usually long, untrimmed and contain multiple action instances. This problem requires not only recognizing action categories but also detecting start time and end time of each action instance. Many state-of-the-art methods adopt the \"detection by classi cation\" framework: rst do proposal, and then classify proposals. The main drawback of this framework is that the boundaries of action instance proposals have been xed during the classi cation step. To address this issue, we propose a novel Single Shot Action Detector (SSAD) network based on 1D temporal convolutional layers to skip the proposal generation step via directly detecting action instances in untrimmed video. On pursuit of designing a particular SSAD network that can work e ectively for temporal action detection, we empirically search for the best network architecture of SSAD due to lacking existing models that can be directly adopted. Moreover, we investigate into input feature types and fusion strategies to further improve detection accuracy. We conduct extensive experiments on two challenging datasets: THUMOS 2014 and MEXaction2. When setting Intersection-over-Union threshold to 0.5 during evaluation, SSAD signi cantly outperforms other state-of-the-art systems by increasing mAP from 19.0% to 24.6% on THUMOS 2014 and from 7.4% to 11.0% on MEXaction2.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Lin","given":"Tianwei"},{"family":"Zhao","given":"Xu"},{"family":"Shou","given":"Zheng"}],"citation-key":"linSingleShotTemporal2017","container-title":"Proceedings of the 25th ACM international conference on Multimedia","DOI":"10.1145/3123266.3123343","event-place":"Mountain View California USA","event-title":"MM '17: ACM Multimedia Conference","ISBN":"978-1-4503-4906-2","issued":{"date-parts":[["2017",10,19]]},"language":"en","note":"ZSCC:00378","page":"988-996","publisher":"ACM","publisher-place":"Mountain View California USA","source":"DOI.org (Crossref)","title":"Single Shot Temporal Action Detection","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3123266.3123343"},
  {"id":"linTemporalActionLocalization2017","abstract":"Temporal Action localization is a more challenging vision task than action recognition because videos to be analyzed are usually untrimmed and contain multiple action instances. In this paper, we investigate the potential of recurrent neural network, toward three critical aspects for solving this problem, namely, high-performance feature, high-quality temporal segments and effective recurrent neural network architecture. First of all, we introduce the two-stream (spatial and temporal) network for feature extraction. Then, we propose a novel temporal selective search method to generate temporal segments with variable lengths. Finally, we design a twobranch LSTM architecture for category prediction and conﬁdence score computation. Our proposed approach to action localization, along with the key components, say, segments generation and classiﬁcation architecture, are evaluated on the THUMOS’14 dataset and achieve promising performance by comparing with other state-of-the-art methods.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Lin","given":"Tianwei"},{"family":"Zhao","given":"Xu"},{"family":"Fan","given":"Zhaoxuan"}],"citation-key":"linTemporalActionLocalization2017","container-title":"2017 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2017.8296913","event-place":"Beijing","event-title":"2017 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-5090-2175-8","issued":{"date-parts":[["2017",9]]},"language":"en","note":"ZSCC:00008","page":"3400-3404","publisher":"IEEE","publisher-place":"Beijing","source":"DOI.org (Crossref)","title":"Temporal action localization with two-stream segment-based RNN","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/8296913/"},
  {"id":"liOmniFusion360Monocular2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Li","given":"Yuyan"},{"family":"Guo","given":"Yuliang"},{"family":"Yan","given":"Zhixin"},{"family":"Huang","given":"Xinyu"},{"family":"Duan","given":"Ye"},{"family":"Ren","given":"Liu"}],"citation-key":"liOmniFusion360Monocular2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"2801-2810","source":"openaccess.thecvf.com","title":"OmniFusion: 360 Monocular Depth Estimation via Geometry-Aware Fusion","title-short":"OmniFusion","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Li_OmniFusion_360_Monocular_Depth_Estimation_via_Geometry-Aware_Fusion_CVPR_2022_paper.html"},
  {"id":"liOnlineKnowledgeDistillation2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Li","given":"Zheng"},{"family":"Ye","given":"Jingwen"},{"family":"Song","given":"Mingli"},{"family":"Huang","given":"Ying"},{"family":"Pan","given":"Zhigeng"}],"citation-key":"liOnlineKnowledgeDistillation2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11740-11750","source":"openaccess.thecvf.com","title":"Online Knowledge Distillation for Efficient Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Li_Online_Knowledge_Distillation_for_Efficient_Pose_Estimation_ICCV_2021_paper.html"},
  {"id":"liSimultaneousFaceDetection2019","abstract":"In this paper, we present a fast and uniﬁed framework for simultaneous face detection and 3D pose (pitch, yaw, roll) estimation of unconstrained faces using deep convolutional neural networks (CNN). Face detection is implemented with region-based framework as previous work like Faster RCNN. We model the pose estimation as a classiﬁcation and regression problem: ﬁrst divide continuous head poses into several discrete clusters, then adjust poses within each class with a classspeciﬁc regressor to achieve more accurate results. All classiﬁcation and regressions for the two tasks are trained and tested simultaneously in one uniﬁed network. Our approach runs at 10 fps, which is the fastest implementation among the recently proposed methods as far as we know. Moreover, it is able to predict pose without using any 3D information. Extensive evaluations on several challenging benchmarks such as AFLW and AFW demonstrate the eﬀectiveness of the proposed method with competitive results.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Li","given":"Tingfeng"},{"family":"Zhao","given":"Xu"}],"citation-key":"liSimultaneousFaceDetection2019","container-title":"Computer Vision – ACCV 2018","DOI":"10.1007/978-3-030-20887-5_12","event-place":"Cham","ISBN":"978-3-030-20886-8 978-3-030-20887-5","issued":{"date-parts":[["2019"]]},"language":"en","note":"ZSCC:00001","page":"187-202","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Simultaneous Face Detection and Head Pose Estimation: A Fast and Unified Framework","title-short":"Simultaneous Face Detection and Head Pose Estimation","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-030-20887-5_12","volume":"11361"},
  {"id":"liSurvey3DHand2019","abstract":"3D Hand pose estimation has received an increasing amount of attention, especially since consumer depth cameras came onto the market in 2010. Although substantial progress has occurred recently, no overview has kept up with the latest developments. To bridge the gap, we provide a comprehensive survey, including depth cameras, hand pose estimation methods, and public benchmark datasets. First, a markerless approach is proposed to evaluate the tracking accuracy of depth cameras with the aid of a numerical control linear motion guide. Traditional approaches focus only on static characteristics. The evaluation of dynamic tracking capability has been long neglected. Second, we summarize the state-of-the-art methods and analyze the lines of research. Third, existing benchmark datasets and evaluation criteria are identified to provide further insight into the field of hand pose estimation. In addition, realistic challenges, recent trends, dataset creation and annotation, and open problems for future research directions are also discussed.","accessed":{"date-parts":[["2023",10,11]]},"author":[{"family":"Li","given":"Rui"},{"family":"Liu","given":"Zhenyu"},{"family":"Tan","given":"Jianrong"}],"citation-key":"liSurvey3DHand2019","container-title":"Pattern Recognition","container-title-short":"Pattern Recognition","DOI":"10.1016/j.patcog.2019.04.026","ISSN":"0031-3203","issued":{"date-parts":[["2019",9,1]]},"page":"251-272","source":"ScienceDirect","title":"A survey on 3D hand pose estimation: Cameras, methods, and datasets","title-short":"A survey on 3D hand pose estimation","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S0031320319301724","volume":"93"},
  {"id":"liTokenPoseLearningKeypoint2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Li","given":"Yanjie"},{"family":"Zhang","given":"Shoukui"},{"family":"Wang","given":"Zhicheng"},{"family":"Yang","given":"Sen"},{"family":"Yang","given":"Wankou"},{"family":"Xia","given":"Shu-Tao"},{"family":"Zhou","given":"Erjin"}],"citation-key":"liTokenPoseLearningKeypoint2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11313-11322","source":"openaccess.thecvf.com","title":"TokenPose: Learning Keypoint Tokens for Human Pose Estimation","title-short":"TokenPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Li_TokenPose_Learning_Keypoint_Tokens_for_Human_Pose_Estimation_ICCV_2021_paper.html"},
  {"id":"liuDeepDualConsecutive2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Liu","given":"Zhenguang"},{"family":"Chen","given":"Haoming"},{"family":"Feng","given":"Runyang"},{"family":"Wu","given":"Shuang"},{"family":"Ji","given":"Shouling"},{"family":"Yang","given":"Bailin"},{"family":"Wang","given":"Xun"}],"citation-key":"liuDeepDualConsecutive2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"525-534","source":"openaccess.thecvf.com","title":"Deep Dual Consecutive Network for Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Liu_Deep_Dual_Consecutive_Network_for_Human_Pose_Estimation_CVPR_2021_paper.html"},
  {"id":"liuFastHumanPose2023","abstract":"Current approaches for human pose estimation in videos can be categorized into per-frame and warping-based methods. Both approaches have their pros and cons. For example, per-frame methods are generally more accurate, but they are often slow. Warping-based approaches are more efficient, but the performance is usually not good. To bridge the gap, in this paper, we propose a novel fast framework for human pose estimation to meet the real-time inference with controllable accuracy degradation in compressed video domain. Our approach takes advantage of the motion representation (called “motion vector”) that is readily available in a compressed video. Pose joints in a frame are obtained by directly warping the pose joints from the previous frame using the motion vectors. We also propose modules to correct possible errors introduced by the pose warping when needed. Extensive experimental results demonstrate the effectiveness of our proposed framework for accelerating the speed of top-down human pose estimation in videos.","accessed":{"date-parts":[["2023",12,4]]},"author":[{"family":"Liu","given":"Huan"},{"family":"Liu","given":"Wentao"},{"family":"Chi","given":"Zhixiang"},{"family":"Wang","given":"Yang"},{"family":"Yu","given":"Yuanhao"},{"family":"Chen","given":"Jun"},{"family":"Tang","given":"Jin"}],"citation-key":"liuFastHumanPose2023","container-title":"IEEE Transactions on Multimedia","DOI":"10.1109/TMM.2022.3141888","ISSN":"1941-0077","issued":{"date-parts":[["2023"]]},"page":"1390-1400","source":"IEEE Xplore","title":"Fast Human Pose Estimation in Compressed Videos","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9677941","volume":"25"},
  {"id":"liuImproving3DHuman2019","abstract":"3D human pose estimation from monocular images has become a heated area in computer vision recently. For years, most deep neural network based practices have adopted either an end-to-end approach, or a two-stage approach. An end-to-end network typically estimates 3D human poses directly from 2D input images, but it suffers from the shortage of 3D human pose data. It is also obscure to know if the inaccuracy stems from limited visual under-standing or 2D-to-3D mapping. Whereas a two-stage directly lifts those 2D keypoint outputs to the 3D space, after utilizing an existing network for 2D keypoint detections. However, they tend to ignore some useful contextual hints from the 2D raw image pixels. In this paper, we introduce a two-stage architecture that can eliminate the main disadvantages of both these approaches. During the first stage we use an existing state-of-the-art detector to estimate 2D poses. To add more con-textual information to help lifting 2D poses to 3D poses, we propose 3D Part Affinity Fields (3D-PAFs). We use 3D-PAFs to infer 3D limb vectors, and combine them with 2D poses to regress the 3D coordinates. We trained and tested our proposed framework on Human3.6M, the most popular 3D human pose benchmark dataset. Our approach achieves the state-of-the-art performance, which proves that with right selections of contextual information, a simple regression model can be very powerful in estimating 3D poses.","author":[{"family":"Liu","given":"Ding"},{"family":"Zhao","given":"Zixu"},{"family":"Wang","given":"Xinchao"},{"family":"Hu","given":"Yuxiao"},{"family":"Zhang","given":"Lei"},{"family":"Huang","given":"Thomas"}],"citation-key":"liuImproving3DHuman2019","container-title":"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)","DOI":"10.1109/WACV.2019.00112","event-title":"2019 IEEE Winter Conference on Applications of Computer Vision (WACV)","ISSN":"1550-5790","issued":{"date-parts":[["2019",1]]},"page":"1004-1013","source":"IEEE Xplore","title":"Improving 3D Human Pose Estimation Via 3D Part Affinity Fields","type":"paper-conference"},
  {"id":"liuKernelLeastMeanSquareAlgorithm2008","abstract":"The combination of the famed kernel trick and the least-mean-square (LMS) algorithm provides an interesting sample-by-sample update for an adaptive filter in reproducing kernel Hilbert spaces (RKHS), which is named in this paper the KLMS. Unlike the accepted view in kernel methods, this paper shows that in the finite training data case, the KLMS algorithm is well posed in RKHS without the addition of an extra regularization term to penalize solution norms as was suggested by Kivinen [Kivinen, Smola and Williamson, ldquoOnline Learning With Kernels,rdquo IEEE Transactions on Signal Processing, vol. 52, no. 8, pp. 2165-2176, Aug. 2004] and Smale [Smale and Yao, ldquoOnline Learning Algorithms,rdquo Foundations in Computational Mathematics, vol. 6, no. 2, pp. 145-176, 2006]. This result is the main contribution of the paper and enhances the present understanding of the LMS algorithm with a machine learning perspective. The effect of the KLMS step size is also studied from the viewpoint of regularization. Two experiments are presented to support our conclusion that with finite data the KLMS algorithm can be readily used in high dimensional spaces and particularly in RKHS to derive nonlinear, stable algorithms with comparable performance to batch, regularized solutions.","author":[{"family":"Liu","given":"Weifeng"},{"family":"Pokharel","given":"Puskal P."},{"family":"Principe","given":"Jose C."}],"citation-key":"liuKernelLeastMeanSquareAlgorithm2008","container-title":"IEEE Transactions on Signal Processing","DOI":"10.1109/TSP.2007.907881","ISSN":"1941-0476","issue":"2","issued":{"date-parts":[["2008",2]]},"page":"543-554","source":"IEEE Xplore","title":"The Kernel Least-Mean-Square Algorithm","type":"article-journal","volume":"56"},
  {"id":"liuMarkerlessMotionCapture2011","abstract":"We present a markerless motion capture approach that reconstructs the skeletal motion and detailed time-varying surface geometry of two closely interacting people from multi-view video. Due to ambiguities in feature-to-person assignments and frequent occlusions, it is not feasible to directly apply single-person capture approaches to the multi-person case. We therefore propose a combined image segmentation and tracking approach to overcome these difficulties. A new probabilistic shape and appearance model is employed to segment the input images and to assign each pixel uniquely to one person. Thereafter, a single-person markerless motion and surface capture approach can be applied to each individual, either one-by-one or in parallel, even under strong occlusions. We demonstrate the performance of our approach on several challenging multi-person motions, including dance and martial arts, and also provide a reference dataset for multi-person motion capture with ground truth.","author":[{"family":"Liu","given":"Yebin"},{"family":"Stoll","given":"Carsten"},{"family":"Gall","given":"Juergen"},{"family":"Seidel","given":"Hans-Peter"},{"family":"Theobalt","given":"Christian"}],"citation-key":"liuMarkerlessMotionCapture2011","container-title":"CVPR 2011","DOI":"10.1109/CVPR.2011.5995424","event-title":"CVPR 2011","ISSN":"1063-6919","issued":{"date-parts":[["2011",6]]},"page":"1249-1256","source":"IEEE Xplore","title":"Markerless motion capture of interacting characters using multi-view image segmentation","type":"paper-conference"},
  {"id":"liuMultilevelDisparityReconstruction2022","abstract":"Recently, stereo matching algorithms based on end-to-end convolutional neural networks achieve excellent performance far exceeding traditional algorithms. Current state-of-the-art stereo matching networks mostly rely on full cost volume and 3D convolutions to regress dense disparity maps. These modules are computationally complex and high consumption of memory, and diﬃcult to deploy in real-time applications. To overcome this problem, we propose multilevel disparity reconstruction network, MDRNet, a lightweight stereo matching network without any 3D convolutions. We use stacked residual pyramids to gradually reconstruct disparity maps from low-level resolution to full-level resolution, replacing common 3D computation and optimization convolutions. Our approach achieves a competitive performance compared with other algorithms on stereo benchmarks and real-time inference at 30 frames per second with 4×104 resolutions.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Liu","given":"Zhuoran"},{"family":"Zhao","given":"Xu"}],"citation-key":"liuMultilevelDisparityReconstruction2022","container-title":"Journal of Shanghai Jiaotong University (Science)","container-title-short":"J. Shanghai Jiaotong Univ. (Sci.)","DOI":"10.1007/s12204-022-2435-4","ISSN":"1007-1172, 1995-8188","issue":"5","issued":{"date-parts":[["2022",10]]},"language":"en","note":"ZSCC:00000","number":"5","page":"715-722","source":"DOI.org (Crossref)","title":"Multilevel Disparity Reconstruction Network for Real-Time Stereo Matching","type":"article-journal","URL":"https://link.springer.com/10.1007/s12204-022-2435-4","volume":"27"},
  {"id":"liuPARISPartlevelReconstruction","abstract":"We address the task of simultaneous part-level reconstruction and motion parameter estimation for articulated objects. Given two sets of multi-view images of an object in two static articulation states, we decouple the movable part from the static part and reconstruct shape and appearance while predicting the motion parameters. To tackle this problem, we present PARIS: a self-supervised, end-to-end architecture that learns part-level implicit shape and appearance models and optimizes motion parameters jointly without any 3D supervision, motion, or semantic annotation. Our experiments show that our method generalizes better across object categories, and outperforms baselines and prior work that are given 3D point clouds as input. Our approach improves reconstruction relative to state-ofthe-art baselines with a Chamfer-L1 distance reduction of 3.94 (45.2%) for objects and 26.79 (84.5%) for parts, and achieves 5% error rate for motion estimation across 10 object categories.","author":[{"family":"Liu","given":"Jiayi"},{"family":"Mahdavi-Amiri","given":"Ali"},{"family":"Savva","given":"Manolis"}],"citation-key":"liuPARISPartlevelReconstruction","language":"en","source":"Zotero","title":"PARIS: Part-level Reconstruction and Motion Analysis for Articulated Objects","type":"article-journal"},
  {"id":"liuPoseExaminerAutomatedTesting2023","abstract":"Human pose and shape (HPS) estimation methods achieve remarkable results. However, current HPS benchmarks are mostly designed to test models in scenarios that are similar to the training data. This can lead to critical situations in real-world applications when the observed data differs significantly from the training data and hence is out-of-distribution (OOD). It is therefore important to test and improve the OOD robustness of HPS methods. To address this fundamental problem, we develop a simulator that can be controlled in a fine-grained manner using interpretable parameters to explore the manifold of images of human pose, e.g. by varying poses, shapes, and clothes. We introduce a learning-based testing method, termed PoseExaminer, that automatically diagnoses HPS algorithms by searching over the parameter space of human pose images to find the failure modes. Our strategy for exploring this high-dimensional parameter space is a multi-agent reinforcement learning system, in which the agents collaborate to explore different parts of the parameter space. We show that our PoseExaminer discovers a variety of limitations in current state-of-the-art models that are relevant in real-world scenarios but are missed by current benchmarks. For example, it finds large regions of realistic human poses that are not predicted correctly, as well as reduced performance for humans with skinny and corpulent body shapes. In addition, we show that fine-tuning HPS methods by exploiting the failure modes found by PoseExaminer improve their robustness and even their performance on standard benchmarks by a significant margin. The code are available for research purposes.","accessed":{"date-parts":[["2023",5,15]]},"author":[{"family":"Liu","given":"Qihao"},{"family":"Kortylewski","given":"Adam"},{"family":"Yuille","given":"Alan"}],"citation-key":"liuPoseExaminerAutomatedTesting2023","DOI":"10.48550/arXiv.2303.07337","issued":{"date-parts":[["2023",3,30]]},"number":"arXiv:2303.07337","publisher":"arXiv","source":"arXiv.org","title":"PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation","title-short":"PoseExaminer","type":"article","URL":"http://arxiv.org/abs/2303.07337"},
  {"id":"liuPoseExaminerAutomatedTesting2023a","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Liu","given":"Qihao"},{"family":"Kortylewski","given":"Adam"},{"family":"Yuille","given":"Alan L."}],"citation-key":"liuPoseExaminerAutomatedTesting2023a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"672-681","source":"openaccess.thecvf.com","title":"PoseExaminer: Automated Testing of Out-of-Distribution Robustness in Human Pose and Shape Estimation","title-short":"PoseExaminer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Liu_PoseExaminer_Automated_Testing_of_Out-of-Distribution_Robustness_in_Human_Pose_and_CVPR_2023_paper.html"},
  {"id":"liuQuantizedSegmentationFibrotic2017","abstract":"The pathogenesis of atrial fibrillation (AF) is closely related to the fibrotic tissues in left atrial (LA). Delayenhancement magnetic resonance imaging (DE-MRI) has been widely used in the ablation of atrial fibrillation, which can accurately describe the distribution of myocardial fibrosis and postoperative scars. Combining EM algorithm, level-set and graph-cut, this paper proposes a method to segment the fibrotic tissues and postoperative scars and also quantify their proportion in left atrial from DE-MRI. In 4 clinical cases, our method can accomplish the extraction of heart, the segmentation of left atrium and sequentially the quantification of the fibrotic tissues nearby with little manual intervention. Experimental results show that accurate segmentation of LA is achieved in 55 slices with 96 slices containing LA among 4 cases in total. With manual correction in the rest slices, the final results about the proportion of fibrotic tissues in LA are 14.78%, 21.02%, 25.17%, 14.77% respectively which are consistent with the clinical diagnosis. Evaluated by the clinician, our method is robust against different resolution and can provide auxiliary function for ablation of AF.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Liu","given":"Xun"},{"family":"Shen","given":"Yan"},{"family":"Zhao","given":"Xu"},{"family":"Zhang","given":"Su"}],"citation-key":"liuQuantizedSegmentationFibrotic2017","container-title":"2017 International Conference on Machine Vision and Information Technology (CMVIT)","DOI":"10.1109/CMVIT.2017.13","event-place":"Singapore, Singapore","event-title":"2017 International Conference on Machine Vision and Information Technology (CMVIT)","ISBN":"978-1-5090-4993-6","issued":{"date-parts":[["2017",2]]},"language":"en","note":"ZSCC:00003","page":"23-27","publisher":"IEEE","publisher-place":"Singapore, Singapore","source":"DOI.org (Crossref)","title":"Quantized Segmentation of Fibrotic Tissue of Left Atrial from Delay-Enhancement MRI Images Using Level-Set and Graph-Cut","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7878649/"},
  {"id":"liuSegmentationLeftAtrium2018","abstract":"Rationale and Objectives: Catheter ablation is a major treatment for atrial ﬁbrillation (AF), and its effect is closely related to the accuracy of left atrium (LA) segmentation. 3D magnetic resonance imaging (MRI) can be used to depict the cardiac structure with advantages of high resolution and non-radiation. However, obtaining an accurate segmentation of left atrium in MRI is difﬁcult because of complex anatomy and variation in image quality. Materials and Methods: We propose a model to achieve accurate and efﬁcient segmentation of LA by combining convolutional neural network (CNN) and recurrent neural network (RNN). To apply the deep network architecture to medical image processing, we modify the network structure and loss function to adapt the problem. The model was evaluated by using the Left Atrium Segmentation Challenge benchmark, with 10 patients as train set and 20 patients as test set. Results: Visualization and numerical results, which were obtained from the segmentation challenge, denote that our model can produce reasonable segmentation meshes and achieve thestate-of-the-art performance in Dice Index (0.9387) and efﬁciency levels (23 s per volume in GPU). Conclusions: Our automated model exhibits accuracy and efﬁciency and can play a supporting role in ablation surgery by providing a nearly real-time image guide.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Liu","given":"Xun"},{"family":"Shen","given":"Yan"},{"family":"Zhang","given":"Su"},{"family":"Zhao","given":"Xu"}],"citation-key":"liuSegmentationLeftAtrium2018","container-title":"Journal of Medical Imaging and Health Informatics","container-title-short":"j med imaging hlth inform","DOI":"10.1166/jmihi.2018.2511","ISSN":"2156-7018","issue":"8","issued":{"date-parts":[["2018",10,1]]},"language":"en","note":"ZSCC:00001","number":"8","page":"1578-1584","source":"DOI.org (Crossref)","title":"Segmentation of Left Atrium Through Combination of Deep Convolutional and Recurrent Neural Networks","type":"article-journal","URL":"https://www.ingentaconnect.com/content/10.1166/jmihi.2018.2511","volume":"8"},
  {"id":"liuTemporalFeatureAlignment2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Liu","given":"Zhenguang"},{"family":"Feng","given":"Runyang"},{"family":"Chen","given":"Haoming"},{"family":"Wu","given":"Shuang"},{"family":"Gao","given":"Yixing"},{"family":"Gao","given":"Yunjun"},{"family":"Wang","given":"Xiang"}],"citation-key":"liuTemporalFeatureAlignment2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"11006-11016","source":"openaccess.thecvf.com","title":"Temporal Feature Alignment and Mutual Information Maximization for Video-Based Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Liu_Temporal_Feature_Alignment_and_Mutual_Information_Maximization_for_Video-Based_Human_CVPR_2022_paper.html"},
  {"id":"liuTSITemporalScale2021","abstract":"Despite the great progress in temporal action proposal generation, most state-of-the-art methods ignore the impact of action scales and the performance of short actions is still far from satisfaction. In this paper, we ﬁrst analyze the sample imbalance issue in action proposal generation, and correspondingly devise a novel scale-invariant loss function to alleviate the insuﬃcient learning of short actions. To further achieve proposal generation task, we adopt the pipeline of boundary evaluation and proposal completeness regression, and propose the Temporal Scale Invariant network. To better leverage the temporal context, boundary evaluation module generates action boundaries with high-precision-assured global branch and high-recall-assured local branch. Simultaneously, the proposal evaluation module is supervised with introduced scale-invariant loss, predicting accurate proposal completeness for diﬀerent scales of actions. Comprehensive experiments are conducted on ActivityNet-1.3 and THUMOS14 benchmarks, where TSI achieves state-of-the-art performance. Especially, AUC performance of short actions is boosted from 36.53% to 39.63% compared with baseline.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Liu","given":"Shuming"},{"family":"Zhao","given":"Xu"},{"family":"Su","given":"Haisheng"},{"family":"Hu","given":"Zhilan"}],"citation-key":"liuTSITemporalScale2021","container-title":"Computer Vision – ACCV 2020","DOI":"10.1007/978-3-030-69541-5_32","event-place":"Cham","ISBN":"978-3-030-69540-8 978-3-030-69541-5","issued":{"date-parts":[["2021"]]},"language":"en","note":"ZSCC:00017","page":"530-546","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"TSI: Temporal Scale Invariant Network for Action Proposal Generation","title-short":"TSI","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-030-69541-5_32","volume":"12626"},
  {"id":"liWeakMetricLearning2010","abstract":"With extracted local features of a given image, computing its global feature under perceptual framework has shown promising performance in object recognition. However, under some tough applications with large intra-class variance, using only one kind of local feature is inadequate to build a robust classiﬁcation system. To integrate the discriminability of complementary local features, in this paper, we extend the eﬃcacy of perceptual framework to adapt to heterogeneous features. Given multiple raw global features, we propose a fusion strategy through metric learning, which is called weak metric learning in this work, for fusing high dimensional features. The fusion model is solved with the maximal kernel canonical correlation formulation with the multiple global features as outputs. Experimental results show that our method achieves signiﬁcant improvements about 5% to 11% than the benchmark perceptual framework system, HMAX, on several diﬃcult categories of object recognition with much less training samples and feature elements.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Li","given":"Xiong"},{"family":"Zhao","given":"Xu"},{"family":"Fu","given":"Yun"},{"family":"Liu","given":"Yuncai"}],"citation-key":"liWeakMetricLearning2010","collection-editor":[{"family":"Hutchison","given":"David"},{"family":"Kanade","given":"Takeo"},{"family":"Kittler","given":"Josef"},{"family":"Kleinberg","given":"Jon M."},{"family":"Mattern","given":"Friedemann"},{"family":"Mitchell","given":"John C."},{"family":"Naor","given":"Moni"},{"family":"Nierstrasz","given":"Oscar"},{"family":"Pandu Rangan","given":"C."},{"family":"Steffen","given":"Bernhard"},{"family":"Sudan","given":"Madhu"},{"family":"Terzopoulos","given":"Demetri"},{"family":"Tygar","given":"Doug"},{"family":"Vardi","given":"Moshe Y."},{"family":"Weikum","given":"Gerhard"}],"container-title":"Advances in Multimedia Modeling","DOI":"10.1007/978-3-642-11301-7_29","editor":[{"family":"Boll","given":"Susanne"},{"family":"Tian","given":"Qi"},{"family":"Zhang","given":"Lei"},{"family":"Zhang","given":"Zili"},{"family":"Chen","given":"Yi-Ping Phoebe"}],"event-place":"Berlin, Heidelberg","ISBN":"978-3-642-11300-0 978-3-642-11301-7","issued":{"date-parts":[["2010"]]},"language":"en","note":"Series Editors: _:n646","page":"273-283","publisher":"Springer Berlin Heidelberg","publisher-place":"Berlin, Heidelberg","source":"DOI.org (Crossref)","title":"Weak Metric Learning for Feature Fusion towards Perception-Inspired Object Recognition","type":"chapter","URL":"http://link.springer.com/10.1007/978-3-642-11301-7_29","volume":"5916"},
  {"id":"loperSMPLSkinnedMultiperson2015","accessed":{"date-parts":[["2021",9,1]]},"author":[{"family":"Loper","given":"Matthew"},{"family":"Mahmood","given":"Naureen"},{"family":"Romero","given":"Javier"},{"family":"Pons-Moll","given":"Gerard"},{"family":"Black","given":"Michael J."}],"citation-key":"loperSMPLSkinnedMultiperson2015","container-title":"ACM Transactions on Graphics","container-title-short":"ACM Trans. Graph.","DOI":"10.1145/2816795.2818013","ISSN":"0730-0301, 1557-7368","issue":"6","issued":{"date-parts":[["2015",11,4]]},"language":"en","page":"1-16","source":"DOI.org (Crossref)","title":"SMPL: a skinned multi-person linear model","title-short":"SMPL","type":"article-journal","URL":"https://dl.acm.org/doi/10.1145/2816795.2818013","volume":"34"},
  {"id":"lorenzoNEXTNetworkNonconvex2016","abstract":"We study nonconvex distributed optimization in multiagent networks with time-varying (nonsymmetric) connectivity. We introduce the first algorithmic framework for the distributed minimization of the sum of a smooth (possibly nonconvex and nonseparable) function-the agents' sum-utility-plus a convex (possibly nonsmooth and nonseparable) regularizer. The latter is usually employed to enforce some structure in the solution, typically sparsity. The proposed method hinges on successive convex approximation techniques while leveraging dynamic consensus as a mechanism to distribute the computation among the agents: each agent first solves (possibly inexactly) a local convex approximation of the nonconvex original problem, and then performs local averaging operations. Asymptotic convergence to (stationary) solutions of the nonconvex problem is established. Our algorithmic framework is then customized to a variety of convex and nonconvex problems in several fields, including signal processing, communications, networking, and machine learning. Numerical results show that the new method compares favorably to existing distributed algorithms on both convex and nonconvex problems.","author":[{"family":"Lorenzo","given":"Paolo Di"},{"family":"Scutari","given":"Gesualdo"}],"citation-key":"lorenzoNEXTNetworkNonconvex2016","container-title":"IEEE Transactions on Signal and Information Processing over Networks","DOI":"10.1109/TSIPN.2016.2524588","ISSN":"2373-776X","issue":"2","issued":{"date-parts":[["2016",6]]},"page":"120-136","source":"IEEE Xplore","title":"NEXT: In-Network Nonconvex Optimization","title-short":"NEXT","type":"article-journal","volume":"2"},
  {"id":"luoIntelligentCarpetInferring2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Luo","given":"Yiyue"},{"family":"Li","given":"Yunzhu"},{"family":"Foshey","given":"Michael"},{"family":"Shou","given":"Wan"},{"family":"Sharma","given":"Pratyusha"},{"family":"Palacios","given":"Tomas"},{"family":"Torralba","given":"Antonio"},{"family":"Matusik","given":"Wojciech"}],"citation-key":"luoIntelligentCarpetInferring2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"11255-11265","source":"openaccess.thecvf.com","title":"Intelligent Carpet: Inferring 3D Human Pose From Tactile Signals","title-short":"Intelligent Carpet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Intelligent_Carpet_Inferring_3D_Human_Pose_From_Tactile_Signals_CVPR_2021_paper.html"},
  {"id":"luoRethinkingHeatmapRegression2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Luo","given":"Zhengxiong"},{"family":"Wang","given":"Zhicheng"},{"family":"Huang","given":"Yan"},{"family":"Wang","given":"Liang"},{"family":"Tan","given":"Tieniu"},{"family":"Zhou","given":"Erjin"}],"citation-key":"luoRethinkingHeatmapRegression2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"13264-13273","source":"openaccess.thecvf.com","title":"Rethinking the Heatmap Regression for Bottom-Up Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Luo_Rethinking_the_Heatmap_Regression_for_Bottom-Up_Human_Pose_Estimation_CVPR_2021_paper.html"},
  {"id":"luvizon2D3DPose2018","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Luvizon","given":"Diogo C."},{"family":"Picard","given":"David"},{"family":"Tabia","given":"Hedi"}],"citation-key":"luvizon2D3DPose2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"5137-5146","source":"openaccess.thecvf.com","title":"2D/3D Pose Estimation and Action Recognition Using Multitask Deep Learning","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Luvizon_2D3D_Pose_Estimation_CVPR_2018_paper.html"},
  {"id":"ma3DHumanMesh2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Ma","given":"Xiaoxuan"},{"family":"Su","given":"Jiajun"},{"family":"Wang","given":"Chunyu"},{"family":"Zhu","given":"Wentao"},{"family":"Wang","given":"Yizhou"}],"citation-key":"ma3DHumanMesh2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"534-543","source":"openaccess.thecvf.com","title":"3D Human Mesh Estimation From Virtual Markers","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Ma_3D_Human_Mesh_Estimation_From_Virtual_Markers_CVPR_2023_paper.html"},
  {"id":"maatenVisualizingDataUsing2008","abstract":"We present a new technique called “t-SNE” that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualiza-tions produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.","author":[{"family":"Maaten","given":"Laurens","dropping-particle":"van der"},{"family":"Hinton","given":"Geoffrey"}],"citation-key":"maatenVisualizingDataUsing2008","issued":{"date-parts":[["2008"]]},"source":"CiteSeer","title":"Visualizing data using t-SNE","type":"document"},
  {"id":"maContextModeling3D2021","accessed":{"date-parts":[["2021",8,27]]},"author":[{"family":"Ma","given":"Xiaoxuan"},{"family":"Su","given":"Jiajun"},{"family":"Wang","given":"Chunyu"},{"family":"Ci","given":"Hai"},{"family":"Wang","given":"Yizhou"}],"citation-key":"maContextModeling3D2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"6238-6247","source":"openaccess.thecvf.com","title":"Context Modeling in 3D Human Pose Estimation: A Unified Perspective","title-short":"Context Modeling in 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Ma_Context_Modeling_in_3D_Human_Pose_Estimation_A_Unified_Perspective_CVPR_2021_paper.html"},
  {"id":"maDeepFeedbackInverse2021","abstract":"We present an efficient, effective, and generic approach towards solving inverse problems. The key idea is to leverage the feedback signal provided by the forward process and learn an iterative update model. Specifically, at each iteration, the neural network takes the feedback as input and outputs an update on the current estimation. Our approach does not have any restrictions on the forward process; it does not require any prior knowledge either. Through the feedback information, our model not only can produce accurate estimations that are coherent to the input observation but also is capable of recovering from early incorrect predictions. We verify the performance of our approach over a wide range of inverse problems, including 6-DOF pose estimation, illumination estimation, as well as inverse kinematics. Comparing to traditional optimization-based methods, we can achieve comparable or better performance while being two to three orders of magnitude faster. Compared to deep learning-based approaches, our model consistently improves the performance on all metrics. Please refer to the project page for videos, animations, supplementary materials, etc.","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Ma","given":"Wei-Chiu"},{"family":"Wang","given":"Shenlong"},{"family":"Gu","given":"Jiayuan"},{"family":"Manivasagam","given":"Sivabalan"},{"family":"Torralba","given":"Antonio"},{"family":"Urtasun","given":"Raquel"}],"citation-key":"maDeepFeedbackInverse2021","DOI":"10.48550/arXiv.2101.07719","issued":{"date-parts":[["2021",1,19]]},"number":"arXiv:2101.07719","publisher":"arXiv","source":"arXiv.org","title":"Deep Feedback Inverse Problem Solver","type":"article","URL":"http://arxiv.org/abs/2101.07719"},
  {"id":"maPPTTokenPrunedPose2022","abstract":"Recently, the vision transformer and its variants have played an increasingly important role in both monocular and multi-view human pose estimation. Considering image patches as tokens, transformers can model the global dependencies within the entire image or across images from other views. However, global attention is computationally expensive. As a consequence, it is difficult to scale up these transformer-based methods to high-resolution features and many views.","author":[{"family":"Ma","given":"Haoyu"},{"family":"Wang","given":"Zhe"},{"family":"Chen","given":"Yifei"},{"family":"Kong","given":"Deying"},{"family":"Chen","given":"Liangjian"},{"family":"Liu","given":"Xingwei"},{"family":"Yan","given":"Xiangyi"},{"family":"Tang","given":"Hao"},{"family":"Xie","given":"Xiaohui"}],"citation-key":"maPPTTokenPrunedPose2022","collection-title":"Lecture Notes in Computer Science","container-title":"Computer Vision – ECCV 2022","DOI":"10.1007/978-3-031-20065-6_25","editor":[{"family":"Avidan","given":"Shai"},{"family":"Brostow","given":"Gabriel"},{"family":"Cissé","given":"Moustapha"},{"family":"Farinella","given":"Giovanni Maria"},{"family":"Hassner","given":"Tal"}],"event-place":"Cham","ISBN":"978-3-031-20065-6","issued":{"date-parts":[["2022"]]},"language":"en","page":"424-442","publisher":"Springer Nature Switzerland","publisher-place":"Cham","source":"Springer Link","title":"PPT: Token-Pruned Pose Transformer for Monocular and Multi-view Human Pose Estimation","title-short":"PPT","type":"paper-conference"},
  {"id":"martinezSimpleEffectiveBaseline2017","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Martinez","given":"Julieta"},{"family":"Hossain","given":"Rayat"},{"family":"Romero","given":"Javier"},{"family":"Little","given":"James J."}],"citation-key":"martinezSimpleEffectiveBaseline2017","event-title":"Proceedings of the IEEE International Conference on Computer Vision","issued":{"date-parts":[["2017"]]},"page":"2640-2649","source":"openaccess.thecvf.com","title":"A Simple yet Effective Baseline for 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_iccv_2017/html/Martinez_A_Simple_yet_ICCV_2017_paper.html"},
  {"id":"mehtaMonocular3DHuman2017","abstract":"We propose a CNN-based approach for 3D human body pose estimation from single RGB images that addresses the issue of limited generalizability of models trained solely on the starkly limited publicly available 3D pose data. Using only the existing 3D pose data and 2D pose data, we show state-of-the-art performance on established benchmarks through transfer of learned features, while also generalizing to in-the-wild scenes. We further introduce a new training set for human body pose estimation from monocular images of real humans that has the ground truth captured with a multi-camera marker-less motion capture system. It complements existing corpora with greater diversity in pose, human appearance, clothing, occlusion, and viewpoints, and enables an increased scope of augmentation. We also contribute a new benchmark that covers outdoor and indoor scenes, and demonstrate that our 3D pose dataset shows better in-the-wild performance than existing annotated data, which is further improved in conjunction with transfer learning from 2D pose data. All in all, we argue that the use of transfer learning of representations in tandem with algorithmic and data contributions is crucial for general 3D body pose estimation.","accessed":{"date-parts":[["2022",11,16]]},"author":[{"family":"Mehta","given":"Dushyant"},{"family":"Rhodin","given":"Helge"},{"family":"Casas","given":"Dan"},{"family":"Fua","given":"Pascal"},{"family":"Sotnychenko","given":"Oleksandr"},{"family":"Xu","given":"Weipeng"},{"family":"Theobalt","given":"Christian"}],"citation-key":"mehtaMonocular3DHuman2017","issued":{"date-parts":[["2017",10,4]]},"language":"en","number":"arXiv:1611.09813","publisher":"arXiv","source":"arXiv.org","title":"Monocular 3D Human Pose Estimation In The Wild Using Improved CNN Supervision","type":"article","URL":"http://arxiv.org/abs/1611.09813"},
  {"id":"memonHandwrittenOpticalCharacter2020","abstract":"Given the ubiquity of handwritten documents in human transactions, Optical Character Recognition (OCR) of documents have invaluable practical worth. Optical character recognition is a science that enables to translate various types of documents or images into analyzable, editable and searchable data. During last decade, researchers have used artificial intelligence/machine learning tools to automatically analyze handwritten and printed documents in order to convert them into electronic format. The objective of this review paper is to summarize research that has been conducted on character recognition of handwritten documents and to provide research directions. In this Systematic Literature Review (SLR) we collected, synthesized and analyzed research articles on the topic of handwritten OCR (and closely related topics) which were published between year 2000 to 2019. We followed widely used electronic databases by following pre-defined review protocol. Articles were searched using keywords, forward reference searching and backward reference searching in order to search all the articles related to the topic. After carefully following study selection process 176 articles were selected for this SLR. This review article serves the purpose of presenting state of the art results and techniques on OCR and also provide research directions by highlighting research gaps.","author":[{"family":"Memon","given":"Jamshed"},{"family":"Sami","given":"Maira"},{"family":"Khan","given":"Rizwan Ahmed"},{"family":"Uddin","given":"Mueen"}],"citation-key":"memonHandwrittenOpticalCharacter2020","container-title":"IEEE Access","DOI":"10.1109/ACCESS.2020.3012542","ISSN":"2169-3536","issued":{"date-parts":[["2020"]]},"page":"142642-142668","source":"IEEE Xplore","title":"Handwritten Optical Character Recognition (OCR): A Comprehensive Systematic Literature Review (SLR)","title-short":"Handwritten Optical Character Recognition (OCR)","type":"article-journal","volume":"8"},
  {"id":"mingbomaPrototypeBasedFeature2013","abstract":"Recognizing human face from image set has recently seen its prosperity because of its effectiveness in dealing with variations in illumination, expressions, or poses. In this paper, inspired by the prototype notion originating from cognition ﬁeld, we obtain discriminative feature representation for face recognition by implementing prototype formation on image set. The contribution of this paper is twofold: ﬁrst, we propose to use prototype image sets as a common reference to sufﬁciently represent any image set with the same type; in addition, we propose a novel framework to extract image set’s features through hyperplane supervised by max-margin criterion between any image set and prototype image set. The ﬁnal features are summarized through pooling technique along the prototype image sets. We experimentally prove the effectiveness of the method through extensive experiments on several databases, and show that it is superior to the stateof-the-art methods in terms of both time complexity and recognition accuracy.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"literal":"Mingbo Ma"},{"literal":"Ming Shao"},{"literal":"Xu Zhao"},{"literal":"Yun Fu"}],"citation-key":"mingbomaPrototypeBasedFeature2013","container-title":"2013 10th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG)","DOI":"10.1109/FG.2013.6553702","event-place":"Shanghai, China","event-title":"2013 10th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2013)","ISBN":"978-1-4673-5546-9 978-1-4673-5545-2 978-1-4673-5544-5","issued":{"date-parts":[["2013",4]]},"language":"en","note":"ZSCC:00015","page":"1-6","publisher":"IEEE","publisher-place":"Shanghai, China","source":"DOI.org (Crossref)","title":"Prototype based feature learning for face image set classification","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/6553702/"},
  {"id":"mirjaliliGrasshopperOptimizationAlgorithm2018","abstract":"This work proposes a new multi-objective algorithm inspired from the navigation of grass hopper swarms in nature. A mathematical model is first employed to model the interaction of individuals in the swam including attraction force, repulsion force, and comfort zone. A mechanism is then proposed to use the model in approximating the global optimum in a single-objective search space. Afterwards, an archive and target selection technique are integrated to the algorithm to estimate the Pareto optimal front for multi-objective problems. To benchmark the performance of the algorithm proposed, a set of diverse standard multi-objective test problems is utilized. The results are compared with the most well-regarded and recent algorithms in the literature of evolutionary multi-objective optimization using three performance indicators quantitatively and graphs qualitatively. The results show that the proposed algorithm is able to provide very competitive results in terms of accuracy of obtained Pareto optimal solutions and their distribution.","accessed":{"date-parts":[["2021",10,13]]},"author":[{"family":"Mirjalili","given":"Seyedeh Zahra"},{"family":"Mirjalili","given":"Seyedali"},{"family":"Saremi","given":"Shahrzad"},{"family":"Faris","given":"Hossam"},{"family":"Aljarah","given":"Ibrahim"}],"citation-key":"mirjaliliGrasshopperOptimizationAlgorithm2018","container-title":"Applied Intelligence","container-title-short":"Appl Intell","DOI":"10.1007/s10489-017-1019-8","ISSN":"1573-7497","issue":"4","issued":{"date-parts":[["2018",4,1]]},"language":"en","page":"805-820","source":"Springer Link","title":"Grasshopper optimization algorithm for multi-objective optimization problems","type":"article-journal","URL":"https://doi.org/10.1007/s10489-017-1019-8","volume":"48"},
  {"id":"mitraMultiviewConsistentSemiSupervisedLearning2020","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Mitra","given":"Rahul"},{"family":"Gundavarapu","given":"Nitesh B."},{"family":"Sharma","given":"Abhishek"},{"family":"Jain","given":"Arjun"}],"citation-key":"mitraMultiviewConsistentSemiSupervisedLearning2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"6907-6916","source":"openaccess.thecvf.com","title":"Multiview-Consistent Semi-Supervised Learning for 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Mitra_Multiview-Consistent_Semi-Supervised_Learning_for_3D_Human_Pose_Estimation_CVPR_2020_paper.html"},
  {"id":"mokDetectingLowQualityWorkers2017","abstract":"Human activity recognition is a challenging highlevel vision task, for which multiple factors, such as subject, object, and their diverse interactions, have to be considered and modeled. Current learning-based methods are limited in the capability to integrate human-level concepts into an easily extensible computational framework. Inspired by the existing human memory model, we present a context-associative approach to recognize activity with human-object interaction. The proposed system can recognize incoming visual content based on the previous experienced activities. The high-level activity is parsed into consecutive subactivities, and we build a context cluster to model the temporal relations. The semantic attributes of the subactivity are organized by a concept hierarchy. Based on the hierarchy, a series of similarity functions are deﬁned to turn the recognition computing into retrievals over the contextual memory, similar to the auto-associative characteristics of human memory. Partially matching in retrieval and stored memory make the activity prediction possible. The dynamical evolution of the brain memory is mimicked to allow decay and reinforcement of the input information, providing a natural way to maintain data and save computational time. We evaluate our approach on three data sets: CAD-120, MHOI, and OPPORTUNITY. The proposed method demonstrates promising results compared with other stateof-the-art techniques.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Mok","given":"Ricky K. P."},{"family":"Chang","given":"Rocky K. C."},{"family":"Li","given":"Weichao"}],"citation-key":"mokDetectingLowQualityWorkers2017","container-title":"IEEE Transactions on Multimedia","container-title-short":"IEEE Trans. Multimedia","DOI":"10.1109/TMM.2016.2619901","ISSN":"1520-9210, 1941-0077","issue":"3","issued":{"date-parts":[["2017",3]]},"language":"en","note":"ZSCC:00034","number":"3","page":"530-543","source":"DOI.org (Crossref)","title":"Detecting Low-Quality Workers in QoE Crowdtesting: A Worker Behavior-Based Approach","title-short":"Detecting Low-Quality Workers in QoE Crowdtesting","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/7604053/","volume":"19"},
  {"id":"moonCameraDistanceAwareTopApproach2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Moon","given":"Gyeongsik"},{"family":"Chang","given":"Ju Yong"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"moonCameraDistanceAwareTopApproach2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"10133-10142","source":"openaccess.thecvf.com","title":"Camera Distance-Aware Top-Down Approach for 3D Multi-Person Pose Estimation From a Single RGB Image","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Moon_Camera_Distance-Aware_Top-Down_Approach_for_3D_Multi-Person_Pose_Estimation_From_ICCV_2019_paper.html"},
  {"id":"moonV2VPoseNetVoxelVoxelPrediction2018","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Moon","given":"Gyeongsik"},{"family":"Chang","given":"Ju Yong"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"moonV2VPoseNetVoxelVoxelPrediction2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"5079-5088","source":"openaccess.thecvf.com","title":"V2V-PoseNet: Voxel-to-Voxel Prediction Network for Accurate 3D Hand and Human Pose Estimation From a Single Depth Map","title-short":"V2V-PoseNet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Moon_V2V-PoseNet_Voxel-to-Voxel_Prediction_CVPR_2018_paper.html"},
  {"id":"moreno-noguer3DHumanPose2017","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Moreno-Noguer","given":"Francesc"}],"citation-key":"moreno-noguer3DHumanPose2017","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2017"]]},"page":"2823-2832","source":"openaccess.thecvf.com","title":"3D Human Pose Estimation From a Single Image via Distance Matrix Regression","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2017/html/Moreno-Noguer_3D_Human_Pose_CVPR_2017_paper.html"},
  {"id":"moriHistoricalReviewOCR1992","abstract":"Research and development of OCR systems are considered from a historical point of view. The historical development of commercial systems is included. Both template matching and structure analysis approaches to R&D are considered. It is noted that the two approaches are coming closer and tending to merge. Commercial products are divided into three generations, for each of which some representative OCR systems are chosen and described in some detail. Some comments are made on recent techniques applied to OCR, such as expert systems and neural networks, and some open problems are indicated. The authors' views and hopes regarding future trends are presented.<>","author":[{"family":"Mori","given":"S."},{"family":"Suen","given":"C.Y."},{"family":"Yamamoto","given":"K."}],"citation-key":"moriHistoricalReviewOCR1992","container-title":"Proceedings of the IEEE","DOI":"10.1109/5.156468","ISSN":"1558-2256","issue":"7","issued":{"date-parts":[["1992",7]]},"page":"1029-1058","source":"IEEE Xplore","title":"Historical review of OCR research and development","type":"article-journal","volume":"80"},
  {"id":"mullerSelfContactHumanPose2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Muller","given":"Lea"},{"family":"Osman","given":"Ahmed A. A."},{"family":"Tang","given":"Siyu"},{"family":"Huang","given":"Chun-Hao P."},{"family":"Black","given":"Michael J."}],"citation-key":"mullerSelfContactHumanPose2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"9990-9999","source":"openaccess.thecvf.com","title":"On Self-Contact and Human Pose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Muller_On_Self-Contact_and_Human_Pose_CVPR_2021_paper.html"},
  {"id":"MultipleShotPersonReidentification2015","abstract":"Person re-identification is an important and challenging task in computer vision with numerous real world applications. Despite significant progress has been made in the past few years, person re-identification remains an unsolved problem. This paper presents a novel appearance-based approach to person re-identification. The approach exploits region covariance matrix and color histograms to capture the statistical properties and chromatic information of each object. Robustness against low resolution, viewpoint changes and pose variations is achieved by a novel signature, that is, the combination of Log Covariance Matrix feature and HSV histogram (LCMH). In order to further improve re-identification performance, third-party image sets are utilized as a common reference to sufficiently represent any image set with the same type. Distinctive and reliable features for a given image set are extracted through decision boundary between the specific set and a third-party image set supervised by max-margin criteria. This method enables the usage of an existing dataset to represent new image data without time-consuming data collection and annotation. Comparisons with state-of-the-art methods carried out on benchmark datasets demonstrate promising performance of our method.","accessed":{"date-parts":[["2023",4,1]]},"citation-key":"MultipleShotPersonReidentification2015","container-title":"KSII Transactions on Internet and Information Systems","container-title-short":"KSII TIIS","DOI":"10.3837/tiis.2015.02.017","ISSN":"19767277","issue":"2","issued":{"date-parts":[["2015",2,28]]},"language":"en","number":"2","page":"775-792","source":"DOI.org (Crossref)","title":"Multiple-Shot Person Re-identification by Features Learned from Third-party Image Sets","type":"article-journal","URL":"http://www.itiis.org/digital-library/manuscript/964","volume":"9"},
  {"id":"navarro-morenoDetectionContinuoustimeQuaternion2012","abstract":"Different kinds of quaternion signal detection problems in continuous-time by using a widely linear processing are dealt with. The suggested solutions are based on an extension of the Karhunen-Loève expansion to the quaternion domain which provides uncorrelated scalar real-valued random coefficients. This expansion presents the notable advantage of transforming the original four-dimensional eigen problem to a one-dimensional problem. Firstly, we address the problem of detecting a quaternion deterministic signal in quaternion Gaussian noise and a version of Pitcher’s Theorem is given. Also the particular case of a general quaternion Wiener noise is studied and an extension of the Cameron-Martin formula is presented. Finally, the problem of detecting a quaternion random signal in quaternion white Gaussian noise is tackled. In such a case, it is shown that the detector depends on the quaternion widely linear estimator of the signal.","accessed":{"date-parts":[["2020",8,17]]},"author":[{"family":"Navarro-Moreno","given":"Jesús"},{"family":"Ruiz-Molina","given":"Juan Carlos"},{"family":"Oya","given":"Antonia"},{"family":"Quesada-Rubio","given":"José M."}],"citation-key":"navarro-morenoDetectionContinuoustimeQuaternion2012","container-title":"EURASIP Journal on Advances in Signal Processing","container-title-short":"EURASIP J. Adv. Signal Process.","DOI":"10.1186/1687-6180-2012-234","ISSN":"1687-6180","issue":"1","issued":{"date-parts":[["2012",11,12]]},"language":"en","page":"234","source":"Springer Link","title":"Detection of continuous-time quaternion signals in additive noise","type":"article-journal","URL":"https://doi.org/10.1186/1687-6180-2012-234","volume":"2012"},
  {"id":"necoaraLinearConvergenceFirst2019","abstract":"The standard assumption for proving linear convergence of first order methods for smooth convex optimization is the strong convexity of the objective function, an assumption which does not hold for many practical applications. In this paper, we derive linear convergence rates of several first order methods for solving smooth non-strongly convex constrained optimization problems, i.e. involving an objective function with a Lipschitz continuous gradient that satisfies some relaxed strong convexity condition. In particular, in the case of smooth constrained convex optimization, we provide several relaxations of the strong convexity conditions and prove that they are sufficient for getting linear convergence for several first order methods such as projected gradient, fast gradient and feasible descent methods. We also provide examples of functional classes that satisfy our proposed relaxations of strong convexity conditions. Finally, we show that the proposed relaxed strong convexity conditions cover important applications ranging from solving linear systems, Linear Programming, and dual formulations of linearly constrained convex problems.","accessed":{"date-parts":[["2021",6,12]]},"author":[{"family":"Necoara","given":"I."},{"family":"Nesterov","given":"Yu."},{"family":"Glineur","given":"F."}],"citation-key":"necoaraLinearConvergenceFirst2019","container-title":"Mathematical Programming","container-title-short":"Math. Program.","DOI":"10.1007/s10107-018-1232-1","ISSN":"1436-4646","issue":"1","issued":{"date-parts":[["2019",5,1]]},"language":"en","page":"69-107","source":"Springer Link","title":"Linear convergence of first order methods for non-strongly convex optimization","type":"article-journal","URL":"https://doi.org/10.1007/s10107-018-1232-1","volume":"175"},
  {"id":"ngYou2MeInferringBody2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Ng","given":"Evonne"},{"family":"Xiang","given":"Donglai"},{"family":"Joo","given":"Hanbyul"},{"family":"Grauman","given":"Kristen"}],"citation-key":"ngYou2MeInferringBody2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"9890-9900","source":"openaccess.thecvf.com","title":"You2Me: Inferring Body Pose in Egocentric Video via First and Second Person Interactions","title-short":"You2Me","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Ng_You2Me_Inferring_Body_Pose_in_Egocentric_Video_via_First_and_CVPR_2020_paper.html"},
  {"id":"ni3DTRIPGeneralFramework2023","abstract":"In an autonomous driving system, recovering the moving trajectories of the road users and predicting their future trend can greatly assist further driving decision-making. However, there are still many difﬁculties to solve this problem, due to the insufﬁcient accuracy of the sensors and occlusion between objects. In this work, we propose a general framework for 3D trajectory recovery integrated with prediction (called 3DTRIP) based on deep learning methods through sensor fusion. The proposed method contains two modules: 1) trajectory recovery module to recover trajectories of moving objects from RGB and point cloud data and 2) trajectory prediction module to predict the future trajectories of objects. In the recovery module, we combine 2D multi-object tracker with 3D object detector to locate the accurate positions in trajectory, and propose a post-process method to further reﬁne the results. In the prediction module, we apply Multilayer Perceptron (MLP) to extract global features of historical trajectories, and combine them with local features extracted by sequential model. Finally, we integrate these two modules to improve the accuracy of trajectory recovery. This framework helps autonomous cars to capture the historical movement of road users and predict their movement trends. The novelty lies in the efﬁcient integration mechanism of trajectory recovery and prediction, making the trajectory recovery more accurate. We validate the method on KITTI tracking dataset and NuScenes dataset. The extensive experiments show improvements on trajectory recovery with the aid of prediction.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Ni","given":"Yiyang"},{"family":"Zhao","given":"Xu"}],"citation-key":"ni3DTRIPGeneralFramework2023","container-title":"IEEE Robotics and Automation Letters","container-title-short":"IEEE Robot. Autom. Lett.","DOI":"10.1109/LRA.2022.3228155","ISSN":"2377-3766, 2377-3774","issue":"2","issued":{"date-parts":[["2023",2]]},"language":"en","note":"ZSCC:00000","number":"2","page":"512-519","source":"DOI.org (Crossref)","title":"3DTRIP: A General Framework for 3D Trajectory Recovery Integrated With Prediction","title-short":"3DTRIP","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9978687/","volume":"8"},
  {"id":"nieDynamicKernelDistillation2019","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Nie","given":"Xuecheng"},{"family":"Li","given":"Yuncheng"},{"family":"Luo","given":"Linjie"},{"family":"Zhang","given":"Ning"},{"family":"Feng","given":"Jiashi"}],"citation-key":"nieDynamicKernelDistillation2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"6942-6950","source":"openaccess.thecvf.com","title":"Dynamic Kernel Distillation for Efficient Pose Estimation in Videos","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Nie_Dynamic_Kernel_Distillation_for_Efficient_Pose_Estimation_in_Videos_ICCV_2019_paper.html"},
  {"id":"ningISDNetImportanceGuided2019","abstract":"Recent deep neural networks have achieved great success in medical image segmentation. However, massive labeled training data should be provided during network training, which is time consuming with intensive labor work and even requires expertise knowledge. To address such challenge, inspired by typical GANs, we propose a novel end-to-end semi-supervised adversarial learning framework for medical image segmentation, called “Importance guided Semi-supervised Deep Networks” (ISDNet). While most existing works based on GANs use a classiﬁer discriminator to achieve adversarial learning, we combine a fully convolutional discriminator and a classiﬁer discriminator to fulﬁll better adversarial learning and self-taught learning. Speciﬁcally, we propose an importance weight network combined with our FCN-based conﬁdence network, which can assist segmentation network to learn better local and global information. Extensive experiments are conducted on the LASC 2013 and the LiTS 2017 datasets to demonstrate the eﬀectiveness of our approach.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Ning","given":"Qingtian"},{"family":"Zhao","given":"Xu"},{"family":"Qian","given":"Dahong"}],"citation-key":"ningISDNetImportanceGuided2019","container-title":"ICIG","DOI":"10.1007/978-3-030-34110-7_38","event-place":"Cham","event-title":"Image and Graphics","ISBN":"978-3-030-34109-1 978-3-030-34110-7","issued":{"date-parts":[["2019"]]},"language":"en","note":"ZSCC:00000","page":"459-470","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"ISDNet: Importance Guided Semi-supervised Adversarial Learning for Medical Image Segmentation","title-short":"ISDNet","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-030-34110-7_38"},
  {"id":"NumericallyStableDual","accessed":{"date-parts":[["2022",4,13]]},"citation-key":"NumericallyStableDual","title":"A numerically stable dual method for solving strictly convex quadratic programs | SpringerLink","type":"webpage","URL":"https://link.springer.com/article/10.1007/BF02591962"},
  {"id":"NumpyClamp_BaiDuSouSuo","accessed":{"date-parts":[["2023",8,1]]},"citation-key":"NumpyClamp_BaiDuSouSuo","title":"numpy clamp_百度搜索","type":"webpage","URL":"https://www.baidu.com/s?wd=numpy%20%20clamp&rsv_spt=1&rsv_iqid=0x99601019000be318&issp=1&f=8&rsv_bp=1&rsv_idx=2&ie=utf-8&rqlang=cn&tn=baiduhome_pg&rsv_enter=1&rsv_dl=tb&oq=tensor%2520clamp&rsv_btype=t&inputT=3645&rsv_t=6cf8EZ2Dr6VJgqgZIR2TklrNlHZN0XNN4h2LtIyXvJx6IZWOJS0ClbumfTODSnUQJOPG&rsv_pq=ada3fb720001e580&rsv_sug3=77&rsv_sug1=36&rsv_sug7=100&rsv_sug2=0&rsv_sug4=3645"},
  {"id":"NumpyClamp_BaiDuSouSuoa","accessed":{"date-parts":[["2023",8,1]]},"citation-key":"NumpyClamp_BaiDuSouSuoa","title":"numpy clamp_百度搜索","type":"webpage","URL":"https://www.baidu.com/s?wd=numpy%20%20clamp&rsv_spt=1&rsv_iqid=0x99601019000be318&issp=1&f=8&rsv_bp=1&rsv_idx=2&ie=utf-8&rqlang=cn&tn=baiduhome_pg&rsv_enter=1&rsv_dl=tb&oq=tensor%2520clamp&rsv_btype=t&inputT=3645&rsv_t=6cf8EZ2Dr6VJgqgZIR2TklrNlHZN0XNN4h2LtIyXvJx6IZWOJS0ClbumfTODSnUQJOPG&rsv_pq=ada3fb720001e580&rsv_sug3=77&rsv_sug1=36&rsv_sug7=100&rsv_sug2=0&rsv_sug4=3645"},
  {"id":"ochsIPianoInertialProximal2014","abstract":"In this paper we study an algorithm for solving a minimization problem composed of a differentiable (possibly non-convex) and a convex (possibly non-differentiable) function. The algorithm iPiano combines forward-backward splitting with an inertial force. It can be seen as a non-smooth split version of the Heavy-ball method from Polyak. A rigorous analysis of the algorithm for the proposed class of problems yields global convergence of the function values and the arguments. This makes the algorithm robust for usage on non-convex problems. The convergence result is obtained based on the \\KL inequality. This is a very weak restriction, which was used to prove convergence for several other gradient methods. First, an abstract convergence theorem for a generic algorithm is proved, and, then iPiano is shown to satisfy the requirements of this theorem. Furthermore, a convergence rate is established for the general problem class. We demonstrate iPiano on computer vision problems: image denoising with learned priors and diffusion based image compression.","accessed":{"date-parts":[["2021",6,30]]},"author":[{"family":"Ochs","given":"Peter"},{"family":"Chen","given":"Yunjin"},{"family":"Brox","given":"Thomas"},{"family":"Pock","given":"Thomas"}],"citation-key":"ochsIPianoInertialProximal2014","container-title":"arXiv:1404.4805 [cs, math]","issued":{"date-parts":[["2014",4,18]]},"source":"arXiv.org","title":"iPiano: Inertial Proximal Algorithm for Non-Convex Optimization","title-short":"iPiano","type":"article-journal","URL":"http://arxiv.org/abs/1404.4805"},
  {"id":"ohkawaAssemblyHandsEgocentricActivity2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Ohkawa","given":"Takehiko"},{"family":"He","given":"Kun"},{"family":"Sener","given":"Fadime"},{"family":"Hodan","given":"Tomas"},{"family":"Tran","given":"Luan"},{"family":"Keskin","given":"Cem"}],"citation-key":"ohkawaAssemblyHandsEgocentricActivity2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12999-13008","source":"openaccess.thecvf.com","title":"AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation","title-short":"AssemblyHands","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"OptimizationBasedFramework","citation-key":"OptimizationBasedFramework","container-title":"IEEE SIGNAL PROCESSING LETTERS","title":"An Optimization Based Framework for Human Pose Estimation","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/5492191"},
  {"id":"pantelerisBackRGB3D2017","accessed":{"date-parts":[["2021",1,6]]},"author":[{"family":"Panteleris","given":"Paschalis"},{"family":"Argyros","given":"Antonis"}],"citation-key":"pantelerisBackRGB3D2017","event-title":"Proceedings of the IEEE International Conference on Computer Vision Workshops","issued":{"date-parts":[["2017"]]},"page":"575-584","source":"openaccess.thecvf.com","title":"Back to RGB: 3D Tracking of Hands and Hand-Object Interactions Based on Short-Baseline Stereo","title-short":"Back to RGB","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2017_workshops/w11/html/Panteleris_Back_to_RGB_ICCV_2017_paper.html"},
  {"id":"parthibanOpticalCharacterRecognition2020","abstract":"Manually written Text Recognition is an innovation that is genuinely necessary right now of today.Before appropriate execution of this innovation we have depended on composing writings with own may leads to some mistakes. It is hard to maintain safely and gathering that information with effectiveness.Difficult work is required so as to keep up appropriate association of the data.Recurrent neural networkis utilized to discover the arrangement of character.Today we have OCRs effectively accessible for the English language.We can discover OCRs for formal texted English also yet OCRs for written by hand content are uncommon.Furthermore,those which are accessible don't have a goodaccuracy.We expect to make such an OCR which gives us an impressiverecognition exactness for manually written Text using recurrent neural network.The proposed model is implemented using Conda, used with TensorflowFramework. The purpose of Recurrent neural network is to improve accuracy.","author":[{"family":"Parthiban","given":"R."},{"family":"Ezhilarasi","given":"R."},{"family":"Saravanan","given":"D."}],"citation-key":"parthibanOpticalCharacterRecognition2020","container-title":"2020 International Conference on System, Computation, Automation and Networking (ICSCAN)","DOI":"10.1109/ICSCAN49426.2020.9262379","event-title":"2020 International Conference on System, Computation, Automation and Networking (ICSCAN)","issued":{"date-parts":[["2020",7]]},"page":"1-5","source":"IEEE Xplore","title":"Optical Character Recognition for English Handwritten Text Using Recurrent Neural Network","type":"paper-conference"},
  {"id":"pavlakosCoarseFineVolumetricPrediction2017","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Pavlakos","given":"Georgios"},{"family":"Zhou","given":"Xiaowei"},{"family":"Derpanis","given":"Konstantinos G."},{"family":"Daniilidis","given":"Kostas"}],"citation-key":"pavlakosCoarseFineVolumetricPrediction2017","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2017"]]},"page":"7025-7034","source":"openaccess.thecvf.com","title":"Coarse-To-Fine Volumetric Prediction for Single-Image 3D Human Pose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2017/html/Pavlakos_Coarse-To-Fine_Volumetric_Prediction_CVPR_2017_paper.html"},
  {"id":"pavlakosHarvestingMultipleViews2017","accessed":{"date-parts":[["2022",4,17]]},"author":[{"family":"Pavlakos","given":"Georgios"},{"family":"Zhou","given":"Xiaowei"},{"family":"Derpanis","given":"Konstantinos G."},{"family":"Daniilidis","given":"Kostas"}],"citation-key":"pavlakosHarvestingMultipleViews2017","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2017"]]},"page":"6988-6997","source":"openaccess.thecvf.com","title":"Harvesting Multiple Views for Marker-Less 3D Human Pose Annotations","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2017/html/Pavlakos_Harvesting_Multiple_Views_CVPR_2017_paper.html"},
  {"id":"pavlakosHumanMeshRecovery2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Pavlakos","given":"Georgios"},{"family":"Malik","given":"Jitendra"},{"family":"Kanazawa","given":"Angjoo"}],"citation-key":"pavlakosHumanMeshRecovery2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"1485-1495","source":"openaccess.thecvf.com","title":"Human Mesh Recovery From Multiple Shots","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Pavlakos_Human_Mesh_Recovery_From_Multiple_Shots_CVPR_2022_paper.html"},
  {"id":"pavlakosLearningEstimate3D2018","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Pavlakos","given":"Georgios"},{"family":"Zhu","given":"Luyang"},{"family":"Zhou","given":"Xiaowei"},{"family":"Daniilidis","given":"Kostas"}],"citation-key":"pavlakosLearningEstimate3D2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"459-468","source":"openaccess.thecvf.com","title":"Learning to Estimate 3D Human Pose and Shape From a Single Color Image","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pavlakos_Learning_to_Estimate_CVPR_2018_paper.html"},
  {"id":"pavlakosOrdinalDepthSupervision2018","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Pavlakos","given":"Georgios"},{"family":"Zhou","given":"Xiaowei"},{"family":"Daniilidis","given":"Kostas"}],"citation-key":"pavlakosOrdinalDepthSupervision2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"7307-7316","source":"openaccess.thecvf.com","title":"Ordinal Depth Supervision for 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Pavlakos_Ordinal_Depth_Supervision_CVPR_2018_paper.html"},
  {"id":"pavllo3DHumanPose2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Pavllo","given":"Dario"},{"family":"Feichtenhofer","given":"Christoph"},{"family":"Grangier","given":"David"},{"family":"Auli","given":"Michael"}],"citation-key":"pavllo3DHumanPose2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"7753-7762","source":"openaccess.thecvf.com","title":"3D Human Pose Estimation in Video With Temporal Convolutions and Semi-Supervised Training","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Pavllo_3D_Human_Pose_Estimation_in_Video_With_Temporal_Convolutions_and_CVPR_2019_paper.html"},
  {"id":"pengRethinkingDepthEstimation2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Peng","given":"Rui"},{"family":"Wang","given":"Rongjie"},{"family":"Wang","given":"Zhenyu"},{"family":"Lai","given":"Yawen"},{"family":"Wang","given":"Ronggang"}],"citation-key":"pengRethinkingDepthEstimation2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"8645-8654","source":"openaccess.thecvf.com","title":"Rethinking Depth Estimation for Multi-View Stereo: A Unified Representation","title-short":"Rethinking Depth Estimation for Multi-View Stereo","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Peng_Rethinking_Depth_Estimation_for_Multi-View_Stereo_A_Unified_Representation_CVPR_2022_paper.html"},
  {"id":"pengSourcefreeDomainAdaptive2023","abstract":"Human Pose Estimation (HPE) is widely used in various fields, including motion analysis, healthcare, and virtual reality. However, the great expenses of labeled real-world datasets present a significant challenge for HPE. To overcome this, one approach is to train HPE models on synthetic datasets and then perform domain adaptation (DA) on real-world data. Unfortunately, existing DA methods for HPE neglect data privacy and security by using both source and target data in the adaptation process. To this end, we propose a new task, named source-free domain adaptive HPE, which aims to address the challenges of cross-domain learning of HPE without access to source data during the adaptation process. We further propose a novel framework that consists of three models: source model, intermediate model, and target model, which explores the task from both source-protect and target-relevant perspectives. The source-protect module preserves source information more effectively while resisting noise, and the target-relevant module reduces the sparsity of spatial representations by building a novel spatial probability space, and pose-specific contrastive learning and information maximization are proposed on the basis of this space. Comprehensive experiments on several domain adaptive HPE benchmarks show that the proposed method outperforms existing approaches by a considerable margin. The codes are available at https://github.com/davidpengucf/SFDAHPE.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Peng","given":"Qucheng"},{"family":"Zheng","given":"Ce"},{"family":"Chen","given":"Chen"}],"citation-key":"pengSourcefreeDomainAdaptive2023","DOI":"10.48550/arXiv.2308.03202","issued":{"date-parts":[["2023",8,18]]},"number":"arXiv:2308.03202","publisher":"arXiv","source":"arXiv.org","title":"Source-free Domain Adaptive Human Pose Estimation","type":"article","URL":"http://arxiv.org/abs/2308.03202"},
  {"id":"petersenHttpMatrixcookbookcom","author":[{"family":"Petersen","given":"Kaare Brandt"},{"family":"Pedersen","given":"Michael Syskind"}],"citation-key":"petersenHttpMatrixcookbookcom","language":"en","page":"72","source":"Zotero","title":"[ http://matrixcookbook.com ]","type":"article-journal"},
  {"id":"petrovaiExploitingPseudoLabels2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Petrovai","given":"Andra"},{"family":"Nedevschi","given":"Sergiu"}],"citation-key":"petrovaiExploitingPseudoLabels2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"1578-1588","source":"openaccess.thecvf.com","title":"Exploiting Pseudo Labels in a Self-Supervised Learning Framework for Improved Monocular Depth Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Petrovai_Exploiting_Pseudo_Labels_in_a_Self-Supervised_Learning_Framework_for_Improved_CVPR_2022_paper.html"},
  {"id":"petrovObjectPopCanWe2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Petrov","given":"Ilya A."},{"family":"Marin","given":"Riccardo"},{"family":"Chibane","given":"Julian"},{"family":"Pons-Moll","given":"Gerard"}],"citation-key":"petrovObjectPopCanWe2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"4726-4736","source":"openaccess.thecvf.com","title":"Object Pop-Up: Can We Infer 3D Objects and Their Poses From Human Interactions Alone?","title-short":"Object Pop-Up","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Petrov_Object_Pop-Up_Can_We_Infer_3D_Objects_and_Their_Poses_CVPR_2023_paper.html"},
  {"id":"PoseFieldEfficientMeanField","accessed":{"date-parts":[["2022",9,5]]},"citation-key":"PoseFieldEfficientMeanField","title":"PoseField: An Efficient Mean-Field Based Method for Joint Estimation of Human Pose, Segmentation, and Depth | SpringerLink","type":"webpage","URL":"https://link.springer.com/chapter/10.1007/978-3-642-40395-8_14"},
  {"id":"qianMagic123OneImage2023","abstract":"We present Magic123, a two-stage coarse-to-fine approach for high-quality, textured 3D meshes generation from a single unposed image in the wild using both2D and 3D priors. In the first stage, we optimize a neural radiance field to produce a coarse geometry. In the second stage, we adopt a memory-efficient differentiable mesh representation to yield a high-resolution mesh with a visually appealing texture. In both stages, the 3D content is learned through reference view supervision and novel views guided by a combination of 2D and 3D diffusion priors. We introduce a single trade-off parameter between the 2D and 3D priors to control exploration (more imaginative) and exploitation (more precise) of the generated geometry. Additionally, we employ textual inversion and monocular depth regularization to encourage consistent appearances across views and to prevent degenerate solutions, respectively. Magic123 demonstrates a significant improvement over previous image-to-3D techniques, as validated through extensive experiments on synthetic benchmarks and diverse real-world images. Our code, models, and generated 3D assets are available at https://github.com/guochengqian/Magic123.","accessed":{"date-parts":[["2023",10,23]]},"author":[{"family":"Qian","given":"Guocheng"},{"family":"Mai","given":"Jinjie"},{"family":"Hamdi","given":"Abdullah"},{"family":"Ren","given":"Jian"},{"family":"Siarohin","given":"Aliaksandr"},{"family":"Li","given":"Bing"},{"family":"Lee","given":"Hsin-Ying"},{"family":"Skorokhodov","given":"Ivan"},{"family":"Wonka","given":"Peter"},{"family":"Tulyakov","given":"Sergey"},{"family":"Ghanem","given":"Bernard"}],"citation-key":"qianMagic123OneImage2023","issued":{"date-parts":[["2023",7,23]]},"number":"arXiv:2306.17843","publisher":"arXiv","source":"arXiv.org","title":"Magic123: One Image to High-Quality 3D Object Generation Using Both 2D and 3D Diffusion Priors","title-short":"Magic123","type":"article","URL":"http://arxiv.org/abs/2306.17843"},
  {"id":"qianOrientedSpatialTransformer2020","abstract":"Pedestrian detection using ﬁsh-eye cameras is a principal research focus in computer vision. Lack of pedestrian datasets of ﬁsh-eye images and pedestrian distortion in ﬁsh-eye images are two primary challenges. In this paper, two approaches are proposed to deal with these two challenges, respectively. On the one hand, the projective model transformation (PMT) algorithm is proposed, which can transform normal images into ﬁsh-eye images. The PMT can be applied to most of the pedestrian datasets and generates corresponding ﬁsh-eye image datasets. In this way, enough training data can be provided through the PMT. On the other hand, the oriented spatial transformer network (OSTN) is designed to rectify warped pedestrian features using CNNs, so that pedestrians in ﬁsh-eye images are easier for detectors to recognize. The OSTN can be embedded into universal deep learning based detectors easily. Moreover, the new pedestrian detector, where the OSTN is embedded, can be trained end to end. Finally, the OSTN based ﬁsh-eye pedestrian detectors can be trained using ﬁsh-eye images, which are generated using the PMT. Experiments on ETH, KITTI, Citypersons, and real pedestrian datasets show the effectiveness of the PMT and accuracy improvement of pedestrian detection in ﬁsh-eye images using the OSTN.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Qian","given":"Yeqiang"},{"family":"Yang","given":"Ming"},{"family":"Zhao","given":"Xu"},{"family":"Wang","given":"Chunxiang"},{"family":"Wang","given":"Bing"}],"citation-key":"qianOrientedSpatialTransformer2020","container-title":"IEEE Transactions on Multimedia","container-title-short":"IEEE Trans. Multimedia","DOI":"10.1109/TMM.2019.2929949","ISSN":"1520-9210, 1941-0077","issue":"2","issued":{"date-parts":[["2020",2]]},"language":"en","note":"ZSCC:00027","number":"2","page":"421-431","source":"DOI.org (Crossref)","title":"Oriented Spatial Transformer Network for Pedestrian Detection Using Fish-Eye Camera","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/8772168/","volume":"22"},
  {"id":"qiPointNetDeepHierarchical2017","abstract":"Few prior works study deep learning on point sets. PointNet by Qi et al. is a pioneer in this direction. However, by design PointNet does not capture local structures induced by the metric space points live in, limiting its ability to recognize fine-grained patterns and generalizability to complex scenes. In this work, we introduce a hierarchical neural network that applies PointNet recursively on a nested partitioning of the input point set. By exploiting metric space distances, our network is able to learn local features with increasing contextual scales. With further observation that point sets are usually sampled with varying densities, which results in greatly decreased performance for networks trained on uniform densities, we propose novel set learning layers to adaptively combine features from multiple scales. Experiments show that our network called PointNet++ is able to learn deep point set features efficiently and robustly. In particular, results significantly better than state-of-the-art have been obtained on challenging benchmarks of 3D point clouds.","accessed":{"date-parts":[["2023",10,10]]},"author":[{"family":"Qi","given":"Charles R."},{"family":"Yi","given":"Li"},{"family":"Su","given":"Hao"},{"family":"Guibas","given":"Leonidas J."}],"citation-key":"qiPointNetDeepHierarchical2017","container-title":"arXiv.org","issued":{"date-parts":[["2017",6,7]]},"language":"en","title":"PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space","title-short":"PointNet++","type":"webpage","URL":"https://arxiv.org/abs/1706.02413v1"},
  {"id":"qiuCrossViewFusion2019","abstract":"We present an approach to recover absolute 3D human poses from multi-view images by incorporating multi-view geometric priors in our model. It consists of two separate steps: (1) estimating the 2D poses in multi-view images and (2) recovering the 3D poses from the multi-view 2D poses. First, we introduce a cross-view fusion scheme into CNN to jointly estimate 2D poses for multiple views. Consequently, the 2D pose estimation for each view already benefits from other views. Second, we present a recursive Pictorial Structure Model to recover the 3D pose from the multi-view 2D poses. It gradually improves the accuracy of 3D pose with affordable computational cost. We test our method on two public datasets H36M and Total Capture. The Mean Per Joint Position Errors on the two datasets are 26mm and 29mm, which outperforms the state-of-the-arts remarkably (26mm vs 52mm, 29mm vs 35mm). Our code is released at \\url{https://github.com/microsoft/multiview-human-pose-estimation-pytorch}.","accessed":{"date-parts":[["2021",5,14]]},"author":[{"family":"Qiu","given":"Haibo"},{"family":"Wang","given":"Chunyu"},{"family":"Wang","given":"Jingdong"},{"family":"Wang","given":"Naiyan"},{"family":"Zeng","given":"Wenjun"}],"citation-key":"qiuCrossViewFusion2019","container-title":"arXiv:1909.01203 [cs]","issued":{"date-parts":[["2019",9,3]]},"source":"arXiv.org","title":"Cross View Fusion for 3D Human Pose Estimation","type":"article-journal","URL":"http://arxiv.org/abs/1909.01203","version":"1"},
  {"id":"qiuCrossViewFusion2019a","abstract":"We present an approach to recover absolute 3D human poses from multi-view images by incorporating multi-view geometric priors in our model. It consists of two separate steps: (1) estimating the 2D poses in multi-view images and (2) recovering the 3D poses from the multi-view 2D poses. First, we introduce a cross-view fusion scheme into CNN to jointly estimate 2D poses for multiple views. Consequently, the 2D pose estimation for each view already beneﬁts from other views. Second, we present a recursive Pictorial Structure Model to recover the 3D pose from the multi-view 2D poses. It gradually improves the accuracy of 3D pose with affordable computational cost. We test our method on two public datasets H36M and Total Capture. The Mean Per Joint Position Errors on the two datasets are 26mm and 29mm, which outperforms the state-of-the-arts remarkably (26mm vs 52mm, 29mm vs 35mm).","accessed":{"date-parts":[["2021",1,16]]},"author":[{"family":"Qiu","given":"Haibo"},{"family":"Wang","given":"Chunyu"},{"family":"Wang","given":"Jingdong"},{"family":"Wang","given":"Naiyan"},{"family":"Zeng","given":"Wenjun"}],"citation-key":"qiuCrossViewFusion2019a","container-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV.2019.00444","event-place":"Seoul, Korea (South)","event-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","ISBN":"978-1-7281-4803-8","issued":{"date-parts":[["2019",10]]},"language":"en","page":"4341-4350","publisher":"IEEE","publisher-place":"Seoul, Korea (South)","source":"DOI.org (Crossref)","title":"Cross View Fusion for 3D Human Pose Estimation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9008809/"},
  {"id":"qiuEfficientTemporalSpatialFeature2020","abstract":"Temporal information plays an important role in action recognition. Recently, 3D CNN is widely used in extracting temporal features from videos. Compared to 2D CNN, 3D CNN has more parameters and brings heavy computation burden. It is necessary to improve the efﬁciency of action recognition. In this paper, inspired by group convolution and convolution kernel decomposition, we propose a novel module called grouped decomposed module (GDM) which separates channels into three groups and applies 3D, 2D and 1D convolution in parallel respectively. This module extracts spatial and temporal features efﬁciently. Based on GDM, we design a new network named grouped decomposed network (GDN). The grouped decomposed network achieves state-of-the-art performance on two temporal-related datasets (SomethingSomething V1 & V2) but requires few parameters and FLOPs.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Qiu","given":"Zhikang"},{"family":"Zhao","given":"Xu"},{"family":"Hu","given":"Zhilan"}],"citation-key":"qiuEfficientTemporalSpatialFeature2020","container-title":"2020 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP40778.2020.9190997","event-place":"Abu Dhabi, United Arab Emirates","event-title":"2020 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-7281-6395-6","issued":{"date-parts":[["2020",10]]},"language":"en","note":"ZSCC:00004","page":"2176-2180","publisher":"IEEE","publisher-place":"Abu Dhabi, United Arab Emirates","source":"DOI.org (Crossref)","title":"Efficient Temporal-Spatial Feature Grouping For Video Action Recognition","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9190997/"},
  {"id":"qiuPSVTEndEndMultiPerson2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Qiu","given":"Zhongwei"},{"family":"Yang","given":"Qiansheng"},{"family":"Wang","given":"Jian"},{"family":"Feng","given":"Haocheng"},{"family":"Han","given":"Junyu"},{"family":"Ding","given":"Errui"},{"family":"Xu","given":"Chang"},{"family":"Fu","given":"Dongmei"},{"family":"Wang","given":"Jingdong"}],"citation-key":"qiuPSVTEndEndMultiPerson2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"21254-21263","source":"openaccess.thecvf.com","title":"PSVT: End-to-End Multi-Person 3D Pose and Shape Estimation With Progressive Video Transformers","title-short":"PSVT","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Qiu_PSVT_End-to-End_Multi-Person_3D_Pose_and_Shape_Estimation_With_Progressive_CVPR_2023_paper.html"},
  {"id":"quCharacteristicFunctionBasedMethod2023","accessed":{"date-parts":[["2023",6,10]]},"author":[{"family":"Qu","given":"Haoxuan"},{"family":"Cai","given":"Yujun"},{"family":"Foo","given":"Lin Geng"},{"family":"Kumar","given":"Ajay"},{"family":"Liu","given":"Jun"}],"citation-key":"quCharacteristicFunctionBasedMethod2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"13009-13018","source":"openaccess.thecvf.com","title":"A Characteristic Function-Based Method for Bottom-Up Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Qu_A_Characteristic_Function-Based_Method_for_Bottom-Up_Human_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"rajasegaranBenefits3DPose2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Rajasegaran","given":"Jathushan"},{"family":"Pavlakos","given":"Georgios"},{"family":"Kanazawa","given":"Angjoo"},{"family":"Feichtenhofer","given":"Christoph"},{"family":"Malik","given":"Jitendra"}],"citation-key":"rajasegaranBenefits3DPose2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"640-649","source":"openaccess.thecvf.com","title":"On the Benefits of 3D Pose and Tracking for Human Action Recognition","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Rajasegaran_On_the_Benefits_of_3D_Pose_and_Tracking_for_Human_CVPR_2023_paper.html"},
  {"id":"rajasegaranTrackingPeoplePredicting2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Rajasegaran","given":"Jathushan"},{"family":"Pavlakos","given":"Georgios"},{"family":"Kanazawa","given":"Angjoo"},{"family":"Malik","given":"Jitendra"}],"citation-key":"rajasegaranTrackingPeoplePredicting2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"2740-2749","source":"openaccess.thecvf.com","title":"Tracking People by Predicting 3D Appearance, Location and Pose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Rajasegaran_Tracking_People_by_Predicting_3D_Appearance_Location_and_Pose_CVPR_2022_paper.html"},
  {"id":"ramakrishnaPoseMachinesArticulated2014","abstract":"State-of-the-art approaches for articulated human pose estimation are rooted in parts-based graphical models. These models are often restricted to tree-structured representations and simple parametric potentials in order to enable tractable inference. However, these simple dependencies fail to capture all the interactions between body parts. While models with more complex interactions can be defined, learning the parameters of these models remains challenging with intractable or approximate inference. In this paper, instead of performing inference on a learned graphical model, we build upon the inference machine framework and present a method for articulated human pose estimation. Our approach incorporates rich spatial interactions among multiple parts and information across parts of different scales. Additionally, the modular framework of our approach enables both ease of implementation without specialized optimization solvers, and efficient inference. We analyze our approach on two challenging datasets with large pose variation and outperform the state-of-the-art on these benchmarks.","author":[{"family":"Ramakrishna","given":"Varun"},{"family":"Munoz","given":"Daniel"},{"family":"Hebert","given":"Martial"},{"family":"Andrew Bagnell","given":"James"},{"family":"Sheikh","given":"Yaser"}],"citation-key":"ramakrishnaPoseMachinesArticulated2014","collection-title":"Lecture Notes in Computer Science","container-title":"Computer Vision – ECCV 2014","DOI":"10.1007/978-3-319-10605-2_3","editor":[{"family":"Fleet","given":"David"},{"family":"Pajdla","given":"Tomas"},{"family":"Schiele","given":"Bernt"},{"family":"Tuytelaars","given":"Tinne"}],"event-place":"Cham","ISBN":"978-3-319-10605-2","issued":{"date-parts":[["2014"]]},"language":"en","page":"33-47","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"Pose Machines: Articulated Pose Estimation via Inference Machines","title-short":"Pose Machines","type":"paper-conference"},
  {"id":"raychaudhuriPriorguidedSourcefreeDomain2023","abstract":"Domain adaptation methods for 2D human pose estimation typically require continuous access to the source data during adaptation, which can be challenging due to privacy, memory, or computational constraints. To address this limitation, we focus on the task of source-free domain adaptation for pose estimation, where a source model must adapt to a new target domain using only unlabeled target data. Although recent advances have introduced source-free methods for classification tasks, extending them to the regression task of pose estimation is non-trivial. In this paper, we present Prior-guided Self-training (POST), a pseudo-labeling approach that builds on the popular Mean Teacher framework to compensate for the distribution shift. POST leverages prediction-level and feature-level consistency between a student and teacher model against certain image transformations. In the absence of source data, POST utilizes a human pose prior that regularizes the adaptation process by directing the model to generate more accurate and anatomically plausible pose pseudo-labels. Despite being simple and intuitive, our framework can deliver significant performance gains compared to applying the source model directly to the target data, as demonstrated in our extensive experiments and ablation studies. In fact, our approach achieves comparable performance to recent state-of-the-art methods that use source data for adaptation.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Raychaudhuri","given":"Dripta S."},{"family":"Ta","given":"Calvin-Khang"},{"family":"Dutta","given":"Arindam"},{"family":"Lal","given":"Rohit"},{"family":"Roy-Chowdhury","given":"Amit K."}],"citation-key":"raychaudhuriPriorguidedSourcefreeDomain2023","DOI":"10.48550/arXiv.2308.13954","issued":{"date-parts":[["2023",8,26]]},"number":"arXiv:2308.13954","publisher":"arXiv","source":"arXiv.org","title":"Prior-guided Source-free Domain Adaptation for Human Pose Estimation","type":"article","URL":"http://arxiv.org/abs/2308.13954"},
  {"id":"redmonYOLO9000BetterFaster2017","abstract":"We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time.","author":[{"family":"Redmon","given":"Joseph"},{"family":"Farhadi","given":"Ali"}],"citation-key":"redmonYOLO9000BetterFaster2017","container-title":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR.2017.690","event-title":"2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)","ISSN":"1063-6919","issued":{"date-parts":[["2017",7]]},"page":"6517-6525","source":"IEEE Xplore","title":"YOLO9000: Better, Faster, Stronger","title-short":"YOLO9000","type":"paper-conference"},
  {"id":"remelliLightweightMultiView3D2020","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Remelli","given":"Edoardo"},{"family":"Han","given":"Shangchen"},{"family":"Honari","given":"Sina"},{"family":"Fua","given":"Pascal"},{"family":"Wang","given":"Robert"}],"citation-key":"remelliLightweightMultiView3D2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"6040-6049","source":"openaccess.thecvf.com","title":"Lightweight Multi-View 3D Pose Estimation Through Camera-Disentangled Representation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Remelli_Lightweight_Multi-View_3D_Pose_Estimation_Through_Camera-Disentangled_Representation_CVPR_2020_paper.html"},
  {"id":"RenTiJianCeBuFenXiangYingTeZhengYingSheDeRenTiDongZuoShiBiepdf2015","citation-key":"RenTiJianCeBuFenXiangYingTeZhengYingSheDeRenTiDongZuoShiBiepdf2015","container-title":"软件学报","issued":{"date-parts":[["2015"]]},"title":"人体检测部分响应特征映射的人体动作识别.pdf","type":"article-journal"},
  {"id":"RethinkingPose3D","accessed":{"date-parts":[["2022",3,5]]},"citation-key":"RethinkingPose3D","title":"Rethinking Pose in 3D: Multi-stage Refinement and Recovery for Markerless Motion Capture | IEEE Conference Publication | IEEE Xplore","type":"webpage","URL":"https://ieeexplore.ieee.org/document/8490999"},
  {"id":"rhodinLearningMonocular3D2018","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Rhodin","given":"Helge"},{"family":"Spörri","given":"Jörg"},{"family":"Katircioglu","given":"Isinsu"},{"family":"Constantin","given":"Victor"},{"family":"Meyer","given":"Frédéric"},{"family":"Müller","given":"Erich"},{"family":"Salzmann","given":"Mathieu"},{"family":"Fua","given":"Pascal"}],"citation-key":"rhodinLearningMonocular3D2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"8437-8446","source":"openaccess.thecvf.com","title":"Learning Monocular 3D Human Pose Estimation From Multi-View Images","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Rhodin_Learning_Monocular_3D_CVPR_2018_paper.html"},
  {"id":"rhodinUnsupervisedGeometryAwareRepresentation2018","abstract":"Modern 3D human pose estimation techniques rely on deep networks, which require large amounts of training data. While weakly-supervised methods require less supervision, by utilizing 2D poses or multi-view imagery without annotations, they still need a sufficiently large set of samples with 3D annotations for learning to succeed. In this paper, we propose to overcome this problem by learning a geometry-aware body representation from multi-view images without annotations. To this end, we use an encoder-decoder that predicts an image from one viewpoint given an image from another viewpoint. Because this representation encodes 3D geometry, using it in a semi-supervised setting makes it easier to learn a mapping from it to 3D human pose. As evidenced by our experiments, our approach significantly outperforms fully-supervised methods given the same amount of labeled data, and improves over other semi-supervised methods while using as little as 1% of the labeled data.","accessed":{"date-parts":[["2022",5,20]]},"author":[{"family":"Rhodin","given":"Helge"},{"family":"Salzmann","given":"Mathieu"},{"family":"Fua","given":"Pascal"}],"citation-key":"rhodinUnsupervisedGeometryAwareRepresentation2018","DOI":"10.48550/arXiv.1804.01110","issued":{"date-parts":[["2018",4,3]]},"number":"arXiv:1804.01110","publisher":"arXiv","source":"arXiv.org","title":"Unsupervised Geometry-Aware Representation for 3D Human Pose Estimation","type":"article","URL":"http://arxiv.org/abs/1804.01110"},
  {"id":"RobustDifferentiableSVD","accessed":{"date-parts":[["2023",3,15]]},"citation-key":"RobustDifferentiableSVD","title":"Robust Differentiable SVD | IEEE Journals & Magazine | IEEE Xplore","type":"webpage","URL":"https://ieeexplore.ieee.org/document/9400752"},
  {"id":"rogezLCRNetLocalizationClassificationRegressionHuman2017","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Rogez","given":"Gregory"},{"family":"Weinzaepfel","given":"Philippe"},{"family":"Schmid","given":"Cordelia"}],"citation-key":"rogezLCRNetLocalizationClassificationRegressionHuman2017","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2017"]]},"page":"3433-3441","source":"openaccess.thecvf.com","title":"LCR-Net: Localization-Classification-Regression for Human Pose","title-short":"LCR-Net","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2017/html/Rogez_LCR-Net_Localization-Classification-Regression_for_CVPR_2017_paper.html"},
  {"id":"romeroEmbodiedHandsModeling2017","abstract":"Humans move their hands and bodies together to communicate and solve tasks. Capturing and replicating such coordinated activity is critical for virtual characters that behave realistically. Surprisingly, most methods treat the 3D modeling and tracking of bodies and hands separately. Here we formulate a model of hands and bodies interacting together and fit it to full-body 4D sequences. When scanning or capturing the full body in 3D, hands are small and often partially occluded, making their shape and pose hard to recover. To cope with low-resolution, occlusion, and noise, we develop a new model called MANO (hand Model with Articulated and Non-rigid defOrmations). MANO is learned from around 1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand poses. The model is realistic, low-dimensional, captures non-rigid shape changes with pose, is compatible with standard graphics packages, and can fit any human hand. MANO provides a compact mapping from hand poses to pose blend shape corrections and a linear manifold of pose synergies. We attach MANO to a standard parameterized 3D body shape model (SMPL), resulting in a fully articulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting complex, natural, activities of subjects captured with a 4D scanner. The fitting is fully automatic and results in full body models that move naturally with detailed hand motions and a realism not seen before in full body performance capture. The models and data are freely available for research purposes in our website (http://mano.is.tue.mpg.de).","accessed":{"date-parts":[["2023",10,11]]},"author":[{"family":"Romero","given":"Javier"},{"family":"Tzionas","given":"Dimitrios"},{"family":"Black","given":"Michael J."}],"citation-key":"romeroEmbodiedHandsModeling2017","container-title":"ACM Transactions on Graphics","container-title-short":"ACM Trans. Graph.","DOI":"10.1145/3130800.3130883","ISSN":"0730-0301, 1557-7368","issue":"6","issued":{"date-parts":[["2017",12,31]]},"page":"1-17","source":"arXiv.org","title":"Embodied Hands: Modeling and Capturing Hands and Bodies Together","title-short":"Embodied Hands","type":"article-journal","URL":"http://arxiv.org/abs/2201.02610","volume":"36"},
  {"id":"romeroEmbodiedHandsModeling2017a","abstract":"Humans move their hands and bodies together to communicate and solve tasks. Capturing and replicating such coordinated activity is critical for virtual characters that behave realistically. Surprisingly, most methods treat the 3D modeling and tracking of bodies and hands separately. Here we formulate a model of hands and bodies interacting together and fit it to full-body 4D sequences. When scanning or capturing the full body in 3D, hands are small and often partially occluded, making their shape and pose hard to recover. To cope with low-resolution, occlusion, and noise, we develop a new model called MANO (hand Model with Articulated and Non-rigid defOrmations). MANO is learned from around 1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand poses. The model is realistic, low-dimensional, captures non-rigid shape changes with pose, is compatible with standard graphics packages, and can fit any human hand. MANO provides a compact mapping from hand poses to pose blend shape corrections and a linear manifold of pose synergies. We attach MANO to a standard parameterized 3D body shape model (SMPL), resulting in a fully articulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting complex, natural, activities of subjects captured with a 4D scanner. The fitting is fully automatic and results in full body models that move naturally with detailed hand motions and a realism not seen before in full body performance capture. The models and data are freely available for research purposes in our website (http://mano.is.tue.mpg.de).","accessed":{"date-parts":[["2023",9,24]]},"author":[{"family":"Romero","given":"Javier"},{"family":"Tzionas","given":"Dimitrios"},{"family":"Black","given":"Michael J."}],"citation-key":"romeroEmbodiedHandsModeling2017a","container-title":"ACM Transactions on Graphics","container-title-short":"ACM Trans. Graph.","DOI":"10.1145/3130800.3130883","ISSN":"0730-0301, 1557-7368","issue":"6","issued":{"date-parts":[["2017",12,31]]},"page":"1-17","source":"arXiv.org","title":"Embodied Hands: Modeling and Capturing Hands and Bodies Together","title-short":"Embodied Hands","type":"article-journal","URL":"http://arxiv.org/abs/2201.02610","volume":"36"},
  {"id":"royTriangulationFormSelfSupervision2022","abstract":"Supervised approaches to 3D pose estimation from single images are remarkably effective when labeled data is abundant. However, as the acquisition of ground-truth 3D labels is labor intensive and time consuming, recent attention has shifted towards semi- and weakly-supervised learning. Generating an effective form of supervision with little annotations still poses major challenge in crowded scenes. In this paper we propose to impose multi-view geometrical constraints by means of a weighted differentiable triangulation and use it as a form of self-supervision when no labels are available. We therefore train a 2D pose estimator in such a way that its predictions correspond to the re-projection of the triangulated 3D pose and train an auxiliary network on them to produce the final 3D poses. We complement the triangulation with a weighting mechanism that alleviates the impact of noisy predictions caused by self-occlusion or occlusion from other subjects. We demonstrate the effectiveness of our semi-supervised approach on Human3.6M and MPI-INF-3DHP datasets, as well as on a new multi-view multi-person dataset that features occlusion","citation-key":"royTriangulationFormSelfSupervision2022","collection-title":"International Conference on 3D Vision","container-title":"2022 International Conference On 3D Vision, 3Dv","DOI":"10.1109/3DV57658.2022.00068","editor":[{"family":"Roy","given":"Soumava Kumar"},{"family":"Citraro","given":"Leonardo"},{"family":"Honari","given":"Sina"},{"family":"Fua","given":"Pascal"}],"event-place":"New York","ISBN":"9781665456708","ISSN":"2475-7888","issued":{"date-parts":[["2022"]]},"number-of-pages":"10","publisher":"IEEE","publisher-place":"New York","source":"Infoscience","title":"On Triangulation as a Form of Self-Supervision for 3D Human Pose Estimation","type":"article-journal"},
  {"id":"sammonNonlinearMappingData1969","abstract":"An algorithm for the analysis of multivariate data is presented along with some experimental results. The algorithm is based upon a point mapping of N L-dimensional vectors from the L-space to a lower-dimensional space such that the inherent data \"structure\" is approximately preserved.","author":[{"family":"Sammon","given":"J.W."}],"citation-key":"sammonNonlinearMappingData1969","container-title":"IEEE Transactions on Computers","DOI":"10.1109/T-C.1969.222678","ISSN":"1557-9956","issue":"5","issued":{"date-parts":[["1969",5]]},"page":"401-409","source":"IEEE Xplore","title":"A Nonlinear Mapping for Data Structure Analysis","type":"article-journal","volume":"C-18"},
  {"id":"sarandiNeuralLocalizerFields2024","abstract":"With the explosive growth of available training data, single-image 3D human modeling is ahead of a transition to a data-centric paradigm. A key to successfully exploiting data scale is to design flexible models that can be supervised from various heterogeneous data sources produced by different researchers or vendors. To this end, we propose a simple yet powerful paradigm for seamlessly unifying different human pose and shape-related tasks and datasets. Our formulation is centered on the ability -- both at training and test time -- to query any arbitrary point of the human volume, and obtain its estimated location in 3D. We achieve this by learning a continuous neural field of body point localizer functions, each of which is a differently parameterized 3D heatmap-based convolutional point localizer (detector). For generating parametric output, we propose an efficient post-processing step for fitting SMPL-family body models to nonparametric joint and vertex predictions. With this approach, we can naturally exploit differently annotated data sources including mesh, 2D/3D skeleton and dense pose, without having to convert between them, and thereby train large-scale 3D human mesh and skeleton estimation models that considerably outperform the state-of-the-art on several public benchmarks including 3DPW, EMDB, EHF, SSP-3D and AGORA.","accessed":{"date-parts":[["2025",6,9]]},"author":[{"family":"Sárándi","given":"István"},{"family":"Pons-Moll","given":"Gerard"}],"citation-key":"sarandiNeuralLocalizerFields2024","container-title":"arXiv.org","issued":{"date-parts":[["2024",7,10]]},"language":"en","title":"Neural Localizer Fields for Continuous 3D Human Pose and Shape Estimation","type":"webpage","URL":"https://arxiv.org/abs/2407.07532v2"},
  {"id":"schmidtkeUnsupervisedHumanPose2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Schmidtke","given":"Luca"},{"family":"Vlontzos","given":"Athanasios"},{"family":"Ellershaw","given":"Simon"},{"family":"Lukens","given":"Anna"},{"family":"Arichi","given":"Tomoki"},{"family":"Kainz","given":"Bernhard"}],"citation-key":"schmidtkeUnsupervisedHumanPose2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"2484-2494","source":"openaccess.thecvf.com","title":"Unsupervised Human Pose Estimation Through Transforming Shape Templates","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Schmidtke_Unsupervised_Human_Pose_Estimation_Through_Transforming_Shape_Templates_CVPR_2021_paper.html"},
  {"id":"schmittJointEstimationPose2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Schmitt","given":"Carolin"},{"family":"Donne","given":"Simon"},{"family":"Riegler","given":"Gernot"},{"family":"Koltun","given":"Vladlen"},{"family":"Geiger","given":"Andreas"}],"citation-key":"schmittJointEstimationPose2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"3493-3503","source":"openaccess.thecvf.com","title":"On Joint Estimation of Pose, Geometry and svBRDF From a Handheld Scanner","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Schmitt_On_Joint_Estimation_of_Pose_Geometry_and_svBRDF_From_a_CVPR_2020_paper.html"},
  {"id":"senguptaHierarchicalKinematicProbability2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Sengupta","given":"Akash"},{"family":"Budvytis","given":"Ignas"},{"family":"Cipolla","given":"Roberto"}],"citation-key":"senguptaHierarchicalKinematicProbability2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11219-11229","source":"openaccess.thecvf.com","title":"Hierarchical Kinematic Probability Distributions for 3D Human Shape and Pose Estimation From Images in the Wild","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Sengupta_Hierarchical_Kinematic_Probability_Distributions_for_3D_Human_Shape_and_Pose_ICCV_2021_paper.html"},
  {"id":"senguptaProbabilistic3DHuman2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Sengupta","given":"Akash"},{"family":"Budvytis","given":"Ignas"},{"family":"Cipolla","given":"Roberto"}],"citation-key":"senguptaProbabilistic3DHuman2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"16094-16104","source":"openaccess.thecvf.com","title":"Probabilistic 3D Human Shape and Pose Estimation From Multiple Unconstrained Images in the Wild","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Sengupta_Probabilistic_3D_Human_Shape_and_Pose_Estimation_From_Multiple_Unconstrained_CVPR_2021_paper.html"},
  {"id":"shanDiffusionBased3DHuman2023","abstract":"In this paper, a novel Diffusion-based 3D Pose estimation (D3DP) method with Joint-wise reProjection-based Multi-hypothesis Aggregation (JPMA) is proposed for probabilistic 3D human pose estimation. On the one hand, D3DP generates multiple possible 3D pose hypotheses for a single 2D observation. It gradually diffuses the ground truth 3D poses to a random distribution, and learns a denoiser conditioned on 2D keypoints to recover the uncontaminated 3D poses. The proposed D3DP is compatible with existing 3D pose estimators and supports users to balance efficiency and accuracy during inference through two customizable parameters. On the other hand, JPMA is proposed to assemble multiple hypotheses generated by D3DP into a single 3D pose for practical use. It reprojects 3D pose hypotheses to the 2D camera plane, selects the best hypothesis joint-by-joint based on the reprojection errors, and combines the selected joints into the final pose. The proposed JPMA conducts aggregation at the joint level and makes use of the 2D prior information, both of which have been overlooked by previous approaches. Extensive experiments on Human3.6M and MPI-INF-3DHP datasets show that our method outperforms the state-of-the-art deterministic and probabilistic approaches by 1.5% and 8.9%, respectively. Code is available at https://github.com/paTRICK-swk/D3DP.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Shan","given":"Wenkang"},{"family":"Liu","given":"Zhenhua"},{"family":"Zhang","given":"Xinfeng"},{"family":"Wang","given":"Zhao"},{"family":"Han","given":"Kai"},{"family":"Wang","given":"Shanshe"},{"family":"Ma","given":"Siwei"},{"family":"Gao","given":"Wen"}],"citation-key":"shanDiffusionBased3DHuman2023","DOI":"10.48550/arXiv.2303.11579","issued":{"date-parts":[["2023",8,22]]},"number":"arXiv:2303.11579","publisher":"arXiv","source":"arXiv.org","title":"Diffusion-Based 3D Human Pose Estimation with Multi-Hypothesis Aggregation","type":"article","URL":"http://arxiv.org/abs/2303.11579"},
  {"id":"sharmaMonocular3DHuman2019","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Sharma","given":"Saurabh"},{"family":"Varigonda","given":"Pavan Teja"},{"family":"Bindal","given":"Prashast"},{"family":"Sharma","given":"Abhishek"},{"family":"Jain","given":"Arjun"}],"citation-key":"sharmaMonocular3DHuman2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"2325-2334","source":"openaccess.thecvf.com","title":"Monocular 3D Human Pose Estimation by Generation and Ordinal Ranking","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Sharma_Monocular_3D_Human_Pose_Estimation_by_Generation_and_Ordinal_Ranking_ICCV_2019_paper.html"},
  {"id":"shenGlobalLocalModelingVideoBased2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Shen","given":"Xiaolong"},{"family":"Yang","given":"Zongxin"},{"family":"Wang","given":"Xiaohan"},{"family":"Ma","given":"Jianxin"},{"family":"Zhou","given":"Chang"},{"family":"Yang","given":"Yi"}],"citation-key":"shenGlobalLocalModelingVideoBased2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8887-8896","source":"openaccess.thecvf.com","title":"Global-to-Local Modeling for Video-Based 3D Human Pose and Shape Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Shen_Global-to-Local_Modeling_for_Video-Based_3D_Human_Pose_and_Shape_Estimation_CVPR_2023_paper.html"},
  {"id":"sheraliReformulationlinearizationTechniqueSolving1999","author":[{"family":"Sherali","given":"Hanif D."},{"family":"Adams","given":"Warren P."}],"call-number":"T57.817 .S43 1999","citation-key":"sheraliReformulationlinearizationTechniqueSolving1999","collection-number":"v. 31","collection-title":"Nonconvex optimization and its applications","event-place":"Dordrecht ; Boston, Mass","ISBN":"978-0-7923-5487-1","issued":{"date-parts":[["1999"]]},"language":"en","number-of-pages":"514","publisher":"Kluwer Academic","publisher-place":"Dordrecht ; Boston, Mass","source":"Library of Congress ISBN","title":"A reformulation-linearization technique for solving discrete and continuous nonconvex problems","type":"book"},
  {"id":"shettyPLIKSPseudoLinearInverse2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Shetty","given":"Karthik"},{"family":"Birkhold","given":"Annette"},{"family":"Jaganathan","given":"Srikrishna"},{"family":"Strobel","given":"Norbert"},{"family":"Kowarschik","given":"Markus"},{"family":"Maier","given":"Andreas"},{"family":"Egger","given":"Bernhard"}],"citation-key":"shettyPLIKSPseudoLinearInverse2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"574-584","source":"openaccess.thecvf.com","title":"PLIKS: A Pseudo-Linear Inverse Kinematic Solver for 3D Human Body Estimation","title-short":"PLIKS","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Shetty_PLIKS_A_Pseudo-Linear_Inverse_Kinematic_Solver_for_3D_Human_Body_CVPR_2023_paper.html"},
  {"id":"shibataListeningHumanBehavior2023","accessed":{"date-parts":[["2023",6,10]]},"author":[{"family":"Shibata","given":"Yuto"},{"family":"Kawashima","given":"Yutaka"},{"family":"Isogawa","given":"Mariko"},{"family":"Irie","given":"Go"},{"family":"Kimura","given":"Akisato"},{"family":"Aoki","given":"Yoshimitsu"}],"citation-key":"shibataListeningHumanBehavior2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"13323-13332","source":"openaccess.thecvf.com","title":"Listening Human Behavior: 3D Human Pose Estimation With Acoustic Signals","title-short":"Listening Human Behavior","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Shibata_Listening_Human_Behavior_3D_Human_Pose_Estimation_With_Acoustic_Signals_CVPR_2023_paper.html"},
  {"id":"shiEndEndMultiPersonPose2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Shi","given":"Dahu"},{"family":"Wei","given":"Xing"},{"family":"Li","given":"Liangqi"},{"family":"Ren","given":"Ye"},{"family":"Tan","given":"Wenming"}],"citation-key":"shiEndEndMultiPersonPose2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"11069-11078","source":"openaccess.thecvf.com","title":"End-to-End Multi-Person Pose Estimation With Transformers","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Shi_End-to-End_Multi-Person_Pose_Estimation_With_Transformers_CVPR_2022_paper.html"},
  {"id":"shiMatchingNotEnough2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Shi","given":"Min"},{"family":"Huang","given":"Zihao"},{"family":"Ma","given":"Xianzheng"},{"family":"Hu","given":"Xiaowei"},{"family":"Cao","given":"Zhiguo"}],"citation-key":"shiMatchingNotEnough2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"7308-7317","source":"openaccess.thecvf.com","title":"Matching Is Not Enough: A Two-Stage Framework for Category-Agnostic Pose Estimation","title-short":"Matching Is Not Enough","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Shi_Matching_Is_Not_Enough_A_Two-Stage_Framework_for_Category-Agnostic_Pose_CVPR_2023_paper.html"},
  {"id":"shiMotioNet3DHuman2020","abstract":"We introduce MotioNet, a deep neural network that directly reconstructs the motion of a 3D human skeleton from monocular video.While previous methods rely on either rigging or inverse kinematics (IK) to associate a consistent skeleton with temporally coherent joint rotations, our method is the first data-driven approach that directly outputs a kinematic skeleton, which is a complete, commonly used, motion representation. At the crux of our approach lies a deep neural network with embedded kinematic priors, which decomposes sequences of 2D joint positions into two separate attributes: a single, symmetric, skeleton, encoded by bone lengths, and a sequence of 3D joint rotations associated with global root positions and foot contact labels. These attributes are fed into an integrated forward kinematics (FK) layer that outputs 3D positions, which are compared to a ground truth. In addition, an adversarial loss is applied to the velocities of the recovered rotations, to ensure that they lie on the manifold of natural joint rotations. The key advantage of our approach is that it learns to infer natural joint rotations directly from the training data, rather than assuming an underlying model, or inferring them from joint positions using a data-agnostic IK solver. We show that enforcing a single consistent skeleton along with temporally coherent joint rotations constrains the solution space, leading to a more robust handling of self-occlusions and depth ambiguities.","accessed":{"date-parts":[["2022",5,17]]},"author":[{"family":"Shi","given":"Mingyi"},{"family":"Aberman","given":"Kfir"},{"family":"Aristidou","given":"Andreas"},{"family":"Komura","given":"Taku"},{"family":"Lischinski","given":"Dani"},{"family":"Cohen-Or","given":"Daniel"},{"family":"Chen","given":"Baoquan"}],"citation-key":"shiMotioNet3DHuman2020","DOI":"10.48550/arXiv.2006.12075","issued":{"date-parts":[["2020",6,22]]},"number":"arXiv:2006.12075","publisher":"arXiv","source":"arXiv.org","title":"MotioNet: 3D Human Motion Reconstruction from Monocular Video with Skeleton Consistency","title-short":"MotioNet","type":"article","URL":"http://arxiv.org/abs/2006.12075","version":"1"},
  {"id":"singhNewFusionSalp2020","abstract":"The foremost objective of this article is to develop a novel hybrid powerful meta-heuristic that integrates the salp swarm algorithm with sine cosine algorithm (called HSSASCA) for improving the convergence performance with the exploration and exploitation being superior to other comparative standard algorithms. In this method, the position of salp swarm in the search space is updated using the position equations of sine cosine; hence the best and possible optimal solutions are obtained based on the sine or cosine function. During this process, each salp adopts the information sharing strategy of sine and cosine functions to improve their exploration and exploitation ability. The inspiration behind incorporating changes in salp swarm optimizer algorithm is to assist the basic approach to avoid premature convergence and to rapidly guide the search towards the probable search space. The algorithm is validated on 22 standard mathematical optimization functions and 3 applications namely the 3-bar truss, tension/compression spring and cantilever beam design problems. The aim is to examine and confirm the valuable behaviors of HSSASCA in searching the best solutions for optimization functions. The experimental results reveal that HSSASCA algorithm achieves the highest accuracies with least runtime in comparison with the others.","accessed":{"date-parts":[["2021",11,16]]},"author":[{"family":"Singh","given":"Narinder"},{"family":"Son","given":"Le Hoang"},{"family":"Chiclana","given":"Francisco"},{"family":"Magnot","given":"Jean-Pierre"}],"citation-key":"singhNewFusionSalp2020","container-title":"Engineering with Computers","container-title-short":"Engineering with Computers","DOI":"10.1007/s00366-018-00696-8","ISSN":"1435-5663","issue":"1","issued":{"date-parts":[["2020",1,1]]},"language":"en","page":"185-212","source":"Springer Link","title":"A new fusion of salp swarm with sine cosine for optimization of non-linear functions","type":"article-journal","URL":"https://doi.org/10.1007/s00366-018-00696-8","volume":"36"},
  {"id":"sinhaSparsePoseSparseViewCamera2022","abstract":"Camera pose estimation is a key step in standard 3D reconstruction pipelines that operate on a dense set of images of a single object or scene. However, methods for pose estimation often fail when only a few images are available because they rely on the ability to robustly identify and match visual features between image pairs. While these methods can work robustly with dense camera views, capturing a large set of images can be time-consuming or impractical. We propose SparsePose for recovering accurate camera poses given a sparse set of wide-baseline images (fewer than 10). The method learns to regress initial camera poses and then iteratively refine them after training on a large-scale dataset of objects (Co3D: Common Objects in 3D). SparsePose significantly outperforms conventional and learning-based baselines in recovering accurate camera rotations and translations. We also demonstrate our pipeline for high-fidelity 3D reconstruction using only 5-9 images of an object.","accessed":{"date-parts":[["2023",5,15]]},"author":[{"family":"Sinha","given":"Samarth"},{"family":"Zhang","given":"Jason Y."},{"family":"Tagliasacchi","given":"Andrea"},{"family":"Gilitschenski","given":"Igor"},{"family":"Lindell","given":"David B."}],"citation-key":"sinhaSparsePoseSparseViewCamera2022","DOI":"10.48550/arXiv.2211.16991","issued":{"date-parts":[["2022",11,29]]},"number":"arXiv:2211.16991","publisher":"arXiv","source":"arXiv.org","title":"SparsePose: Sparse-View Camera Pose Regression and Refinement","title-short":"SparsePose","type":"article","URL":"http://arxiv.org/abs/2211.16991"},
  {"id":"SMPLSkinnedMultiperson","accessed":{"date-parts":[["2021",9,1]]},"citation-key":"SMPLSkinnedMultiperson","title":"SMPL: a skinned multi-person linear model: ACM Transactions on Graphics: Vol 34, No 6","type":"webpage","URL":"https://dl.acm.org/doi/10.1145/2816795.2818013"},
  {"id":"sohl-dicksteinDeepUnsupervisedLearning","abstract":"A central problem in machine learning involves modeling complex data-sets using highly ﬂexible families of probability distributions in which learning, sampling, inference, and evaluation are still analytically or computationally tractable. Here, we develop an approach that simultaneously achieves both ﬂexibility and tractability. The essential idea, inspired by non-equilibrium statistical physics, is to systematically and slowly destroy structure in a data distribution through an iterative forward diffusion process. We then learn a reverse diffusion process that restores structure in data, yielding a highly ﬂexible and tractable generative model of the data. This approach allows us to rapidly learn, sample from, and evaluate probabilities in deep generative models with thousands of layers or time steps, as well as to compute conditional and posterior probabilities under the learned model. We additionally release an open source reference implementation of the algorithm.","author":[{"family":"Sohl-Dickstein","given":"Jascha"},{"family":"Weiss","given":"Eric A"},{"family":"Maheswaranathan","given":"Niru"},{"family":"Ganguli","given":"Surya"}],"citation-key":"sohl-dicksteinDeepUnsupervisedLearning","language":"en","source":"Zotero","title":"Deep Unsupervised Learning using Nonequilibrium Thermodynamics","type":"article-journal"},
  {"id":"songDiscriminativeRepresentationCombinations2019","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Song","given":"Xiao"},{"family":"Zhao","given":"Xu"},{"family":"Fang","given":"Liangji"},{"family":"Lin","given":"Tianwei"}],"citation-key":"songDiscriminativeRepresentationCombinations2019","container-title":"Pattern Recognition","container-title-short":"Pattern Recognition","DOI":"10.1016/j.patcog.2018.08.019","ISSN":"00313203","issued":{"date-parts":[["2019",1]]},"language":"en","note":"ZSCC:00057","page":"220-231","source":"DOI.org (Crossref)","title":"Discriminative representation combinations for accurate face spoofing detection","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0031320318303182","volume":"85"},
  {"id":"songEdgeStereoContextIntegrated2019","abstract":"Recent convolutional neural networks, especially end-to-end disparity estimation models, achieve remarkable performance on stereo matching task. However, existed methods, even with the complicated cascade structure, may fail in the regions of non-textures, boundaries and tiny details. Focus on these problems, we propose a multi-task network EdgeStereo that is composed of a backbone disparity network and an edge sub-network. Given a binocular image pair, our model enables end-to-end prediction of both disparity map and edge map. Basically, we design a context pyramid to encode multi-scale context information in disparity branch, followed by a compact residual pyramid for cascaded reﬁnement. To further preserve subtle details, our EdgeStereo model integrates edge cues by feature embedding and edge-aware smoothness loss regularization. Comparative results demonstrates that stereo matching and edge detection can help each other in the uniﬁed model. Furthermore, our method achieves state-of-art performance on both KITTI Stereo and Scene Flow benchmarks, which proves the eﬀectiveness of our design.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Song","given":"Xiao"},{"family":"Zhao","given":"Xu"},{"family":"Hu","given":"Hanwen"},{"family":"Fang","given":"Liangji"}],"citation-key":"songEdgeStereoContextIntegrated2019","container-title":"Computer Vision – ACCV 2018","DOI":"10.1007/978-3-030-20873-8_2","event-place":"Cham","ISBN":"978-3-030-20872-1 978-3-030-20873-8","issued":{"date-parts":[["2019"]]},"language":"en","note":"ZSCC:00163","page":"20-35","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"EdgeStereo: A Context Integrated Residual Pyramid Network for Stereo Matching","title-short":"EdgeStereo","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-030-20873-8_2","volume":"11365"},
  {"id":"songEdgeStereoEffectiveMultitask2020","abstract":"Recently, leveraging on the development of end-to-end convolutional neural networks, deep stereo matching networks have achieved remarkable performance far exceeding traditional approaches. However, state-of-the-art stereo frameworks still have difﬁculties at ﬁnding correct correspondences in texture-less regions, detailed structures, small objects and near boundaries, which could be alleviated by geometric clues such as edge contours and corresponding constraints. To improve the quality of disparity estimates in these challenging areas, we propose an effective multi-task learning network, EdgeStereo, composed of a disparity estimation branch and an edge detection branch, which enables end-to-end predictions of both disparity map and edge map. To effectively incorporate edge cues, we propose the edge-aware smoothness loss and edge feature embedding for inter-task interactions. It is demonstrated that based on our uniﬁed model, edge detection task and stereo matching task can promote each other. In addition, we design a compact module called residual pyramid to replace the commonly-used multi-stage cascaded structures or 3-D convolution based regularization modules in current stereo matching networks. By the time of the paper submission, EdgeStereo achieves state-of-art performance on the FlyingThings3D dataset, KITTI 2012 and KITTI 2015 stereo benchmarks, outperforming other published stereo matching methods by a noteworthy margin. EdgeStereo also achieves comparable generalization performance for disparity estimation because of the incorporation of edge cues.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Song","given":"Xiao"},{"family":"Zhao","given":"Xu"},{"family":"Fang","given":"Liangji"},{"family":"Hu","given":"Hanwen"},{"family":"Yu","given":"Yizhou"}],"citation-key":"songEdgeStereoEffectiveMultitask2020","container-title":"International Journal of Computer Vision","container-title-short":"Int J Comput Vis","DOI":"10.1007/s11263-019-01287-w","ISSN":"0920-5691, 1573-1405","issue":"4","issued":{"date-parts":[["2020",4]]},"language":"en","note":"ZSCC:00115","number":"4","page":"910-930","source":"DOI.org (Crossref)","title":"EdgeStereo: An Effective Multi-task Learning Network for Stereo Matching and Edge Detection","title-short":"EdgeStereo","type":"article-journal","URL":"http://link.springer.com/10.1007/s11263-019-01287-w","volume":"128"},
  {"id":"songFaceSpoofingDetection2017","abstract":"Robust features are of vital importance to face spooﬁng detection, because various situations make feature space extremely complicated to partition. Thus in this paper, two novel and robust features for anti-spooﬁng are proposed. The ﬁrst one is a binocular camera based depth feature called Template Face Matched Binocular Depth (TFBD) feature. The second one is a high-level micro-texture based feature called Spatial Pyramid Coding Micro-Texture (SPMT) feature. Novel template face registration algorithm and spatial pyramid coding algorithm are also introduced along with the two novel features. Multi-modal face spooﬁng detection is implemented based on these two robust features. Experiments are conducted on a widely used dataset and a comprehensive dataset constructed by ourselves. The results reveal that face spooﬁng detection with the fusion of our proposed features is of strong robustness and time efﬁciency, meanwhile outperforming other state-of-the-art traditional methods.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Song","given":"Xiao"},{"family":"Zhao","given":"Xu"},{"family":"Lin","given":"Tianwei"}],"citation-key":"songFaceSpoofingDetection2017","container-title":"2017 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2017.8296250","event-place":"Beijing","event-title":"2017 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-5090-2175-8","issued":{"date-parts":[["2017",9]]},"language":"en","note":"ZSCC:00012","page":"96-100","publisher":"IEEE","publisher-place":"Beijing","source":"DOI.org (Crossref)","title":"Face spoofing detection by fusing binocular depth and spatial pyramid coding micro-texture features","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/8296250/"},
  {"id":"songLargeScaleVideoUnderstanding2019","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Song","given":"Jingkuan"},{"family":"Zhao","given":"Xu"},{"family":"Gao","given":"Lianli"},{"family":"Cao","given":"Liangliang"}],"citation-key":"songLargeScaleVideoUnderstanding2019","container-author":[{"family":"Vrochidis","given":"Stefanos"},{"family":"Huet","given":"Benoit"},{"family":"Chang","given":"Edward"},{"family":"Kompatsiaris","given":"Ioannis"}],"container-title":"Big Data Analytics for Large-Scale Multimedia Search","DOI":"10.1002/9781119376996.ch4","event-place":"Chichester, UK","ISBN":"978-1-119-37699-6 978-1-119-37697-2","issued":{"date-parts":[["2019",3,15]]},"language":"en","note":"Book Authors: _:n354","page":"89-120","publisher":"John Wiley & Sons, Ltd","publisher-place":"Chichester, UK","source":"DOI.org (Crossref)","title":"Large-Scale Video Understanding with Limited Training Labels","type":"chapter","URL":"https://onlinelibrary.wiley.com/doi/10.1002/9781119376996.ch4"},
  {"id":"songStereoscopicImageSuperResolution2020","abstract":"We present a first attempt for stereoscopic image super-resolution (SR) for recovering high-resolution details while preserving stereo-consistency between stereoscopic image pair. The most challenging issue in the stereoscopic SR is that the texture details should be consistent for corresponding pixels in stereoscopic SR image pair. However, existing stereo SR methods cannot maintain the stereo-consistency, thus causing 3D fatigue to the viewers. To address this issue, in this paper, we propose a self and parallax attention mechanism (SPAM) to aggregate the information from its own image and the counterpart stereo image simultaneously, thus reconstructing high-quality stereoscopic SR image pairs. Moreover, we design an efficient network architecture and effective loss functions to enforce stereo-consistency constraint. Finally, experimental results demonstrate the superiority of our method over state-of-the-art SR methods in terms of both quantitative metrics and qualitative visual quality while maintaining stereo-consistency between stereoscopic image pair.","accessed":{"date-parts":[["2023",11,8]]},"author":[{"family":"Song","given":"Wonil"},{"family":"Choi","given":"Sungil"},{"family":"Jeong","given":"Somi"},{"family":"Sohn","given":"Kwanghoon"}],"citation-key":"songStereoscopicImageSuperResolution2020","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","DOI":"10.1609/aaai.v34i07.6880","ISSN":"2374-3468","issue":"07","issued":{"date-parts":[["2020",4,3]]},"language":"en","license":"Copyright (c) 2020 Association for the Advancement of Artificial Intelligence","number":"07","page":"12031-12038","source":"ojs.aaai.org","title":"Stereoscopic Image Super-Resolution with Stereo Consistent Feature","type":"article-journal","URL":"https://ojs.aaai.org/index.php/AAAI/article/view/6880","volume":"34"},
  {"id":"StackedHourglassNetworks","accessed":{"date-parts":[["2023",10,26]]},"citation-key":"StackedHourglassNetworks","title":"Stacked Hourglass Networks for Human Pose Estimation | SpringerLink","type":"webpage","URL":"https://link.springer.com/chapter/10.1007/978-3-319-46484-8_29"},
  {"id":"StructuralTriangulationClosedForm","abstract":"一个简洁的在线 LaTeX 编辑器。无需安装，实时共享，版本控制，数百免费模板……","accessed":{"date-parts":[["2022",3,5]]},"citation-key":"StructuralTriangulationClosedForm","language":"cn","title":"Structural Triangulation: A Closed-Form Solution to 3D Human Pose Estimation","title-short":"Structural Triangulation","type":"webpage","URL":"https://latex.sjtu.edu.cn/project/61e90e3ab8e6fc009ca168c8"},
  {"id":"suCascadedPyramidMining2019","abstract":"Weakly supervised temporal action localization, which aims at temporally locating action instances in untrimmed videos using only video-level class labels during training, is an important yet challenging problem in video analysis. Many current methods adopt the “localization by classiﬁcation” framework: ﬁrst do video classiﬁcation, then locate temporal area contributing to the results most. However, this framework fails to locate the entire action instances and gives little consideration to the local context. In this paper, we present a novel architecture called Cascaded Pyramid Mining Network (CPMN) to address these issues using two eﬀective modules. First, to discover the entire temporal interval of speciﬁc action, we design a two-stage cascaded module with proposed Online Adversarial Erasing (OAE) mechanism, where new and complementary regions are mined through feeding the erased feature maps of discovered regions back to the system. Second, to exploit hierarchical contextual information in videos and reduce missing detections, we design a pyramid module which produces a scale-invariant attention map through combining the feature maps from diﬀerent levels. Final, we aggregate the results of two modules to perform action localization via locating high score areas in temporal Class Activation Sequence (CAS). Extensive experiments conducted on THUMOS14 and ActivityNet-1.3 datasets demonstrate the eﬀectiveness of our method.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Su","given":"Haisheng"},{"family":"Zhao","given":"Xu"},{"family":"Lin","given":"Tianwei"}],"citation-key":"suCascadedPyramidMining2019","container-title":"Computer Vision – ACCV 2018","DOI":"10.1007/978-3-030-20890-5_36","event-place":"Cham","ISBN":"978-3-030-20889-9 978-3-030-20890-5","issued":{"date-parts":[["2019"]]},"language":"en","note":"ZSCC:00020","page":"558-574","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Cascaded Pyramid Mining Network for Weakly Supervised Temporal Action Localization","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-030-20890-5_36","volume":"11362"},
  {"id":"suiJointIntentionTrajectory2021","abstract":"Although autonomous driving technology has made tremendous progress in recent years, it is still challenging to predict the intentions and trajectories of pedestrians. The state-of-the-art methods suffer from two problems. (1) Existing works consider these two tasks separately, ignoring the connection between them. (2) The selection and integration of inputs for these tasks are not well designed. In this paper, these two tasks are taken into consideration in a uniﬁed model. In this way, the information provided by the labels of each other is shared, improving the performance of both tasks. Besides, in addition to the bounding boxes and speeds, orientation and road semantic segmentation features are taken into consideration to show the potential intention and road context of the pedestrian. And all the inputs are weighted by an attention module before integration. Meanwhile, a Transformer encoder is applied in our method to extract the temporal information from the fused feature sequence. Our method outperforms all previous models for both trajectory prediction and intention prediction tasks on the JAAD dataset and PIE dataset.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Sui","given":"Ze"},{"family":"Zhou","given":"Yue"},{"family":"Zhao","given":"Xu"},{"family":"Chen","given":"Ao"},{"family":"Ni","given":"Yiyang"}],"citation-key":"suiJointIntentionTrajectory2021","container-title":"2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","DOI":"10.1109/IROS51168.2021.9636241","event-place":"Prague, Czech Republic","event-title":"2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)","ISBN":"978-1-6654-1714-3","issued":{"date-parts":[["2021",9,27]]},"language":"en","note":"ZSCC:00010","page":"7082-7088","publisher":"IEEE","publisher-place":"Prague, Czech Republic","source":"DOI.org (Crossref)","title":"Joint Intention and Trajectory Prediction Based on Transformer","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9636241/"},
  {"id":"sunDeepHighResolutionRepresentation2019","abstract":"This is an official pytorch implementation of Deep High-Resolution Representation Learning for Human Pose Estimation. In this work, we are interested in the human pose estimation problem with a focus on learning reliable high-resolution representations. Most existing methods recover high-resolution representations from low-resolution representations produced by a high-to-low resolution network. Instead, our proposed network maintains high-resolution representations through the whole process. We start from a high-resolution subnetwork as the first stage, gradually add high-to-low resolution subnetworks one by one to form more stages, and connect the mutli-resolution subnetworks in parallel. We conduct repeated multi-scale fusions such that each of the high-to-low resolution representations receives information from other parallel representations over and over, leading to rich high-resolution representations. As a result, the predicted keypoint heatmap is potentially more accurate and spatially more precise. We empirically demonstrate the effectiveness of our network through the superior pose estimation results over two benchmark datasets: the COCO keypoint detection dataset and the MPII Human Pose dataset. The code and models have been publicly available at \\url{https://github.com/leoxiaobin/deep-high-resolution-net.pytorch}.","accessed":{"date-parts":[["2022",11,28]]},"author":[{"family":"Sun","given":"Ke"},{"family":"Xiao","given":"Bin"},{"family":"Liu","given":"Dong"},{"family":"Wang","given":"Jingdong"}],"citation-key":"sunDeepHighResolutionRepresentation2019","DOI":"10.48550/arXiv.1902.09212","issued":{"date-parts":[["2019",2,25]]},"number":"arXiv:1902.09212","publisher":"arXiv","source":"arXiv.org","title":"Deep High-Resolution Representation Learning for Human Pose Estimation","type":"article","URL":"http://arxiv.org/abs/1902.09212"},
  {"id":"suTransferableKnowledgeBasedMultiGranularity2021","abstract":"Despite remarkable progress, temporal action detection is still limited for real application due to the great amount of manual annotations. This issue motivates interest in addressing this task under weak supervision, namely, locating the action instances using only video-level class labels. Many current works on this task are mainly based on the Class Activation Sequence (CAS), which is generated by the video classiﬁcation network to describe the probability of each snippet being in a speciﬁc action class of the video. However, the CAS generated by a simple classiﬁcation network can only focus on local discriminative parts instead of locating the entire interval of target actions. In this paper, we present a novel framework to handle this issue. Speciﬁcally, we propose to utilize convolutional kernels with varied dilation rates to enlarge the receptive ﬁelds, which can transfer the discriminative information to the surrounding non-discriminative regions. Then, we design a cascaded module with the proposed Online Adversarial Erasing (OAE) mechanism to further mine more relevant regions of target actions by feeding the erased-feature maps of discovered regions back into the system. In addition, inspired by the transfer learning method, we adopt an additional module to transfer the knowledge from trimmed videos to untrimmed videos to promote the classiﬁcation performance on untrimmed videos. Finally, we employ a boundary regression module embedded with Outer-Inner-Contrastive (OIC) loss to automatically predict the boundaries based on the enhanced CAS. Extensive experiments are conducted on two challenging datasets, THUMOS14 and ActivityNet-1.3, and the experimental results clearly demonstrate the superiority of our uniﬁed framework.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Su","given":"Haisheng"},{"family":"Zhao","given":"Xu"},{"family":"Lin","given":"Tianwei"},{"family":"Liu","given":"Shuming"},{"family":"Hu","given":"Zhilan"}],"citation-key":"suTransferableKnowledgeBasedMultiGranularity2021","container-title":"IEEE Transactions on Multimedia","container-title-short":"IEEE Trans. Multimedia","DOI":"10.1109/TMM.2020.2999184","ISSN":"1520-9210, 1941-0077","issued":{"date-parts":[["2021"]]},"language":"en","note":"ZSCC:00010","page":"1503-1515","source":"DOI.org (Crossref)","title":"Transferable Knowledge-Based Multi-Granularity Fusion Network for Weakly Supervised Temporal Action Detection","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9105103/","volume":"23"},
  {"id":"suWeaklySupervisedTemporal2018","abstract":"Weakly supervised temporal action detection in untrimmed videos is an important yet challenging task, where only video-level class labels are available for temporally locating actions in the videos during training. In this paper, we propose a novel architecture for this task. Speciﬁcally, we put forward an eﬀective shot-based sampling method aiming at generating a more simpliﬁed but representative feature sequence for action detection, instead of using uniform sampling which causes extremely irrelevant frames retained. Furthermore, in order to distinguish action instances existing in the videos, we design a multistage Temporal Pooling Network (TPN) for the purposes of predicting video categories and localizing class-speciﬁc action instances respectively. Experiments conducted on THUMOS14 dataset conﬁrm that our method outperforms other state-of-the-art weakly supervised approaches.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Su","given":"Haisheng"},{"family":"Zhao","given":"Xu"},{"family":"Lin","given":"Tianwei"},{"family":"Fei","given":"Haiping"}],"citation-key":"suWeaklySupervisedTemporal2018","container-title":"ICONIP","DOI":"10.1007/978-3-030-04212-7_37","event-place":"Cham","event-title":"Neural Information Processing","ISBN":"978-3-030-04211-0 978-3-030-04212-7","issued":{"date-parts":[["2018"]]},"language":"en","page":"426-436","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Weakly Supervised Temporal Action Detection with Shot-Based Temporal Pooling Network","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-030-04212-7_37"},
  {"id":"tang3DHumanPose2023","accessed":{"date-parts":[["2023",6,9]]},"author":[{"family":"Tang","given":"Zhenhua"},{"family":"Qiu","given":"Zhaofan"},{"family":"Hao","given":"Yanbin"},{"family":"Hong","given":"Richang"},{"family":"Yao","given":"Ting"}],"citation-key":"tang3DHumanPose2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"4790-4799","source":"openaccess.thecvf.com","title":"3D Human Pose Estimation With Spatio-Temporal Criss-Cross Attention","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Tang_3D_Human_Pose_Estimation_With_Spatio-Temporal_Criss-Cross_Attention_CVPR_2023_paper.html"},
  {"id":"tangDoesLearningSpecific2019","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Tang","given":"Wei"},{"family":"Wu","given":"Ying"}],"citation-key":"tangDoesLearningSpecific2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"1107-1116","source":"openaccess.thecvf.com","title":"Does Learning Specific Features for Related Parts Help Human Pose Estimation?","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Tang_Does_Learning_Specific_Features_for_Related_Parts_Help_Human_Pose_CVPR_2019_paper.html"},
  {"id":"tangOpeningBlackBox2015","abstract":"We address the problem of hand pose estimation, formulated as an inverse problem. Typical approaches optimize an energy function over pose parameters using a 'black box' image generation procedure. This procedure knows little about either the relationships between the parameters or the form of the energy function. In this paper, we show that we can significantly improving upon black box optimization by exploiting high-level knowledge of the structure of the parameters and using a local surrogate energy function. Our new framework, hierarchical sampling optimization, consists of a sequence of predictors organized into a kinematic hierarchy. Each predictor is conditioned on its ancestors, and generates a set of samples over a subset of the pose parameters. The highly-efficient surrogate energy is used to select among samples. Having evaluated the full hierarchy, the partial pose samples are concatenated to generate a full-pose hypothesis. Several hypotheses are generated using the same procedure, and finally the original full energy function selects the best result. Experimental evaluation on three publically available datasets show that our method is particularly impressive in low-compute scenarios where it significantly outperforms all other state-of-the-art methods.","author":[{"family":"Tang","given":"Danhang"},{"family":"Taylor","given":"Jonathan"},{"family":"Kohli","given":"Pushmeet"},{"family":"Keskin","given":"Cem"},{"family":"Kim","given":"Tae-Kyun"},{"family":"Shotton","given":"Jamie"}],"citation-key":"tangOpeningBlackBox2015","container-title":"2015 IEEE International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV.2015.380","event-title":"2015 IEEE International Conference on Computer Vision (ICCV)","ISSN":"2380-7504","issued":{"date-parts":[["2015",12]]},"page":"3325-3333","source":"IEEE Xplore","title":"Opening the Black Box: Hierarchical Sampling Optimization for Estimating Human Hand Pose","title-short":"Opening the Black Box","type":"paper-conference"},
  {"id":"tiwariPoseNDFModelingHuman2022","abstract":"We present Pose-NDF, a continuous model for plausible human poses based on neural distance fields (NDFs). Pose or motion priors are important for generating realistic new poses and for reconstructing accurate poses from noisy or partial observations. Pose-NDF learns a manifold of plausible poses as the zero level set of a neural implicit function, extending the idea of modeling implicit surfaces in 3D to the high-dimensional domain $$SO(3)^K$$SO(3)K, where a human pose is defined by a single data point, represented by K quaternions. The resulting high-dimensional implicit function can be differentiated with respect to the input poses and thus can be used to project arbitrary poses onto the manifold by using gradient descent on the set of 3-dimensional hyperspheres. In contrast to previous VAE-based human pose priors, which transform the pose space into a Gaussian distribution, we model the actual pose manifold, preserving the distances between poses. We demonstrate that Pose-NDF outperforms existing state-of-the-art methods as a prior in various downstream tasks, ranging from denoising real-world human mocap data, pose recovery from occluded data to 3D pose reconstruction from images. Furthermore, we show that it can be used to generate more diverse poses by random sampling and projection than VAE-based methods. We will release our code and pre-trained model for further research at https://virtualhumans.mpi-inf.mpg.de/posendf/.","author":[{"family":"Tiwari","given":"Garvita"},{"family":"Antić","given":"Dimitrije"},{"family":"Lenssen","given":"Jan Eric"},{"family":"Sarafianos","given":"Nikolaos"},{"family":"Tung","given":"Tony"},{"family":"Pons-Moll","given":"Gerard"}],"citation-key":"tiwariPoseNDFModelingHuman2022","collection-title":"Lecture Notes in Computer Science","container-title":"Computer Vision – ECCV 2022","DOI":"10.1007/978-3-031-20065-6_33","editor":[{"family":"Avidan","given":"Shai"},{"family":"Brostow","given":"Gabriel"},{"family":"Cissé","given":"Moustapha"},{"family":"Farinella","given":"Giovanni Maria"},{"family":"Hassner","given":"Tal"}],"event-place":"Cham","ISBN":"978-3-031-20065-6","issued":{"date-parts":[["2022"]]},"language":"en","page":"572-589","publisher":"Springer Nature Switzerland","publisher-place":"Cham","source":"Springer Link","title":"Pose-NDF: Modeling Human Pose Manifolds with Neural Distance Fields","title-short":"Pose-NDF","type":"paper-conference"},
  {"id":"tomeLiftingDeepConvolutional2017","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Tome","given":"Denis"},{"family":"Russell","given":"Chris"},{"family":"Agapito","given":"Lourdes"}],"citation-key":"tomeLiftingDeepConvolutional2017","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2017"]]},"page":"2500-2509","source":"openaccess.thecvf.com","title":"Lifting From the Deep: Convolutional 3D Pose Estimation From a Single Image","title-short":"Lifting From the Deep","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2017/html/Tome_Lifting_From_the_CVPR_2017_paper.html"},
  {"id":"tomeXREgoPoseEgocentric3D2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Tome","given":"Denis"},{"family":"Peluse","given":"Patrick"},{"family":"Agapito","given":"Lourdes"},{"family":"Badino","given":"Hernan"}],"citation-key":"tomeXREgoPoseEgocentric3D2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"7728-7738","source":"openaccess.thecvf.com","title":"xR-EgoPose: Egocentric 3D Human Pose From an HMD Camera","title-short":"xR-EgoPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Tome_xR-EgoPose_Egocentric_3D_Human_Pose_From_an_HMD_Camera_ICCV_2019_paper.html"},
  {"id":"tookQuaternionLMSAlgorithm2009","abstract":"The quaternion least mean square (QLMS) algorithm is introduced for adaptive filtering of three- and four-dimensional processes, such as those observed in atmospheric modeling (wind, vector fields). These processes exhibit complex nonlinear dynamics and coupling between the dimensions, which make their component-wise processing by multiple univariate LMS, bivariate complex LMS (CLMS), or multichannel LMS (MLMS) algorithms inadequate. The QLMS accounts for these problems naturally, as it is derived directly in the quaternion domain. The analysis shows that QLMS operates inherently based on the so called ldquoaugmentedrdquo statistics, that is, both the covariance E xx H and pseudocovariance E xx T of the tap input vector x are taken into account. In addition, the operation in the quaternion domain facilitates fusion of heterogeneous data sources, for instance, the three vector dimensions of the wind field and air temperature. Simulations on both benchmark and real world data support the approach.","author":[{"family":"Took","given":"Clive Cheong"},{"family":"Mandic","given":"Danilo P."}],"citation-key":"tookQuaternionLMSAlgorithm2009","container-title":"IEEE Transactions on Signal Processing","DOI":"10.1109/TSP.2008.2010600","ISSN":"1941-0476","issue":"4","issued":{"date-parts":[["2009",4]]},"page":"1316-1327","source":"IEEE Xplore","title":"The Quaternion LMS Algorithm for Adaptive Filtering of Hypercomplex Processes","type":"article-journal","volume":"57"},
  {"id":"toshevDeepPoseHumanPose2014","accessed":{"date-parts":[["2022",11,22]]},"author":[{"family":"Toshev","given":"Alexander"},{"family":"Szegedy","given":"Christian"}],"citation-key":"toshevDeepPoseHumanPose2014","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2014"]]},"page":"1653-1660","source":"openaccess.thecvf.com","title":"DeepPose: Human Pose Estimation via Deep Neural Networks","title-short":"DeepPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2014/html/Toshev_DeepPose_Human_Pose_2014_CVPR_paper.html"},
  {"id":"TriangulationScienceDirect","accessed":{"date-parts":[["2021",1,1]]},"citation-key":"TriangulationScienceDirect","title":"Triangulation - ScienceDirect","type":"webpage","URL":"https://www.sciencedirect.com/science/article/pii/S1077314297905476"},
  {"id":"tripathi3DHumanPose2023","accessed":{"date-parts":[["2023",6,10]]},"author":[{"family":"Tripathi","given":"Shashank"},{"family":"Müller","given":"Lea"},{"family":"Huang","given":"Chun-Hao P."},{"family":"Taheri","given":"Omid"},{"family":"Black","given":"Michael J."},{"family":"Tzionas","given":"Dimitrios"}],"citation-key":"tripathi3DHumanPose2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"4713-4725","source":"openaccess.thecvf.com","title":"3D Human Pose Estimation via Intuitive Physics","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Tripathi_3D_Human_Pose_Estimation_via_Intuitive_Physics_CVPR_2023_paper.html"},
  {"id":"trumbleTotalCapture2017","accessed":{"date-parts":[["2022",3,11]]},"author":[{"family":"Trumble","given":"Matthew"},{"family":"Gilbert","given":"Andrew"},{"family":"Malleson","given":"Charles"},{"family":"Hilton","given":"Adrian"},{"family":"Collomosse","given":"John"}],"citation-key":"trumbleTotalCapture2017","container-title":"Procedings of the British Machine Vision Conference 2017","DOI":"10.5244/C.31.14","event-place":"London, UK","event-title":"British Machine Vision Conference 2017","ISBN":"978-1-901725-60-5","issued":{"date-parts":[["2017"]]},"language":"en","page":"14","publisher":"British Machine Vision Association","publisher-place":"London, UK","source":"DOI.org (Crossref)","title":"Total Capture","title-short":"Total Capture","type":"paper-conference","URL":"http://www.bmva.org/bmvc/2017/papers/paper014/index.html"},
  {"id":"tseCollaborativeLearningHand2022","abstract":"Estimating the pose and shape of hands and objects under interaction finds numerous applications including augmented and virtual reality. Existing approaches for hand and object reconstruction require explicitly defined physical constraints and known objects, which limits its application domains. Our algorithm is agnostic to object models, and it learns the physical rules governing hand-object interaction. This requires automatically inferring the shapes and physical interaction of hands and (potentially unknown) objects. We seek to approach this challenging problem by proposing a collaborative learning strategy where two-branches of deep networks are learning from each other. Specifically, we transfer hand mesh information to the object branch and vice versa for the hand branch. The resulting optimisation (training) problem can be unstable, and we address this via two strategies: (i) attention-guided graph convolution which helps identify and focus on mutual occlusion and (ii) unsupervised associative loss which facilitates the transfer of information between the branches. Experiments using four widely-used benchmarks show that our framework achieves beyond state-of-the-art accuracy in 3D pose estimation, as well as recovers dense 3D hand and object shapes. Each technical component above contributes meaningfully in the ablation study.","accessed":{"date-parts":[["2023",9,26]]},"author":[{"family":"Tse","given":"Tze Ho Elden"},{"family":"Kim","given":"Kwang In"},{"family":"Leonardis","given":"Ales"},{"family":"Chang","given":"Hyung Jin"}],"citation-key":"tseCollaborativeLearningHand2022","container-title":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR52688.2022.00171","event-place":"New Orleans, LA, USA","event-title":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-6654-6946-3","issued":{"date-parts":[["2022",6]]},"language":"en","page":"1654-1664","publisher":"IEEE","publisher-place":"New Orleans, LA, USA","source":"DOI.org (Crossref)","title":"Collaborative Learning for Hand and Object Reconstruction with Attention-guided Graph Convolution","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9878674/"},
  {"id":"tseS$^2$ContactGraphbasedNetwork2023","abstract":"Despite the recent efforts in accurate 3D annotations in hand and object datasets, there still exist gaps in 3D hand and object reconstructions. Existing works leverage contact maps to refine inaccurate hand-object pose estimations and generate grasps given object models. However, they require explicit 3D supervision which is seldom available and therefore, are limited to constrained settings, e.g., where thermal cameras observe residual heat left on manipulated objects. In this paper, we propose a novel semi-supervised framework that allows us to learn contact from monocular images. Specifically, we leverage visual and geometric consistency constraints in large-scale datasets for generating pseudo-labels in semi-supervised learning and propose an efficient graph-based network to infer contact. Our semi-supervised learning framework achieves a favourable improvement over the existing supervised learning methods trained on data with `limited' annotations. Notably, our proposed model is able to achieve superior results with less than half the network parameters and memory access cost when compared with the commonly-used PointNet-based approach. We show benefits from using a contact map that rules hand-object interactions to produce more accurate reconstructions. We further demonstrate that training with pseudo-labels can extend contact map estimations to out-of-domain objects and generalise better across multiple datasets.","accessed":{"date-parts":[["2023",9,25]]},"author":[{"family":"Tse","given":"Tze Ho Elden"},{"family":"Zhang","given":"Zhongqun"},{"family":"Kim","given":"Kwang In"},{"family":"Leonardis","given":"Ales"},{"family":"Zheng","given":"Feng"},{"family":"Chang","given":"Hyung Jin"}],"citation-key":"tseS$^2$ContactGraphbasedNetwork2023","issued":{"date-parts":[["2023",8,3]]},"number":"arXiv:2208.00874","publisher":"arXiv","source":"arXiv.org","title":"S$^2$Contact: Graph-based Network for 3D Hand-Object Contact Estimation with Semi-Supervised Learning","title-short":"S$^2$Contact","type":"article","URL":"http://arxiv.org/abs/2208.00874"},
  {"id":"tseSpectralGraphormerSpectral2023","abstract":"We propose a novel transformer-based framework that reconstructs two high fidelity hands from multi-view RGB images. Unlike existing hand pose estimation methods, where one typically trains a deep network to regress hand model parameters from single RGB image, we consider a more challenging problem setting where we directly regress the absolute root poses of two-hands with extended forearm at high resolution from egocentric view. As existing datasets are either infeasible for egocentric viewpoints or lack background variations, we create a large-scale synthetic dataset with diverse scenarios and collect a real dataset from multi-calibrated camera setup to verify our proposed multi-view image feature fusion strategy. To make the reconstruction physically plausible, we propose two strategies: (i) a coarse-to-fine spectral graph convolution decoder to smoothen the meshes during upsampling and (ii) an optimisation-based refinement stage at inference to prevent self-penetrations. Through extensive quantitative and qualitative evaluations, we show that our framework is able to produce realistic two-hand reconstructions and demonstrate the generalisation of synthetic-trained models to real data, as well as real-time AR/VR applications.","accessed":{"date-parts":[["2023",9,25]]},"author":[{"family":"Tse","given":"Tze Ho Elden"},{"family":"Mueller","given":"Franziska"},{"family":"Shen","given":"Zhengyang"},{"family":"Tang","given":"Danhang"},{"family":"Beeler","given":"Thabo"},{"family":"Dou","given":"Mingsong"},{"family":"Zhang","given":"Yinda"},{"family":"Petrovic","given":"Sasa"},{"family":"Chang","given":"Hyung Jin"},{"family":"Taylor","given":"Jonathan"},{"family":"Doosti","given":"Bardia"}],"citation-key":"tseSpectralGraphormerSpectral2023","issued":{"date-parts":[["2023",8,21]]},"number":"arXiv:2308.11015","publisher":"arXiv","source":"arXiv.org","title":"Spectral Graphormer: Spectral Graph-based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images","title-short":"Spectral Graphormer","type":"article","URL":"http://arxiv.org/abs/2308.11015"},
  {"id":"tuConsistent3DHand2023","abstract":"We present a method for reconstructing accurate and consistent 3D hands from a monocular video. We observe that detected 2D hand keypoints and the image texture provide important cues about the geometry and texture of the 3D hand, which can reduce or even eliminate the requirement on 3D hand annotation. Thus we propose ${\\rm {S}^{2}HAND}$, a self-supervised 3D hand reconstruction model, that can jointly estimate pose, shape, texture, and the camera viewpoint from a single RGB input through the supervision of easily accessible 2D detected keypoints. We leverage the continuous hand motion information contained in the unlabeled video data and propose ${\\rm {S}^{2}HAND(V)}$, which uses a set of weights shared ${\\rm {S}^{2}HAND}$ to process each frame and exploits additional motion, texture, and shape consistency constrains to promote more accurate hand poses and more consistent shapes and textures. Experiments on benchmark datasets demonstrate that our self-supervised approach produces comparable hand reconstruction performance compared with the recent full-supervised methods in single-frame as input setup, and notably improves the reconstruction accuracy and consistency when using video training data.","accessed":{"date-parts":[["2023",9,23]]},"author":[{"family":"Tu","given":"Zhigang"},{"family":"Huang","given":"Zhisheng"},{"family":"Chen","given":"Yujin"},{"family":"Kang","given":"Di"},{"family":"Bao","given":"Linchao"},{"family":"Yang","given":"Bisheng"},{"family":"Yuan","given":"Junsong"}],"citation-key":"tuConsistent3DHand2023","container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","container-title-short":"IEEE Trans. Pattern Anal. Mach. Intell.","DOI":"10.1109/TPAMI.2023.3247907","ISSN":"0162-8828, 2160-9292, 1939-3539","issue":"8","issued":{"date-parts":[["2023",8]]},"page":"9469-9485","source":"arXiv.org","title":"Consistent 3D Hand Reconstruction in Video via self-supervised Learning","type":"article-journal","URL":"http://arxiv.org/abs/2201.09548","volume":"45"},
  {"id":"tuVoxelPoseMulticamera3D2020","abstract":"We present VoxelPose to estimate 3D poses of multiple people from multiple camera views. In contrast to the previous efforts which require to establish cross-view correspondence based on noisy and incomplete 2D pose estimates, VoxelPose directly operates in the 3D space therefore avoids making incorrect decisions in each camera view. To achieve this goal, features in all camera views are aggregated in the 3D voxel space and fed into Cuboid Proposal Network (CPN) to localize all people. Then we propose Pose Regression Network (PRN) to estimate a detailed 3D pose for each proposal. The approach is robust to occlusion which occurs frequently in practice. Without bells and whistles, it outperforms the previous methods on several public datasets.","author":[{"family":"Tu","given":"Hanyue"},{"family":"Wang","given":"Chunyu"},{"family":"Zeng","given":"Wenjun"}],"citation-key":"tuVoxelPoseMulticamera3D2020","collection-title":"Lecture Notes in Computer Science","container-title":"Computer Vision – ECCV 2020","DOI":"10.1007/978-3-030-58452-8_12","editor":[{"family":"Vedaldi","given":"Andrea"},{"family":"Bischof","given":"Horst"},{"family":"Brox","given":"Thomas"},{"family":"Frahm","given":"Jan-Michael"}],"event-place":"Cham","ISBN":"978-3-030-58452-8","issued":{"date-parts":[["2020"]]},"language":"en","page":"197-212","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"VoxelPose: Towards Multi-camera 3D Human Pose Estimation in Wild Environment","title-short":"VoxelPose","type":"paper-conference"},
  {"id":"usmanMetaPoseFast3D2022","accessed":{"date-parts":[["2022",11,23]]},"author":[{"family":"Usman","given":"Ben"},{"family":"Tagliasacchi","given":"Andrea"},{"family":"Saenko","given":"Kate"},{"family":"Sud","given":"Avneesh"}],"citation-key":"usmanMetaPoseFast3D2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"6759-6770","source":"openaccess.thecvf.com","title":"MetaPose: Fast 3D Pose From Multiple Views Without 3D Supervision","title-short":"MetaPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Usman_MetaPose_Fast_3D_Pose_From_Multiple_Views_Without_3D_Supervision_CVPR_2022_paper.html"},
  {"id":"usmanMetaPoseFast3D2022a","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Usman","given":"Ben"},{"family":"Tagliasacchi","given":"Andrea"},{"family":"Saenko","given":"Kate"},{"family":"Sud","given":"Avneesh"}],"citation-key":"usmanMetaPoseFast3D2022a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"6759-6770","source":"openaccess.thecvf.com","title":"MetaPose: Fast 3D Pose From Multiple Views Without 3D Supervision","title-short":"MetaPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Usman_MetaPose_Fast_3D_Pose_From_Multiple_Views_Without_3D_Supervision_CVPR_2022_paper.html"},
  {"id":"vendrowJRDBPoseLargeScaleDataset2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Vendrow","given":"Edward"},{"family":"Le","given":"Duy Tho"},{"family":"Cai","given":"Jianfei"},{"family":"Rezatofighi","given":"Hamid"}],"citation-key":"vendrowJRDBPoseLargeScaleDataset2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"4811-4820","source":"openaccess.thecvf.com","title":"JRDB-Pose: A Large-Scale Dataset for Multi-Person Pose Estimation and Tracking","title-short":"JRDB-Pose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Vendrow_JRDB-Pose_A_Large-Scale_Dataset_for_Multi-Person_Pose_Estimation_and_Tracking_CVPR_2023_paper.html"},
  {"id":"wandtCanonPoseSelfSupervisedMonocular2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Wandt","given":"Bastian"},{"family":"Rudolph","given":"Marco"},{"family":"Zell","given":"Petrissa"},{"family":"Rhodin","given":"Helge"},{"family":"Rosenhahn","given":"Bodo"}],"citation-key":"wandtCanonPoseSelfSupervisedMonocular2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"13294-13304","source":"openaccess.thecvf.com","title":"CanonPose: Self-Supervised Monocular 3D Human Pose Estimation in the Wild","title-short":"CanonPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Wandt_CanonPose_Self-Supervised_Monocular_3D_Human_Pose_Estimation_in_the_Wild_CVPR_2021_paper.html"},
  {"id":"wandtElePoseUnsupervised3D2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Wandt","given":"Bastian"},{"family":"Little","given":"James J."},{"family":"Rhodin","given":"Helge"}],"citation-key":"wandtElePoseUnsupervised3D2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"6635-6645","source":"openaccess.thecvf.com","title":"ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses","title-short":"ElePose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Wandt_ElePose_Unsupervised_3D_Human_Pose_Estimation_by_Predicting_Camera_Elevation_CVPR_2022_paper.html"},
  {"id":"wandtElePoseUnsupervised3D2022a","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Wandt","given":"Bastian"},{"family":"Little","given":"James J."},{"family":"Rhodin","given":"Helge"}],"citation-key":"wandtElePoseUnsupervised3D2022a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"6635-6645","source":"openaccess.thecvf.com","title":"ElePose: Unsupervised 3D Human Pose Estimation by Predicting Camera Elevation and Learning Normalizing Flows on 2D Poses","title-short":"ElePose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Wandt_ElePose_Unsupervised_3D_Human_Pose_Estimation_by_Predicting_Camera_Elevation_CVPR_2022_paper.html"},
  {"id":"wandtRepNetWeaklySupervised2019","abstract":"This paper addresses the problem of 3D human pose estimation from single images. While for a long time human skeletons were parameterized and fitted to the observation by satisfying a reprojection error, nowadays researchers directly use neural networks to infer the 3D pose from the observations. However, most of these approaches ignore the fact that a reprojection constraint has to be satisfied and are sensitive to overfitting. We tackle the overfitting problem by ignoring 2D to 3D correspondences. This efficiently avoids a simple memorization of the training data and allows for a weakly supervised training. One part of the proposed reprojection network (RepNet) learns a mapping from a distribution of 2D poses to a distribution of 3D poses using an adversarial training approach. Another part of the network estimates the camera. This allows for the definition of a network layer that performs the reprojection of the estimated 3D pose back to 2D which results in a reprojection loss function. Our experiments show that RepNet generalizes well to unknown data and outperforms state-of-the-art methods when applied to unseen data. Moreover, our implementation runs in real-time on a standard desktop PC.","accessed":{"date-parts":[["2022",6,22]]},"author":[{"family":"Wandt","given":"Bastian"},{"family":"Rosenhahn","given":"Bodo"}],"citation-key":"wandtRepNetWeaklySupervised2019","DOI":"10.48550/arXiv.1902.09868","issued":{"date-parts":[["2019",3,12]]},"number":"arXiv:1902.09868","publisher":"arXiv","source":"arXiv.org","title":"RepNet: Weakly Supervised Training of an Adversarial Reprojection Network for 3D Human Pose Estimation","title-short":"RepNet","type":"article","URL":"http://arxiv.org/abs/1902.09868"},
  {"id":"wanEncoderDecoderMultiLevelAttention2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Wan","given":"Ziniu"},{"family":"Li","given":"Zhengjia"},{"family":"Tian","given":"Maoqing"},{"family":"Liu","given":"Jianbo"},{"family":"Yi","given":"Shuai"},{"family":"Li","given":"Hongsheng"}],"citation-key":"wanEncoderDecoderMultiLevelAttention2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"13033-13042","source":"openaccess.thecvf.com","title":"Encoder-Decoder With Multi-Level Attention for 3D Human Shape and Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Wan_Encoder-Decoder_With_Multi-Level_Attention_for_3D_Human_Shape_and_Pose_ICCV_2021_paper.html"},
  {"id":"wangAdaptiveAppearanceLearning2015","abstract":"We address the problem of pose estimation in videos. The part detectors play important roles, but traditional template-based detectors (e.g. Histogram of Gradient, HoG) fail at pose estimation due to the high variability in appearance. We present an adaptive representation of appearance and shape for articulated human body. The full representation of human body is based on the ﬂexible mixture-of-parts model. We train a Naive Bayes classiﬁer to obtain a conﬁdence score of estimated pose by the basic mixture model, and based on the conﬁdence we learn an instance-speciﬁc appearance model. For between-frame consistency, we design a time-efﬁcient energy function for motion cues instead of complex motion models. We incorporate these models into a framework that allows for efﬁcient inference. Quantitative evaluation of pose estimation conducted on two video datasets demonstrates the effectiveness of the proposed method.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Lei"},{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"wangAdaptiveAppearanceLearning2015","container-title":"2015 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2015.7350975","event-place":"Quebec City, QC, Canada","event-title":"2015 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-4799-8339-1","issued":{"date-parts":[["2015",9]]},"language":"en","note":"ZSCC:00002","page":"1125-1129","publisher":"IEEE","publisher-place":"Quebec City, QC, Canada","source":"DOI.org (Crossref)","title":"Adaptive appearance learning for human pose estimation","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7350975/"},
  {"id":"wangAECNetAttentionEdge2020","abstract":"Semantic segmentation is a fundamental and challenging problem in medical image analysis. At present, deep convolutional neural network plays a dominant role in medical image segmentation. The existing problems of this ﬁeld are making less use of image information and learning few edge features, which may lead to the ambiguous boundary and inhomogeneous intensity distribution of the result. Since the characteristics of different stages are highly inconsistent, these two cannot be directly combined. In this paper, we proposed the Attention and Edge Constraint Network (AEC-Net) to optimize features by introducing attention mechanisms in the lower-level features, so that it can be better combined with higher-level features. Meanwhile, an edge branch is added to the network which can learn edge and texture features simultaneously. We evaluated this model on three datasets, including skin cancer segmentation, vessel segmentation, and lung segmentation. Results demonstrate that the proposed model has achieved state-of-the-art performance on all datasets.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Jingyi"},{"family":"Zhao","given":"Xu"},{"family":"Ning","given":"Qingtian"},{"family":"Qian","given":"Dahong"}],"citation-key":"wangAECNetAttentionEdge2020","container-title":"2020 42nd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)","DOI":"10.1109/EMBC44109.2020.9176670","event-place":"Montreal, QC, Canada","event-title":"2020 42nd Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) in conjunction with the 43rd Annual Conference of the Canadian Medical and Biological Engineering Society","ISBN":"978-1-7281-1990-8","issued":{"date-parts":[["2020",7]]},"language":"en","note":"ZSCC:00008","page":"1616-1619","publisher":"IEEE","publisher-place":"Montreal, QC, Canada","source":"DOI.org (Crossref)","title":"AEC-Net: Attention and Edge Constraint Network for Medical Image Segmentation","title-short":"AEC-Net","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9176670/"},
  {"id":"wangArticulated3DHuman2010","abstract":"We propose a new Particle Filter (PF) based Particle Swarm Optimization (PSO) algorithm for 3D articulated human pose estimation. The sampling covariance and annealing factor items are incorporated into the velocity updating equation of PSO, which are initiated with appropriate values at the beginning of PSO iteration, and decreasing (`annealed') by reasonable steps. The new algorithm can, in some degree, mitigate the not sufficiently reliable image likelihood problem. Experimental results on HumanEvaI data set show that compared with annealed particle filter and standard particle filter, the proposed algorithm can achieve lower estimation errors in tracking real-world 3D human motion.","author":[{"family":"Wang","given":"Xiangyang"},{"family":"Zou","given":"Xiang"},{"family":"Wan","given":"Wanggen"},{"family":"Yu","given":"Xiaoqing"}],"citation-key":"wangArticulated3DHuman2010","container-title":"2010 International Conference on Audio, Language and Image Processing","DOI":"10.1109/ICALIP.2010.5685102","event-title":"2010 International Conference on Audio, Language and Image Processing","issued":{"date-parts":[["2010",11]]},"page":"1094-1099","source":"IEEE Xplore","title":"Articulated 3D human pose estimation with Particle Filter based Particle Swarm Optimization","type":"paper-conference"},
  {"id":"wangBestBothWorlds2022","abstract":"Nonparametric based methods have recently shown promising results in reconstructing human bodies from monocular images while model-based methods can help correct these estimates and improve prediction. However, estimating model parameters from global image features may lead to noticeable misalignment between the estimated meshes and image evidence. To address this issue and leverage the best of both worlds, we propose a framework of three consecutive modules. A dense map prediction module explicitly establishes the dense UV correspondence between the image evidence and each part of the body model. The inverse kinematics module refines the key point prediction and generates a posed template mesh. Finally, a UV inpainting module relies on the corresponding feature, prediction and the posed template, and completes the predictions of occluded body shape. Our framework leverages the best of non-parametric and model-based methods and is also robust to partial occlusion. Experiments demonstrate that our framework outperforms existing 3D human estimation methods on multiple public benchmarks.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Wang","given":"Zhe"},{"family":"Yang","given":"Jimei"},{"family":"Fowlkes","given":"Charless"}],"citation-key":"wangBestBothWorlds2022","container-title":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","DOI":"10.1109/CVPRW56347.2022.00258","event-place":"New Orleans, LA, USA","event-title":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW)","ISBN":"978-1-6654-8739-9","issued":{"date-parts":[["2022",6]]},"language":"en","page":"2317-2326","publisher":"IEEE","publisher-place":"New Orleans, LA, USA","source":"DOI.org (Crossref)","title":"The Best of Both Worlds: Combining Model-based and Nonparametric Approaches for 3D Human Body Estimation","title-short":"The Best of Both Worlds","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9857136/"},
  {"id":"wangClothedHumanPerformance2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Wang","given":"Kangkan"},{"family":"Zhang","given":"Guofeng"},{"family":"Cong","given":"Suxu"},{"family":"Yang","given":"Jian"}],"citation-key":"wangClothedHumanPerformance2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"21098-21107","source":"openaccess.thecvf.com","title":"Clothed Human Performance Capture With a Double-Layer Neural Radiance Fields","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Clothed_Human_Performance_Capture_With_a_Double-Layer_Neural_Radiance_Fields_CVPR_2023_paper.html"},
  {"id":"wangCombiningDetectionTracking2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Wang","given":"Manchen"},{"family":"Tighe","given":"Joseph"},{"family":"Modolo","given":"Davide"}],"citation-key":"wangCombiningDetectionTracking2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"11088-11096","source":"openaccess.thecvf.com","title":"Combining Detection and Tracking for Human Pose Estimation in Videos","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Combining_Detection_and_Tracking_for_Human_Pose_Estimation_in_Videos_CVPR_2020_paper.html"},
  {"id":"wangComplete3DHuman2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Wang","given":"Junying"},{"family":"Yoon","given":"Jae Shin"},{"family":"Wang","given":"Tuanfeng Y."},{"family":"Singh","given":"Krishna Kumar"},{"family":"Neumann","given":"Ulrich"}],"citation-key":"wangComplete3DHuman2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8748-8758","source":"openaccess.thecvf.com","title":"Complete 3D Human Reconstruction From a Single Incomplete Image","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Complete_3D_Human_Reconstruction_From_a_Single_Incomplete_Image_CVPR_2023_paper.html"},
  {"id":"wangContextualInstanceDecoupling2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Wang","given":"Dongkai"},{"family":"Zhang","given":"Shiliang"}],"citation-key":"wangContextualInstanceDecoupling2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"11060-11068","source":"openaccess.thecvf.com","title":"Contextual Instance Decoupling for Robust Multi-Person Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Contextual_Instance_Decoupling_for_Robust_Multi-Person_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"wangContextualInstanceDecoupling2022a","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Wang","given":"Dongkai"},{"family":"Zhang","given":"Shiliang"}],"citation-key":"wangContextualInstanceDecoupling2022a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"11060-11068","source":"openaccess.thecvf.com","title":"Contextual Instance Decoupling for Robust Multi-Person Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Contextual_Instance_Decoupling_for_Robust_Multi-Person_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"wangDeepHighResolutionRepresentation2021","accessed":{"date-parts":[["2023",10,24]]},"author":[{"family":"Wang","given":"Jingdong"},{"family":"Sun","given":"Ke"},{"family":"Cheng","given":"Tianheng"},{"family":"Jiang","given":"Borui"},{"family":"Deng","given":"Chaorui"},{"family":"Zhao","given":"Yang"},{"family":"Liu","given":"Dong"},{"family":"Mu","given":"Yadong"},{"family":"Tan","given":"Mingkui"},{"family":"Wang","given":"Xinggang"},{"family":"Liu","given":"Wenyu"},{"family":"Xiao","given":"Bin"}],"citation-key":"wangDeepHighResolutionRepresentation2021","container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","container-title-short":"IEEE Trans. Pattern Anal. Mach. Intell.","DOI":"10.1109/TPAMI.2020.2983686","ISSN":"0162-8828, 2160-9292, 1939-3539","issue":"10","issued":{"date-parts":[["2021",10,1]]},"language":"en","page":"3349-3364","source":"DOI.org (Crossref)","title":"Deep High-Resolution Representation Learning for Visual Recognition","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9052469/","volume":"43"},
  {"id":"wangDetectingMotionPatterns2011","abstract":"Detecting motion pattern in dynamic crowd scenes is a challenging problem in computer vision ﬁeld. In this paper, we propose a novel approach to detect the motion patterns from global perspective. To extract the discriminative spatial-temporal features, we introduce the Motion History Image (MHI) into the optical ﬂow algorithm. Motion patterns are then detected by automatic clustering of optical ﬂow vectors through hierarchical clustering. Experiment evaluation on some challenging videos shows reliable detection results and demonstrates the effectiveness of our proposed approach.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Chongjing"},{"family":"Zhao","given":"Xu"},{"family":"Zou","given":"Yi"},{"family":"Liu","given":"Yuncai"}],"citation-key":"wangDetectingMotionPatterns2011","container-title":"2011 Sixth International Conference on Image and Graphics","DOI":"10.1109/ICIG.2011.114","event-place":"Hefei, Anhui, China","event-title":"Graphics (ICIG)","ISBN":"978-1-4577-1560-0","issued":{"date-parts":[["2011",8]]},"language":"en","note":"ZSCC:00005","page":"434-439","publisher":"IEEE","publisher-place":"Hefei, Anhui, China","source":"DOI.org (Crossref)","title":"Detecting Motion Patterns in Dynamic Crowd Scenes","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/6005839/"},
  {"id":"wangDiscriminativeTrackletsRepresentation2015","abstract":"In this work, we propose a discriminative tracklets representation for motion pattern extraction from crowded scene. The representation incorporates relative position, velocity, and direction information of tracklet into one compact form by shaping it within a rectangle. We adopt deep belief networks to extract low-dimensional features from this representation. It not only reduces the computational complexity for the following clustering, but also achieves more discriminative tracklets representation which is invariant to noises brought by tracking failures. To determine the spatio-temporal distribution of each motion pattern, a robust clustering scheme composed of three clustering procedures is proposed. Comprehensive experiments in multiple datasets validate the effectiveness of our approach.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Chongjing"},{"family":"Zhao","given":"Xu"},{"family":"Shou","given":"Zheng"},{"family":"Zhou","given":"Yi"},{"family":"Liu","given":"Yuncai"}],"citation-key":"wangDiscriminativeTrackletsRepresentation2015","container-title":"2015 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2015.7351112","event-place":"Quebec City, QC, Canada","event-title":"2015 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-4799-8339-1","issued":{"date-parts":[["2015",9]]},"language":"en","note":"ZSCC:00003","page":"1805-1809","publisher":"IEEE","publisher-place":"Quebec City, QC, Canada","source":"DOI.org (Crossref)","title":"A discriminative tracklets representation for crowd analysis","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7351112/"},
  {"id":"wangDistributionAwareSingleStageModels2022","accessed":{"date-parts":[["2022",11,22]]},"author":[{"family":"Wang","given":"Zitian"},{"family":"Nie","given":"Xuecheng"},{"family":"Qu","given":"Xiaochao"},{"family":"Chen","given":"Yunpeng"},{"family":"Liu","given":"Si"}],"citation-key":"wangDistributionAwareSingleStageModels2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13096-13105","source":"openaccess.thecvf.com","title":"Distribution-Aware Single-Stage Models for Multi-Person 3D Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Distribution-Aware_Single-Stage_Models_for_Multi-Person_3D_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"wangEstimatingEgocentric3D2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Wang","given":"Jian"},{"family":"Liu","given":"Lingjie"},{"family":"Xu","given":"Weipeng"},{"family":"Sarkar","given":"Kripasindhu"},{"family":"Theobalt","given":"Christian"}],"citation-key":"wangEstimatingEgocentric3D2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11500-11509","source":"openaccess.thecvf.com","title":"Estimating Egocentric 3D Human Pose in Global Space","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Wang_Estimating_Egocentric_3D_Human_Pose_in_Global_Space_ICCV_2021_paper.html"},
  {"id":"wangEstimatingEgocentric3D2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Wang","given":"Jian"},{"family":"Liu","given":"Lingjie"},{"family":"Xu","given":"Weipeng"},{"family":"Sarkar","given":"Kripasindhu"},{"family":"Luvizon","given":"Diogo"},{"family":"Theobalt","given":"Christian"}],"citation-key":"wangEstimatingEgocentric3D2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13157-13166","source":"openaccess.thecvf.com","title":"Estimating Egocentric 3D Human Pose in the Wild With External Weak Supervision","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Estimating_Egocentric_3D_Human_Pose_in_the_Wild_With_External_CVPR_2022_paper.html"},
  {"id":"wangGeometricPoseAffordance2021","abstract":"Accurate estimation of 3D human pose from a single image remains a challenging task despite many recent advances. In this paper, we explore the hypothesis that strong prior information about scene geometry can be used to improve pose estimation accuracy. To tackle this question empirically, we have assembled a novel Geometric Pose Aﬀordance dataset, consisting of multi-view imagery of people interacting with a variety of rich 3D environments. We utilized a commercial motion capture system to collect gold-standard estimates of pose and construct accurate geometric 3D models of the scene geometry. To inject prior knowledge of scene constraints into existing frameworks for pose estimation from images, we introduce a view-based representation of scene geometry, a multi-layer depth map, which employs multi-hit ray tracing to concisely encode multiple surface entry and exit points along each camera view ray direction. We propose two diﬀerent mechanisms for integrating multi-layer depth information into pose estimation: input as encoded ray features used in lifting 2D pose to full 3D, and secondly as a diﬀerentiable loss that encourages learned models to favor geometrically consistent pose estimates. We show experimentally that these techniques can improve the accuracy of 3D pose estimates, particularly in the presence of occlusion and complex scene geometry.","accessed":{"date-parts":[["2023",10,17]]},"author":[{"family":"Wang","given":"Zhe"},{"family":"Chen","given":"Liyan"},{"family":"Rathore","given":"Shaurya"},{"family":"Shin","given":"Daeyun"},{"family":"Fowlkes","given":"Charless"}],"citation-key":"wangGeometricPoseAffordance2021","issued":{"date-parts":[["2021",12,8]]},"language":"en","number":"arXiv:1905.07718","publisher":"arXiv","source":"arXiv.org","title":"Geometric Pose Affordance: 3D Human Pose with Scene Constraints","title-short":"Geometric Pose Affordance","type":"article","URL":"http://arxiv.org/abs/1905.07718"},
  {"id":"wangLitePoseEfficient2022","abstract":"Pose estimation plays a critical role in human-centered vision applications. However, it is difficult to deploy state-of-the-art HRNet-based pose estimation models on resource-constrained edge devices due to the high computational cost (more than 150 GMACs per frame). In this paper, we study efficient architecture design for real-time multi-person pose estimation on edge. We reveal that HRNet's high-resolution branches are redundant for models at the low-computation region via our gradual shrinking experiments. Removing them improves both efficiency and performance. Inspired by this finding, we design LitePose, an efficient single-branch architecture for pose estimation, and introduce two simple approaches to enhance the capacity of LitePose, including Fusion Deconv Head and Large Kernel Convs. Fusion Deconv Head removes the redundancy in high-resolution branches, allowing scale-aware feature fusion with low overhead. Large Kernel Convs significantly improve the model's capacity and receptive field while maintaining a low computational cost. With only 25% computation increment, 7x7 kernels achieve +14.0 mAP better than 3x3 kernels on the CrowdPose dataset. On mobile platforms, LitePose reduces the latency by up to 5.0x without sacrificing performance, compared with prior state-of-the-art efficient pose estimation models, pushing the frontier of real-time multi-person pose estimation on edge. Our code and pre-trained models are released at https://github.com/mit-han-lab/litepose.","accessed":{"date-parts":[["2022",11,29]]},"author":[{"family":"Wang","given":"Yihan"},{"family":"Li","given":"Muyang"},{"family":"Cai","given":"Han"},{"family":"Chen","given":"Wei-Ming"},{"family":"Han","given":"Song"}],"citation-key":"wangLitePoseEfficient2022","DOI":"10.48550/arXiv.2205.01271","issued":{"date-parts":[["2022",7,11]]},"number":"arXiv:2205.01271","publisher":"arXiv","source":"arXiv.org","title":"Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation","title-short":"Lite Pose","type":"article","URL":"http://arxiv.org/abs/2205.01271"},
  {"id":"wangLitePoseEfficient2022a","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Wang","given":"Yihan"},{"family":"Li","given":"Muyang"},{"family":"Cai","given":"Han"},{"family":"Chen","given":"Wei-Ming"},{"family":"Han","given":"Song"}],"citation-key":"wangLitePoseEfficient2022a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13126-13136","source":"openaccess.thecvf.com","title":"Lite Pose: Efficient Architecture Design for 2D Human Pose Estimation","title-short":"Lite Pose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Wang_Lite_Pose_Efficient_Architecture_Design_for_2D_Human_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"wangMotionPatternAnalysis2013","abstract":"Crowded scene analysis is becoming increasingly popular in computer vision field. In this paper, we propose a novel approach to analyze motion patterns by clustering the hybrid generative-discriminative feature maps using unsupervised hierarchical clustering algorithm. The hybrid generativediscriminative feature maps are derived by posterior divergence based on the tracklets which are captured by tracking dense points with three effective rules. The feature maps effectively associate low-level features with the semantical motion patterns by exploiting the hidden information in crowded scenes. Motion pattern analyzing is implemented in a completely unsupervised way and the feature maps are clustered automatically through hierarchical clustering algorithm building on the basis of graphic model. The experiment results precisely reveal the distributions of motion patterns in current crowded videos and demonstrate the effectiveness of our approach.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Chongjing"},{"family":"Zhao","given":"Xu"},{"family":"Wu","given":"Zhe"},{"family":"Liu","given":"Yuncai"}],"citation-key":"wangMotionPatternAnalysis2013","container-title":"2013 IEEE International Conference on Image Processing","DOI":"10.1109/ICIP.2013.6738584","event-place":"Melbourne, Australia","event-title":"2013 20th IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-4799-2341-0","issued":{"date-parts":[["2013",9]]},"language":"en","note":"ZSCC:00020","page":"2837-2841","publisher":"IEEE","publisher-place":"Melbourne, Australia","source":"DOI.org (Crossref)","title":"Motion pattern analysis in crowded scenes based on hybrid generative-discriminative feature maps","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/6738584/"},
  {"id":"wangMrCANClassAwareNetwork2022","abstract":"Recognizing action patterns and exploring multiple relations are vital for Temporal Action Detection (TAD) task, which aims at locating and classifying action segments in untrimmed videos. However, most existing methods attempt to build a general model to handle diverse actions, ignoring the huge difference between various classes. Besides, the exploration of temporal and semantic relations between different segments remains an ongoing challenge due to complex video content. In this paper, we contend that different action classes should be processed differently and thus design a new Class-Aware Mechanism to achieve accurate detection. Moreover, an effective module named Multi-relations Builder is proposed to establish temporal and semantic relations simultaneously. These two modules are integrated as Class-Aware Network with Multi-relations (MrCAN). In comprehensive experiments conducted on two benchmarks, it outperforms all other current methods and achieves state-of-the-art performance, improving the average mAP from 45.78% to 48.98% on THUMOS-14 and from 35.52% to 35.87% on ActivityNet-1.3 respectively. Furthermore, the well-designed Multi-relations Builder can also be used to boost some other existing methods.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Dongqi"},{"family":"Zhao","given":"Zixuan"},{"family":"Zhao","given":"Xu"}],"citation-key":"wangMrCANClassAwareNetwork2022","container-title":"2022 IEEE International Conference on Multimedia and Expo (ICME)","DOI":"10.1109/ICME52920.2022.9860007","event-place":"Taipei, Taiwan","event-title":"2022 IEEE International Conference on Multimedia and Expo (ICME)","ISBN":"978-1-6654-8563-0","issued":{"date-parts":[["2022",7,18]]},"language":"en","note":"ZSCC:00000","page":"1-6","publisher":"IEEE","publisher-place":"Taipei, Taiwan","source":"DOI.org (Crossref)","title":"Mr.CAN: Class-Aware Network with Multi-Relations for Temporal Action Detection","title-short":"Mr.CAN","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9860007/"},
  {"id":"wangNotAllParts2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Wang","given":"Jue"},{"family":"Huang","given":"Shaoli"},{"family":"Wang","given":"Xinchao"},{"family":"Tao","given":"Dacheng"}],"citation-key":"wangNotAllParts2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"7771-7780","source":"openaccess.thecvf.com","title":"Not All Parts Are Created Equal: 3D Pose Estimation by Modeling Bi-Directional Dependencies of Body Parts","title-short":"Not All Parts Are Created Equal","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Wang_Not_All_Parts_Are_Created_Equal_3D_Pose_Estimation_by_ICCV_2019_paper.html"},
  {"id":"wangPETNeuSPositionalEncoding2023","abstract":"A signed distance function (SDF) parametrized by an MLP is a common ingredient of neural surface reconstruction. We build on the successful recent method NeuS to extend it by three new components. The first component is to borrow the tri-plane representation from EG3D and represent signed distance fields as a mixture of tri-planes and MLPs instead of representing it with MLPs only. Using triplanes leads to a more expressive data structure but will also introduce noise in the reconstructed surface. The second component is to use a new type of positional encoding with learnable weights to combat noise in the reconstruction process. We divide the features in the tri-plane into multiple frequency scales and modulate them with sin and cos functions of different frequencies. The third component is to use learnable convolution operations on the tri-plane features using self-attention convolution to produce features with different frequency bands. The experiments show that PET-NeuS achieves high-fidelity surface reconstruction on standard datasets. Following previous work and using the Chamfer metric as the most important way to measure surface reconstruction quality, we are able to improve upon the NeuS baseline by 57% on Nerf-synthetic (0.84 compared to 1.97) and by 15.5% on DTU (0.71 compared to 0.84). The qualitative evaluation reveals how our method can better control the interference of high-frequency noise.","accessed":{"date-parts":[["2023",10,23]]},"author":[{"family":"Wang","given":"Yiqun"},{"family":"Skorokhodov","given":"Ivan"},{"family":"Wonka","given":"Peter"}],"citation-key":"wangPETNeuSPositionalEncoding2023","container-title":"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR52729.2023.01212","event-place":"Vancouver, BC, Canada","event-title":"2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"979-8-3503-0129-8","issued":{"date-parts":[["2023",6]]},"language":"en","page":"12598-12607","publisher":"IEEE","publisher-place":"Vancouver, BC, Canada","source":"DOI.org (Crossref)","title":"PET-NeuS: Positional Encoding Tri-Planes for Neural Surfaces","title-short":"PET-NeuS","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10203381/"},
  {"id":"wangReduceFalsePositives2015","abstract":"In this work, we address the problem of reducing the false positives for human detection in videos. We employ the motion cue to build a foreground probability model. Then the mean expectation of the pixel-level foreground probability is computed to assign a priori probability to the sliding window in detection. We combine the response of Deformable Part Models and the mean probability expectation to form the features and train a linear classiﬁer. The proposed approach is threshold-free, and reduces the false positives in human detection by the foreground cues. As well, we describe an integral probability image for fast computation of the mean probability expectation. Experimental results show that the proposed method achieve superior performance over the baseline of Deformable Part Models.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Lei"},{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"wangReduceFalsePositives2015","container-title":"2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","DOI":"10.1109/ACPR.2015.7486570","event-place":"Kuala Lumpur, Malaysia","event-title":"2015 3rd IAPR Asian Conference on Pattern Recognition (ACPR)","ISBN":"978-1-4799-6100-9","issued":{"date-parts":[["2015",11]]},"language":"en","note":"ZSCC:00000","page":"584-588","publisher":"IEEE","publisher-place":"Kuala Lumpur, Malaysia","source":"DOI.org (Crossref)","title":"Reduce false positives for human detection by a priori probability in videos","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7486570/"},
  {"id":"wangReduceFalsePositives2016","abstract":"In this work, we address the problem of reducing the false positives for object detection in videos. We employ the motion cue to build a foreground probability model. Then the mean expectation of the pixellevel foreground probability is computed to assign a priori probability to the sliding window in detection. The proposed foreground model is evaluated with the detection framework of Deformable Part Models (DPM). We combine the response of DPM detector and the mean probability expectation to form the features and train a linear classiﬁer. The proposed approach is threshold-free, and reduces the false positives in object detection by the foreground cues. Besides, we describe an integral probability image for fast computation of the mean probability expectation. Experimental results show that the proposed method achieve superior performance over the baseline of Deformable Part Models.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Lei"},{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"wangReduceFalsePositives2016","container-title":"Neurocomputing","container-title-short":"Neurocomputing","DOI":"10.1016/j.neucom.2016.03.082","ISSN":"09252312","issued":{"date-parts":[["2016",10]]},"language":"en","note":"ZSCC:00003","page":"325-332","source":"DOI.org (Crossref)","title":"Reduce false positives for object detection by a priori probability in videos","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S092523121630474X","volume":"208"},
  {"id":"wangRobustDifferentiableSVD2022","abstract":"Eigendecomposition of symmetric matrices is at the heart of many computer vision algorithms. However, the derivatives of the eigenvectors tend to be numerically unstable, whether using the SVD to compute them analytically or using the Power Iteration (PI) method to approximate them. This instability arises in the presence of eigenvalues that are close to each other. This makes integrating eigendecomposition into deep networks difficult and often results in poor convergence, particularly when dealing with large matrices. While this can be mitigated by partitioning the data into small arbitrary groups, doing so has no theoretical basis and makes it impossible to exploit the full power of eigendecomposition. In previous work, we mitigated this using SVD during the forward pass and PI to compute the gradients during the backward pass. However, the iterative deflation procedure required to compute multiple eigenvectors using PI tends to accumulate errors and yield inaccurate gradients. Here, we show that the Taylor expansion of the SVD gradient is theoretically equivalent to the gradient obtained using PI without relying in practice on an iterative process and thus yields more accurate gradients. We demonstrate the benefits of this increased accuracy for image classification and style transfer.","author":[{"family":"Wang","given":"Wei"},{"family":"Dang","given":"Zheng"},{"family":"Hu","given":"Yinlin"},{"family":"Fua","given":"Pascal"},{"family":"Salzmann","given":"Mathieu"}],"citation-key":"wangRobustDifferentiableSVD2022","container-title":"IEEE Transactions on Pattern Analysis and Machine Intelligence","DOI":"10.1109/TPAMI.2021.3072422","ISSN":"1939-3539","issue":"9","issued":{"date-parts":[["2022",9]]},"page":"5472-5487","source":"IEEE Xplore","title":"Robust Differentiable SVD","type":"article-journal","volume":"44"},
  {"id":"wangSceneAwareEgocentric3D2023","accessed":{"date-parts":[["2023",6,10]]},"author":[{"family":"Wang","given":"Jian"},{"family":"Luvizon","given":"Diogo"},{"family":"Xu","given":"Weipeng"},{"family":"Liu","given":"Lingjie"},{"family":"Sarkar","given":"Kripasindhu"},{"family":"Theobalt","given":"Christian"}],"citation-key":"wangSceneAwareEgocentric3D2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"13031-13040","source":"openaccess.thecvf.com","title":"Scene-Aware Egocentric 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Wang_Scene-Aware_Egocentric_3D_Human_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"wangSequential3DHuman2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Wang","given":"Kangkan"},{"family":"Xie","given":"Jin"},{"family":"Zhang","given":"Guofeng"},{"family":"Liu","given":"Lei"},{"family":"Yang","given":"Jian"}],"citation-key":"wangSequential3DHuman2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"7275-7284","source":"openaccess.thecvf.com","title":"Sequential 3D Human Pose and Shape Estimation From Point Clouds","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Wang_Sequential_3D_Human_Pose_and_Shape_Estimation_From_Point_Clouds_CVPR_2020_paper.html"},
  {"id":"wangSkeletonFeatureFusion2018","abstract":"Human action recognition from skeleton sequences has attracted a lot of attention in the computer vision community. Long short term memory (LSTM) network has shown its promising performance for this problem, due to their strengths in modeling the dependencies and temporal dynamics of sequential data. However, original LSTM is difﬁcult to grasp the dynamics of entire sequence data, if the input feature of each time step is just a simple combination of raw skeleton data. In this paper, we present a fusion model to make full use of the skeleton data through multi-stream LSTM for action recognition. In each stream of the model, skeleton feature fed to each step of LSTM are extracted from different time duration, which are called single frame feature, short term feature, and long term feature, respectively. Single frame feature represents static pose, which is converted from joints coordinates directly. Short term feature represents skeleton kinematics, which is extracted from a short time window. Long term feature represents joints mutuality during the action process, which is extracted from a longer time window. All these features are modeled by LSTM, and the ﬁnal states of LSTM streams are fused to predict the underlying actions. The proposed model makes better use of the skeleton dynamics than standard LSTM model. Experimental results on two benchmark skeleton data sets NTU RGB+D data set and SBU interaction dataset show that our proposed approach achieved signiﬁcant performance.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Lei"},{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"wangSkeletonFeatureFusion2018","container-title":"IEEE Access","container-title-short":"IEEE Access","DOI":"10.1109/ACCESS.2018.2869751","ISSN":"2169-3536","issued":{"date-parts":[["2018"]]},"language":"en","note":"ZSCC:00016","page":"50788-50800","source":"DOI.org (Crossref)","title":"Skeleton Feature Fusion Based on Multi-Stream LSTM for Action Recognition","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/8463451/","volume":"6"},
  {"id":"wangSupportplaneEstimationFloor2014","abstract":"Plane ﬁtting plays an important role in image processing and computer vision. It is challenging because of the outliers that do not follow the plane pattern. In this work, we address the problem of support-plane ﬁtting for room ﬂoor detection from point clouds that are generated from depth image. Based on the geometric layout of data, an optimization problem is derived to estimate the support-plane. Algorithms are also proposed to deal with data noise. The ﬂoor detection is achieved by support-plane ﬁtting, and is employed as a reference to analyze the spatial organization of room scene. A projection method is presented to form the organization map. Experiments demonstrate the proposed method is more robust, and it achieves remarkable performance in understanding the spatial organization.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Lei"},{"family":"Zhou","given":"Zhimin"},{"family":"Wu","given":"Jun"},{"family":"Liu","given":"Yuncai"},{"family":"Zhao","given":"Xu"}],"citation-key":"wangSupportplaneEstimationFloor2014","container-title":"2014 IEEE International Conference on Robotics and Biomimetics (ROBIO 2014)","DOI":"10.1109/ROBIO.2014.7090729","event-place":"Bali, Indonesia","event-title":"2014 IEEE International Conference on Robotics and Biomimetics (ROBIO)","ISBN":"978-1-4799-7397-2","issued":{"date-parts":[["2014",12]]},"language":"en","note":"ZSCC:00004","page":"2576-2581","publisher":"IEEE","publisher-place":"Bali, Indonesia","source":"DOI.org (Crossref)","title":"Support-plane estimation for floor detection to understand regions' spatial organization","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7090729/"},
  {"id":"wangTemporalRegularizedSpatial2019","abstract":"Video-based person re-identiﬁcation aims at matching video sequences of a person across different camera views. How to explore the abundant appearance and motion information in a video sequence is crucial to tackle this problem. To this end, we ﬁrst introduce a parameter-free spatial attention module to emphasize the importance of discriminative regions. Then we apply a temporal regularization term on spatial attention to reﬁne corrupted region caused by occlusion and blur. This term allows the attention response at one position in a frame to be related to other frames of the same position. Extensive experiments are conducted on iLIDS-VID and PRID-2011 datasets. The experimental results demonstrate that our approach surpasses the existing state-of-the-art video-based person reidentiﬁcation methods on iLIDS-VID and PRID-2011.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Xueying"},{"family":"Zhao","given":"Xu"}],"citation-key":"wangTemporalRegularizedSpatial2019","container-title":"2019 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2019.8803321","event-place":"Taipei, Taiwan","event-title":"2019 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-5386-6249-6","issued":{"date-parts":[["2019",9]]},"language":"en","note":"ZSCC:00001","page":"2249-2253","publisher":"IEEE","publisher-place":"Taipei, Taiwan","source":"DOI.org (Crossref)","title":"Temporal Regularized Spatial Attention for Video-Based Person Re-Identification","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8803321/"},
  {"id":"wangUnsupervisedMotionPattern2012","abstract":"Crowded scenes analysis is a challenging topic in computer vision field. How to detect diverse motion patterns in crowded scenarios from videos is the critical yet hard part of this problem. In this paper, we propose a novel approach to mining motion patterns by utilizing motion information during both long-term period and short interval simultaneously. To capture long-term motions effectively, we introduce Motion History Image (MHI) representation to access to the global perspective about the crowd motion. The combination of MHI and optical flow, which is used to get instant motion information, gives rise to discriminative spatial-temporal motion features. Benefitting from the robustness and efficiency of the novel motion representation, the following motion pattern mining is implemented in a completely unsupervised way. The motion vectors are clustered hierarchically through automatic hierarchical clustering algorithm building on the basis of graphic model. This method overcomes the instability of optical flow in dealing with time continuity in crowded scenes. The results of clustering reveal the situations of motion pattern distribution in current crowded videos. To validate the performance of the proposed approach, we conduct experimental evaluations on some challenging videos including vehicles and pedestrians. The reliable detection results demonstrate the effectiveness of our approach.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wang","given":"Chongjing"}],"citation-key":"wangUnsupervisedMotionPattern2012","container-title":"KSII Transactions on Internet and Information Systems","container-title-short":"KSII TIIS","DOI":"10.3837/tiis.2012.12.016","ISSN":"19767277","issued":{"date-parts":[["2012"]]},"language":"en","note":"ZSCC:00001","source":"DOI.org (Crossref)","title":"Unsupervised Motion Pattern Mining for Crowded Scenes Analysis","type":"article-journal","URL":"http://www.itiis.org/digital-library/manuscript/461"},
  {"id":"wangWhenHumanPose2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Wang","given":"Jiahang"},{"family":"Jin","given":"Sheng"},{"family":"Liu","given":"Wentao"},{"family":"Liu","given":"Weizhong"},{"family":"Qian","given":"Chen"},{"family":"Luo","given":"Ping"}],"citation-key":"wangWhenHumanPose2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"11855-11864","source":"openaccess.thecvf.com","title":"When Human Pose Estimation Meets Robustness: Adversarial Algorithms and Benchmarks","title-short":"When Human Pose Estimation Meets Robustness","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Wang_When_Human_Pose_Estimation_Meets_Robustness_Adversarial_Algorithms_and_Benchmarks_CVPR_2021_paper.html"},
  {"id":"wanViewConsistencyAware2023","abstract":"The rapid development of multi-view 3D human pose estimation (HPE) is attributed to the maturation of monocular 2D HPE and the geometry of 3D reconstruction. However, 2D detection outliers in occluded views due to neglect of view consistency, and 3D implausible poses due to lack of pose coherence, remain challenges. To solve this, we introduce a Multi-View Fusion module to refine 2D results by establishing view correlations. Then, Holistic Triangulation is proposed to infer the whole pose as an entirety, and anatomy prior is injected to maintain the pose coherence and improve the plausibility. Anatomy prior is extracted by PCA whose input is skeletal structure features, which can factor out global context and joint-by-joint relationship from abstract to concrete. Benefiting from the closed-form solution, the whole framework is trained end-to-end. Our method outperforms the state of the art in both precision and plausibility which is assessed by a new metric.","author":[{"family":"Wan","given":"Xiaoyue"},{"family":"Chen","given":"Zhuo"},{"family":"Zhao","given":"Xu"}],"citation-key":"wanViewConsistencyAware2023","container-title":"Computer Vision and Image Understanding","DOI":"https://doi.org/10.1016/j.cviu.2023.103830","ISSN":"1077-3142","issued":{"date-parts":[["2023"]]},"page":"103830","title":"View consistency aware holistic triangulation for 3D human pose estimation","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1077314223002102","volume":"236"},
  {"id":"wehrbeinProbabilisticMonocular3D2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Wehrbein","given":"Tom"},{"family":"Rudolph","given":"Marco"},{"family":"Rosenhahn","given":"Bodo"},{"family":"Wandt","given":"Bastian"}],"citation-key":"wehrbeinProbabilisticMonocular3D2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11199-11208","source":"openaccess.thecvf.com","title":"Probabilistic Monocular 3D Human Pose Estimation With Normalizing Flows","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Wehrbein_Probabilistic_Monocular_3D_Human_Pose_Estimation_With_Normalizing_Flows_ICCV_2021_paper.html"},
  {"id":"weiCapturingHumansMotion2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Wei","given":"Wen-Li"},{"family":"Lin","given":"Jen-Chun"},{"family":"Liu","given":"Tyng-Luh"},{"family":"Liao","given":"Hong-Yuan Mark"}],"citation-key":"weiCapturingHumansMotion2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13211-13220","source":"openaccess.thecvf.com","title":"Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation From Monocular Video","title-short":"Capturing Humans in Motion","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Wei_Capturing_Humans_in_Motion_Temporal-Attentive_3D_Human_Pose_and_Shape_CVPR_2022_paper.html"},
  {"id":"weiCapturingHumansMotion2022a","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Wei","given":"Wen-Li"},{"family":"Lin","given":"Jen-Chun"},{"family":"Liu","given":"Tyng-Luh"},{"family":"Liao","given":"Hong-Yuan Mark"}],"citation-key":"weiCapturingHumansMotion2022a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13211-13220","source":"openaccess.thecvf.com","title":"Capturing Humans in Motion: Temporal-Attentive 3D Human Pose and Shape Estimation From Monocular Video","title-short":"Capturing Humans in Motion","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Wei_Capturing_Humans_in_Motion_Temporal-Attentive_3D_Human_Pose_and_Shape_CVPR_2022_paper.html"},
  {"id":"weiConvolutionalPoseMachines2016","accessed":{"date-parts":[["2022",11,21]]},"author":[{"family":"Wei","given":"Shih-En"},{"family":"Ramakrishna","given":"Varun"},{"family":"Kanade","given":"Takeo"},{"family":"Sheikh","given":"Yaser"}],"citation-key":"weiConvolutionalPoseMachines2016","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2016"]]},"page":"4724-4732","source":"openaccess.thecvf.com","title":"Convolutional Pose Machines","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2016/html/Wei_Convolutional_Pose_Machines_CVPR_2016_paper.html"},
  {"id":"weiMultipleBranchesFasterRCNN2017","abstract":"In this work, we primarily address multiple people pose estimation challenge by exploring the performance of Faster RCNN on human parts detection. We develop a multiple-branches Faster RCNN model for our speciﬁc task of detecting persons and their parts. Our model can improve the performance of detecting human parts and the whole persons, meanwhile speeding up detection process with shared weights. A part-based method is proposed to estimate multiple people poses, bringing recent advances on object detection to this task. Experiments demonstrate that our model achieves better performance than the original Faster RCNN model on our task. Compared with other pose estimation approaches, our approach achieves fair or better results.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wei","given":"Kaiqiang"},{"family":"Zhao","given":"Xu"}],"citation-key":"weiMultipleBranchesFasterRCNN2017","container-title":"Computer Vision – ACCV 2016 Workshops","DOI":"10.1007/978-3-319-54526-4_33","event-place":"Cham","ISBN":"978-3-319-54525-7 978-3-319-54526-4","issued":{"date-parts":[["2017"]]},"language":"en","note":"ZSCC:00005","page":"453-462","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"Multiple-Branches Faster RCNN for Human Parts Detection and Pose Estimation","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-319-54526-4_33","volume":"10118"},
  {"id":"weng3DHumanKeypoints2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Weng","given":"Zhenzhen"},{"family":"Gorban","given":"Alexander S."},{"family":"Ji","given":"Jingwei"},{"family":"Najibi","given":"Mahyar"},{"family":"Zhou","given":"Yin"},{"family":"Anguelov","given":"Dragomir"}],"citation-key":"weng3DHumanKeypoints2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"1158-1167","source":"openaccess.thecvf.com","title":"3D Human Keypoints Estimation From Point Clouds in the Wild Without Human Labels","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Weng_3D_Human_Keypoints_Estimation_From_Point_Clouds_in_the_Wild_CVPR_2023_paper.html"},
  {"id":"worrallInterpretableTransformationsEncoderDecoder2017","accessed":{"date-parts":[["2022",5,18]]},"author":[{"family":"Worrall","given":"Daniel E."},{"family":"Garbin","given":"Stephan J."},{"family":"Turmukhambetov","given":"Daniyar"},{"family":"Brostow","given":"Gabriel J."}],"citation-key":"worrallInterpretableTransformationsEncoderDecoder2017","event-title":"Proceedings of the IEEE International Conference on Computer Vision","issued":{"date-parts":[["2017"]]},"page":"5726-5735","source":"openaccess.thecvf.com","title":"Interpretable Transformations With Encoder-Decoder Networks","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_iccv_2017/html/Worrall_Interpretable_Transformations_With_ICCV_2017_paper.html"},
  {"id":"wu3DHumanPose2020","abstract":"In this work, we tackle the problem of estimating 3D human pose in camera space from a monocular image. First, we propose to use densely-generated limb depth maps to ease the learning of body joints depth, which are well aligned with image cues. Then, we design a lifting module from 2D pixel coordinates to 3D camera coordinates which explicitly takes the depth values as inputs, and is aligned with camera perspective projection model. We show our method achieves superior performance on large-scale 3D pose datasets Human3.6M and MPI-INF-3DHP, and sets the new state-of-the-art.","accessed":{"date-parts":[["2023",2,14]]},"author":[{"family":"Wu","given":"Haiping"},{"family":"Xiao","given":"Bin"}],"citation-key":"wu3DHumanPose2020","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","DOI":"10.1609/aaai.v34i07.6923","ISSN":"2374-3468","issue":"07","issued":{"date-parts":[["2020",4,3]]},"language":"en","license":"Copyright (c) 2020 Association for the Advancement of Artificial Intelligence","number":"07","page":"12378-12385","source":"ojs.aaai.org","title":"3D Human Pose Estimation via Explicit Compositional Depth Maps","type":"article-journal","URL":"https://ojs.aaai.org/index.php/AAAI/article/view/6923","volume":"34"},
  {"id":"wuGraphBased3DMultiPerson2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Wu","given":"Size"},{"family":"Jin","given":"Sheng"},{"family":"Liu","given":"Wentao"},{"family":"Bai","given":"Lei"},{"family":"Qian","given":"Chen"},{"family":"Liu","given":"Dong"},{"family":"Ouyang","given":"Wanli"}],"citation-key":"wuGraphBased3DMultiPerson2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11148-11157","source":"openaccess.thecvf.com","title":"Graph-Based 3D Multi-Person Pose Estimation Using Multi-View Images","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Wu_Graph-Based_3D_Multi-Person_Pose_Estimation_Using_Multi-View_Images_ICCV_2021_paper.html"},
  {"id":"wuHybridGenerativediscriminativeRecognition2012","abstract":"We propose a novel human action recognition method based on the generative feature mapping over 3D human body joint sequences. The proposed method relies on Hidden Markov Model (HMM), but diﬀers from the previous methods in the way of incorporating HMM and discriminative classiﬁer, aiming to capture more discriminative information. Firstly, we use HMMs to model the joint sequences of human body. Then the Posterior Divergence is used to build feature mappings from the trained HMMs. The derived feature mappings map a variable-length joint sequence to a ﬁxed-dimension feature vector which will be delivered to SVM for classiﬁcation. We evaluate the proposed method and related methods on a large number of 3D joint sequences. The experimental results show its competitive performance, in comparison with other state-of-the-art methods.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Wu","given":"Zhe"},{"family":"Li","given":"Xiong"},{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"wuHybridGenerativediscriminativeRecognition2012","container-title":"Proceedings of the 20th ACM international conference on Multimedia","DOI":"10.1145/2393347.2396388","event-place":"Nara Japan","event-title":"MM '12: ACM Multimedia Conference","ISBN":"978-1-4503-1089-5","issued":{"date-parts":[["2012",10,29]]},"language":"en","note":"ZSCC:00003","page":"1081-1084","publisher":"ACM","publisher-place":"Nara Japan","source":"DOI.org (Crossref)","title":"Hybrid generative-discriminative recognition of human action in 3D joint space","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/2393347.2396388"},
  {"id":"xiaoSimpleBaselinesHuman2018","accessed":{"date-parts":[["2021",1,18]]},"author":[{"family":"Xiao","given":"Bin"},{"family":"Wu","given":"Haiping"},{"family":"Wei","given":"Yichen"}],"citation-key":"xiaoSimpleBaselinesHuman2018","event-title":"Proceedings of the European Conference on Computer Vision (ECCV)","issued":{"date-parts":[["2018"]]},"page":"466-481","source":"openaccess.thecvf.com","title":"Simple Baselines for Human Pose Estimation and Tracking","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ECCV_2018/html/Bin_Xiao_Simple_Baselines_for_ECCV_2018_paper.html"},
  {"id":"xieEmpiricalStudyCollapsing2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Xie","given":"Rongchang"},{"family":"Wang","given":"Chunyu"},{"family":"Zeng","given":"Wenjun"},{"family":"Wang","given":"Yizhou"}],"citation-key":"xieEmpiricalStudyCollapsing2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11240-11249","source":"openaccess.thecvf.com","title":"An Empirical Study of the Collapsing Problem in Semi-Supervised 2D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Xie_An_Empirical_Study_of_the_Collapsing_Problem_in_Semi-Supervised_2D_ICCV_2021_paper.html"},
  {"id":"xieMetaFusePretrainedFusion2020","accessed":{"date-parts":[["2021",11,1]]},"author":[{"family":"Xie","given":"Rongchang"},{"family":"Wang","given":"Chunyu"},{"family":"Wang","given":"Yizhou"}],"citation-key":"xieMetaFusePretrainedFusion2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"13686-13695","source":"openaccess.thecvf.com","title":"MetaFuse: A Pre-trained Fusion Model for Human Pose Estimation","title-short":"MetaFuse","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Xie_MetaFuse_A_Pre-trained_Fusion_Model_for_Human_Pose_Estimation_CVPR_2020_paper.html"},
  {"id":"xiongA2JAnchorJointRegression2019","abstract":"For 3D hand and body pose estimation task in depth image, a novel anchor-based approach termed Anchor-toJoint regression network (A2J) with the end-to-end learning ability is proposed. Within A2J, anchor points able to capture global-local spatial context information are densely set on depth image as local regressors for the joints. They contribute to predict the positions of the joints in ensemble way to enhance generalization ability. The proposed 3D articulated pose estimation paradigm is different from the state-of-the-art encoder-decoder based FCN, 3D CNN and point-set based manners. To discover informative anchor points towards certain joint, anchor proposal procedure is also proposed for A2J. Meanwhile 2D CNN (i.e., ResNet50) is used as backbone network to drive A2J, without using time-consuming 3D convolutional or deconvolutional layers. The experiments on 3 hand datasets and 2 body datasets verify A2J’s superiority. Meanwhile, A2J is of high running speed around 100 FPS on single NVIDIA 1080Ti GPU.","accessed":{"date-parts":[["2023",12,9]]},"author":[{"family":"Xiong","given":"Fu"},{"family":"Zhang","given":"Boshen"},{"family":"Xiao","given":"Yang"},{"family":"Cao","given":"Zhiguo"},{"family":"Yu","given":"Taidong"},{"family":"Zhou","given":"Joey Tianyi"},{"family":"Yuan","given":"Junsong"}],"citation-key":"xiongA2JAnchorJointRegression2019","container-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV.2019.00088","event-place":"Seoul, Korea (South)","event-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","ISBN":"978-1-7281-4803-8","issued":{"date-parts":[["2019",10]]},"language":"en","page":"793-802","publisher":"IEEE","publisher-place":"Seoul, Korea (South)","source":"DOI.org (Crossref)","title":"A2J: Anchor-to-Joint Regression Network for 3D Articulated Pose Estimation From a Single Depth Image","title-short":"A2J","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9010871/"},
  {"id":"xuD3DHOIDynamic3D2021","abstract":"We introduce D3D-HOI: a dataset of monocular videos with ground truth annotations of 3D object pose, shape and part motion during human-object interactions. Our dataset consists of several common articulated objects captured from diverse real-world scenes and camera viewpoints. Each manipulated object (e.g., microwave oven) is represented with a matching 3D parametric model. This data allows us to evaluate the reconstruction quality of articulated objects and establish a benchmark for this challenging task. In particular, we leverage the estimated 3D human pose for more accurate inference of the object spatial layout and dynamics. We evaluate this approach on our dataset, demonstrating that human-object relations can significantly reduce the ambiguity of articulated object reconstructions from challenging real-world videos. Code and dataset are available at https://github.com/facebookresearch/d3d-hoi.","accessed":{"date-parts":[["2023",8,28]]},"author":[{"family":"Xu","given":"Xiang"},{"family":"Joo","given":"Hanbyul"},{"family":"Mori","given":"Greg"},{"family":"Savva","given":"Manolis"}],"citation-key":"xuD3DHOIDynamic3D2021","DOI":"10.48550/arXiv.2108.08420","issued":{"date-parts":[["2021",8,18]]},"number":"arXiv:2108.08420","publisher":"arXiv","source":"arXiv.org","title":"D3D-HOI: Dynamic 3D Human-Object Interactions from Videos","title-short":"D3D-HOI","type":"article","URL":"http://arxiv.org/abs/2108.08420"},
  {"id":"xuDeepKinematicsAnalysis2020","abstract":"For monocular 3D pose estimation conditioned on 2D detection, noisy/unreliable input is a key obstacle in this task. Simple structure constraints attempting to tackle this problem, e.g., symmetry loss and joint angle limit, could only provide marginal improvements and are commonly treated as auxiliary losses in previous researches. Thus it still remains challenging about how to effectively utilize the power of human prior knowledge for this task. In this paper, we propose to address above issue in a systematic view. Firstly, we show that optimizing the kinematics structure of noisy 2D inputs is critical to obtain accurate 3D estimations. Secondly, based on corrected 2D joints, we further explicitly decompose articulated motion with human topology, which leads to more compact 3D static structure easier for estimation. Finally, temporal refinement emphasizing the validity of 3D dynamic structure is naturally developed to pursue more accurate result. Above three steps are seamlessly integrated into deep neural models, which form a deep kinematics analysis pipeline concurrently considering the static/dynamic structure of 2D inputs and 3D outputs. Extensive experiments show that proposed framework achieves state-of-the-art performance on two widely used 3D human action datasets. Meanwhile, targeted ablation study shows that each former step is critical for the latter one to obtain promising results.","author":[{"family":"Xu","given":"J."},{"family":"Yu","given":"Z."},{"family":"Ni","given":"B."},{"family":"Yang","given":"J."},{"family":"Yang","given":"X."},{"family":"Zhang","given":"W."}],"citation-key":"xuDeepKinematicsAnalysis2020","container-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR42600.2020.00098","event-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISSN":"2575-7075","issued":{"date-parts":[["2020",6]]},"page":"896-905","source":"IEEE Xplore","title":"Deep Kinematics Analysis for Monocular 3D Human Pose Estimation","type":"paper-conference"},
  {"id":"xuDeepKinematicsAnalysis2020a","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Xu","given":"Jingwei"},{"family":"Yu","given":"Zhenbo"},{"family":"Ni","given":"Bingbing"},{"family":"Yang","given":"Jiancheng"},{"family":"Yang","given":"Xiaokang"},{"family":"Zhang","given":"Wenjun"}],"citation-key":"xuDeepKinematicsAnalysis2020a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"899-908","source":"openaccess.thecvf.com","title":"Deep Kinematics Analysis for Monocular 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Xu_Deep_Kinematics_Analysis_for_Monocular_3D_Human_Pose_Estimation_CVPR_2020_paper.html"},
  {"id":"xuDenseRaCJoint3D2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Xu","given":"Yuanlu"},{"family":"Zhu","given":"Song-Chun"},{"family":"Tung","given":"Tony"}],"citation-key":"xuDenseRaCJoint3D2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"7760-7770","source":"openaccess.thecvf.com","title":"DenseRaC: Joint 3D Pose and Shape Estimation by Dense Render-and-Compare","title-short":"DenseRaC","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Xu_DenseRaC_Joint_3D_Pose_and_Shape_Estimation_by_Dense_Render-and-Compare_ICCV_2019_paper.html"},
  {"id":"xueLearningLocalGlobalContextual2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Xue","given":"Nan"},{"family":"Wu","given":"Tianfu"},{"family":"Xia","given":"Gui-Song"},{"family":"Zhang","given":"Liangpei"}],"citation-key":"xueLearningLocalGlobalContextual2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13065-13074","source":"openaccess.thecvf.com","title":"Learning Local-Global Contextual Adaptation for Multi-Person Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Xue_Learning_Local-Global_Contextual_Adaptation_for_Multi-Person_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"xuGloballyConvergentAlgorithm2017","abstract":"Nonconvex optimization arises in many areas of computational science and engineering. However, most nonconvex optimization algorithms are only known to have local convergence or subsequence convergence properties. In this paper, we propose an algorithm for nonconvex optimization and establish its global convergence (of the whole sequence) to a critical point. In addition, we give its asymptotic convergence rate and numerically demonstrate its efficiency. In our algorithm, the variables of the underlying problem are either treated as one block or multiple disjoint blocks. It is assumed that each non-differentiable component of the objective function, or each constraint, applies only to one block of variables. The differentiable components of the objective function, however, can involve multiple blocks of variables together. Our algorithm updates one block of variables at a time by minimizing a certain prox-linear surrogate, along with an extrapolation to accelerate its convergence. The order of update can be either deterministically cyclic or randomly shuffled for each cycle. In fact, our convergence analysis only needs that each block be updated at least once in every fixed number of iterations. We show its global convergence (of the whole sequence) to a critical point under fairly loose conditions including, in particular, the Kurdyka–Łojasiewicz condition, which is satisfied by a broad class of nonconvex/nonsmooth applications. These results, of course, remain valid when the underlying problem is convex. We apply our convergence results to the coordinate descent iteration for non-convex regularized linear regression, as well as a modified rank-one residue iteration for nonnegative matrix factorization. We show that both applications have global convergence. Numerically, we tested our algorithm on nonnegative matrix and tensor factorization problems, where random shuffling clearly improves the chance to avoid low-quality local solutions.","accessed":{"date-parts":[["2021",9,8]]},"author":[{"family":"Xu","given":"Yangyang"},{"family":"Yin","given":"Wotao"}],"citation-key":"xuGloballyConvergentAlgorithm2017","container-title":"Journal of Scientific Computing","container-title-short":"J Sci Comput","DOI":"10.1007/s10915-017-0376-0","ISSN":"1573-7691","issue":"2","issued":{"date-parts":[["2017",8,1]]},"language":"en","page":"700-734","source":"Springer Link","title":"A Globally Convergent Algorithm for Nonconvex Optimization Based on Block Coordinate Update","type":"article-journal","URL":"https://doi.org/10.1007/s10915-017-0376-0","volume":"72"},
  {"id":"xuGraphStackedHourglass2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Xu","given":"Tianhan"},{"family":"Takano","given":"Wataru"}],"citation-key":"xuGraphStackedHourglass2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"16105-16114","source":"openaccess.thecvf.com","title":"Graph Stacked Hourglass Networks for 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Xu_Graph_Stacked_Hourglass_Networks_for_3D_Human_Pose_Estimation_CVPR_2021_paper.html"},
  {"id":"xuLocalizationHuman3D2019","abstract":"With the development of image/video based 3D pose estimation techniques, service robots, human-computer interaction, and 3D somatosensory games have been developed rapidly. However, 3D pose estimation is still one of the most challenging tasks in computer vision. On the one hand, diversity of poses, occlusion and self-occlusion, change in illumination, and complex background increase the complexity of human pose estimation. On the other hand, many application scenarios require high real-time performance for 3D pose estimation. Therefore, we present a 3D pose estimation method based on binocular vision in this paper. For each frame of the binocular videos, the human body is detected firstly; Then Stacked-Hourglass network is used to detect the human joints, and the pixel coordinates of the key joints of all the human bodies in the binocular images are obtained. Finally, with the calibrated camera internal parameters and external parameters, the 3D coordinates of the major joints in the world coordinate system are estimated. This method does not rely on 3D data sets for training. It only requires binocular cameras to perform 3D pose estimation. The experimental results show that the method can locate key joints precisely and the real-time performance is achieved in complex background.","author":[{"family":"Xu","given":"Zheng"},{"family":"Li","given":"Jinping"},{"family":"Yin","given":"Jianqin"},{"family":"Wu","given":"Yanchun"}],"citation-key":"xuLocalizationHuman3D2019","collection-title":"Communications in Computer and Information Science","container-title":"Cognitive Systems and Signal Processing","DOI":"10.1007/978-981-13-7986-4_6","editor":[{"family":"Sun","given":"Fuchun"},{"family":"Liu","given":"Huaping"},{"family":"Hu","given":"Dewen"}],"event-place":"Singapore","ISBN":"978-981-13-7986-4","issued":{"date-parts":[["2019"]]},"language":"en","page":"65-75","publisher":"Springer","publisher-place":"Singapore","source":"Springer Link","title":"Localization of Human 3D Joints Based on Binocular Vision","type":"paper-conference"},
  {"id":"xuLocationFreeHumanPose2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Xu","given":"Xixia"},{"family":"Gao","given":"Yingguo"},{"family":"Yan","given":"Ke"},{"family":"Lin","given":"Xue"},{"family":"Zou","given":"Qi"}],"citation-key":"xuLocationFreeHumanPose2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13137-13146","source":"openaccess.thecvf.com","title":"Location-Free Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Xu_Location-Free_Human_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"xuPoseEverythingCategoryAgnostic2022","abstract":"Existing works on 2D pose estimation mainly focus on a certain category, e.g. human, animal, and vehicle. However, there are lots of application scenarios that require detecting the poses/keypoints of the unseen class of objects. In this paper, we introduce the task of Category-Agnostic Pose Estimation (CAPE), which aims to create a pose estimation model capable of detecting the pose of any class of object given only a few samples with keypoint definition. To achieve this goal, we formulate the pose estimation problem as a keypoint matching problem and design a novel CAPE framework, termed POse Matching Network (POMNet). A transformer-based Keypoint Interaction Module (KIM) is proposed to capture both the interactions among different keypoints and the relationship between the support and query images. We also introduce Multi-category Pose (MP-100) dataset, which is a 2D pose dataset of 100 object categories containing over 20K instances and is well-designed for developing CAPE algorithms. Experiments show that our method outperforms other baseline approaches by a large margin. Codes and data are available at https://github.com/luminxu/Pose-for-Everything.","accessed":{"date-parts":[["2022",11,8]]},"author":[{"family":"Xu","given":"Lumin"},{"family":"Jin","given":"Sheng"},{"family":"Zeng","given":"Wang"},{"family":"Liu","given":"Wentao"},{"family":"Qian","given":"Chen"},{"family":"Ouyang","given":"Wanli"},{"family":"Luo","given":"Ping"},{"family":"Wang","given":"Xiaogang"}],"citation-key":"xuPoseEverythingCategoryAgnostic2022","DOI":"10.48550/arXiv.2207.10387","issued":{"date-parts":[["2022",7,21]]},"number":"arXiv:2207.10387","publisher":"arXiv","source":"arXiv.org","title":"Pose for Everything: Towards Category-Agnostic Pose Estimation","title-short":"Pose for Everything","type":"article","URL":"http://arxiv.org/abs/2207.10387"},
  {"id":"xuzhaoDiscriminativeEstimation3D2008","abstract":"In this paper, we present an efﬁcient discriminative method for human pose estimation. This method learns a direct mapping from visual observations to human body conﬁgurations. The framework requires that the visual features should be powerful enough to discriminate the subtle differences between similar human poses. We propose to describe the image features using salient interest points that are represented by SIFTlike descriptors. The descriptor encode the position, appearance, and local structural information simultaneously. Bag-of-words representation is used to model the distribution of feature space. The descriptor can tolerate a range of illumination and position variations because it is computed on overlapped patches. We use Gaussian process regression to model the mapping from visual observations to human poses. This probabilistic regression algorithm is effective and robust to the pose estimation problem. We test our approach on the HumanEva data set. Experimental results demonstrate that our approach achieves the state of the art performance.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"literal":"Xu Zhao"},{"family":"Ning","given":"Huazhong"},{"literal":"Yuncai Liu"},{"family":"Huang","given":"Thomas"}],"citation-key":"xuzhaoDiscriminativeEstimation3D2008","container-title":"2008 19th International Conference on Pattern Recognition","DOI":"10.1109/ICPR.2008.4761707","event-place":"Tampa, FL, USA","event-title":"2008 19th International Conference on Pattern Recognition (ICPR)","ISBN":"978-1-4244-2174-9","ISSN":"1051-4651","issued":{"date-parts":[["2008",12]]},"language":"en","note":"ZSCC:00041","page":"1-4","publisher":"IEEE","publisher-place":"Tampa, FL, USA","source":"DOI.org (Crossref)","title":"Discriminative estimation of 3D human pose using Gaussian processes","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/4761707/"},
  {"id":"xuzhaoHumanMotionTracking2011","abstract":"Human pose estimation via motion tracking systems can be considered as a regression problem within a discriminative framework. It is always a challenging task to model the mapping from observation space to state space because of the high-dimensional characteristic in the multimodal conditional distribution. In order to build the mapping, existing techniques usually involve a large set of training samples in the learning process which are limited in their capability to deal with multimodality. We propose, in this work, a novel online sparse Gaussian Process (GP) regression model to recover 3-D human motion in monocular videos. Particularly, we investigate the fact that for a given test input, its output is mainly determined by the training samples potentially residing in its local neighborhood and deﬁned in the uniﬁed input-output space. This leads to a local mixture GP experts system composed of different local GP experts, each of which dominates a mapping behavior with the speciﬁc covariance function adapting to a local region. To handle the multimodality, we combine both temporal and spatial information therefore to obtain two categories of local experts. The temporal and spatial experts are integrated into a seamless hybrid system, which is automatically self-initialized and robust for visual tracking of nonlinear human motion. Learning and inference are extremely efﬁcient as all the local experts are deﬁned online within very small neighborhoods. Extensive experiments on two real-world databases, HumanEva and PEAR, demonstrate the effectiveness of our proposed model, which signiﬁcantly improve the performance of existing models.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"literal":"Xu Zhao"},{"literal":"Yun Fu"},{"literal":"Yuncai Liu"}],"citation-key":"xuzhaoHumanMotionTracking2011","container-title":"IEEE Transactions on Image Processing","container-title-short":"IEEE Trans. on Image Process.","DOI":"10.1109/TIP.2010.2076820","ISSN":"1057-7149, 1941-0042","issue":"4","issued":{"date-parts":[["2011",4]]},"language":"en","note":"ZSCC:00037","number":"4","page":"1141-1151","source":"DOI.org (Crossref)","title":"Human Motion Tracking by Temporal-Spatial Local Gaussian Process Experts","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/5575422/","volume":"20"},
  {"id":"xuzhaoHumanPoseRegression2010","abstract":"We consider the problem of estimating 3-D human body pose from visual signals within a discriminative framework. It is challenging because there is a wide gap between complex 3-D human motion and planar visual observation, which makes this a severely ill-conditioned problem. In this paper, we focus on three critical factors to tackle human body pose estimation, namely, feature extraction, learning algorithm, and camera utilization. On the feature level, we describe images using the salient interest points represented by scale-invariant feature transform (SIFT)like descriptors, in which the position, appearance, and local structural information are encoded simultaneously. On the learning algorithm level, we propose to use Gaussian processes and multiple linear (ML) regression to model the mapping between poses and features. Fusing image information from multiple cameras in different views is of great interest to us on the camera level. We make a comprehensive evaluation on the HumanEva database and get two meaningful insights into the three crucial aspects for human pose estimation: 1) although the choice of feature is very important to the problem, once the learning algorithm becomes efﬁcient, the choice of feature is no longer critical, and 2) the impact of information combination from multiple cameras on pose estimation is closely related to not only the quantity of image information, but also its quality. In most cases, it is true that the more information is involved, the better results can be achieved. But when the information quantity is the same, the differences in quality will lead to totally different performance. Furthermore, dense evaluations demonstrate that our approach is an accurate and robust solution to the human body pose estimation problem.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"literal":"Xu Zhao"},{"literal":"Yun Fu"},{"literal":"Huazhong Ning"},{"literal":"Yuncai Liu"},{"family":"Huang","given":"Thomas S"}],"citation-key":"xuzhaoHumanPoseRegression2010","container-title":"IEEE Transactions on Circuits and Systems for Video Technology","container-title-short":"IEEE Trans. Circuits Syst. Video Technol.","DOI":"10.1109/TCSVT.2010.2045916","ISSN":"1051-8215, 1558-2205","issue":"7","issued":{"date-parts":[["2010",7]]},"language":"en","note":"ZSCC:00016","number":"7","page":"957-966","source":"DOI.org (Crossref)","title":"Human Pose Regression Through Multiview Visual Fusion","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/5433014/","volume":"20"},
  {"id":"xuzhaoMultipleSubcategoriesPartsBased2013","abstract":"Small sample set, occlusion, and illumination variations are the critical obstacles for a face identiﬁcation system towards practical application. In this paper, we propose a probabilistic generative model for parts-based data representation to address these difﬁculties. In our approach, multiple subcategories corresponding to the individual face parts, such as nose, mouth, eye, and so forth, are modeled within a probabilistic graphical model framework to mimic the process of generating a face image. The induced face representation, therefore, encodes rich discriminative information. Model training is totally unsupervised. Once the training is completed, a test sample from the face class can be recognized as a novel combination of learned parts. In summary, the main contributions of this work are threefold: 1) A novel hierarchical probabilistic generative model is proposed, which is capable of achieving an efﬁcient parts-based representation for robust face identiﬁcation. 2) A constrained variational EM algorithm is developed to learn the model parameters and infer the variables. 3) Two similarity metrics are specially designed for the novel parts-based feature representation, which are effective for matching score guided one sample face identiﬁcation. The models and similarity metrics are validated on three face databases. Experimental results demonstrate the capabilities of the model to deal with small sample set, occlusions, and illumination variances.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"literal":"Xu Zhao"},{"literal":"Xiong Li"},{"literal":"Zhe Wu"},{"literal":"Yun Fu"},{"literal":"Yuncai Liu"}],"citation-key":"xuzhaoMultipleSubcategoriesPartsBased2013","container-title":"IEEE Transactions on Information Forensics and Security","container-title-short":"IEEE Trans.Inform.Forensic Secur.","DOI":"10.1109/TIFS.2013.2263498","ISSN":"1556-6013, 1556-6021","issue":"10","issued":{"date-parts":[["2013",10]]},"language":"en","note":"ZSCC:00009","number":"10","page":"1654-1664","source":"DOI.org (Crossref)","title":"Multiple Subcategories Parts-Based Representation for One Sample Face Identification","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/6545327/","volume":"8"},
  {"id":"xuzhaoTextCornersNovel2011","abstract":"Detecting text and caption from videos is important and in great demand for video retrieval, annotation, indexing, and content analysis. In this paper, we present a corner based approach to detect text and caption from videos. This approach is inspired by the observation that there exist dense and orderly presences of corner points in characters, especially in text and caption. We use several discriminative features to describe the text regions formed by the corner points. The usage of these features is in a ﬂexible manner, thus, can be adapted to different applications. Language independence is an important advantage of the proposed method. Moreover, based upon the text features, we further develop a novel algorithm to detect moving captions in videos. In the algorithm, the motion features, extracted by optical ﬂow, are combined with text features to detect the moving caption patterns. The decision tree is adopted to learn the classiﬁcation criteria. Experiments conducted on a large volume of real video shots demonstrate the efﬁciency and robustness of our proposed approaches and the real-world system. Our text and caption detection system was recently highlighted in a worldwide multimedia retrieval competition, Star Challenge, by achieving the superior performance with the top ranking.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"literal":"Xu Zhao"},{"literal":"Kai-Hsiang Lin"},{"literal":"Yun Fu"},{"literal":"Yuxiao Hu"},{"literal":"Yuncai Liu"},{"family":"Huang","given":"Thomas S"}],"citation-key":"xuzhaoTextCornersNovel2011","container-title":"IEEE Transactions on Image Processing","container-title-short":"IEEE Trans. on Image Process.","DOI":"10.1109/TIP.2010.2068553","ISSN":"1057-7149, 1941-0042","issue":"3","issued":{"date-parts":[["2011",3]]},"language":"en","note":"ZSCC:00183","number":"3","page":"790-799","source":"DOI.org (Crossref)","title":"Text From Corners: A Novel Approach to Detect Text and Caption in Videos","title-short":"Text From Corners","type":"article-journal","URL":"http://ieeexplore.ieee.org/document/5551198/","volume":"20"},
  {"id":"yang3DHumanPose2018","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Yang","given":"Wei"},{"family":"Ouyang","given":"Wanli"},{"family":"Wang","given":"Xiaolong"},{"family":"Ren","given":"Jimmy"},{"family":"Li","given":"Hongsheng"},{"family":"Wang","given":"Xiaogang"}],"citation-key":"yang3DHumanPose2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"5255-5264","source":"openaccess.thecvf.com","title":"3D Human Pose Estimation in the Wild by Adversarial Learning","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Yang_3D_Human_Pose_CVPR_2018_paper.html"},
  {"id":"yangArticulatedPoseEstimation2011","abstract":"We describe a method for human pose estimation in static images based on a novel representation of part models. Notably, we do not use articulated limb parts, but rather capture orientation with a mixture of templates for each part. We describe a general, flexible mixture model for capturing contextual co-occurrence relations between parts, augmenting standard spring models that encode spatial relations. We show that such relations can capture notions of local rigidity. When co-occurrence and spatial relations are tree-structured, our model can be efficiently optimized with dynamic programming. We present experimental results on standard benchmarks for pose estimation that indicate our approach is the state-of-the-art system for pose estimation, outperforming past work by 50% while being orders of magnitude faster.","author":[{"family":"Yang","given":"Yi"},{"family":"Ramanan","given":"Deva"}],"citation-key":"yangArticulatedPoseEstimation2011","container-title":"CVPR 2011","DOI":"10.1109/CVPR.2011.5995741","event-title":"CVPR 2011","ISSN":"1063-6919","issued":{"date-parts":[["2011",6]]},"page":"1385-1392","source":"IEEE Xplore","title":"Articulated pose estimation with flexible mixtures-of-parts","type":"paper-conference"},
  {"id":"yangDynamicalPoseEstimation2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Yang","given":"Heng"},{"family":"Doran","given":"Chris"},{"family":"Slotine","given":"Jean-Jacques"}],"citation-key":"yangDynamicalPoseEstimation2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"5926-5935","source":"openaccess.thecvf.com","title":"Dynamical Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Yang_Dynamical_Pose_Estimation_ICCV_2021_paper.html"},
  {"id":"yangLearningDynamicsGraph2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Yang","given":"Yiding"},{"family":"Ren","given":"Zhou"},{"family":"Li","given":"Haoxiang"},{"family":"Zhou","given":"Chunluan"},{"family":"Wang","given":"Xinchao"},{"family":"Hua","given":"Gang"}],"citation-key":"yangLearningDynamicsGraph2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"8074-8084","source":"openaccess.thecvf.com","title":"Learning Dynamics via Graph Neural Networks for Human Pose Estimation and Tracking","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Yang_Learning_Dynamics_via_Graph_Neural_Networks_for_Human_Pose_Estimation_CVPR_2021_paper.html"},
  {"id":"yangPoseKernelLifterMetricLifting2022","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Yang","given":"Zhijian"},{"family":"Fan","given":"Xiaoran"},{"family":"Isler","given":"Volkan"},{"family":"Park","given":"Hyun Soo"}],"citation-key":"yangPoseKernelLifterMetricLifting2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13179-13189","source":"openaccess.thecvf.com","title":"PoseKernelLifter: Metric Lifting of 3D Human Pose Using Sound","title-short":"PoseKernelLifter","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Yang_PoseKernelLifter_Metric_Lifting_of_3D_Human_Pose_Using_Sound_CVPR_2022_paper.html"},
  {"id":"yangSeqHANDRGBSequenceBased3D2020","abstract":"3D hand pose estimation based on RGB images has been studied for a long time. Most of the studies, however, have performed frame-by-frame estimation based on independent static images. In this paper, we attempt to not only consider the appearance of a hand but incorporate the temporal movement information of a hand in motion into the learning framework for better 3D hand pose estimation performance, which leads to the necessity of a large scale dataset with sequential RGB hand images. We propose a novel method that generates a synthetic dataset that mimics natural human hand movements by re-engineering annotations of an extant static hand pose dataset into pose-flows. With the generated dataset, we train a newly proposed recurrent framework, exploiting visuo-temporal features from sequential images of synthetic hands in motion and emphasizing temporal smoothness of estimations with a temporal consistency constraint. Our novel training strategy of detaching the recurrent layer of the framework during domain finetuning from synthetic to real allows preservation of the visuo-temporal features learned from sequential synthetic hand images. Hand poses that are sequentially estimated consequently produce natural and smooth hand movements which lead to more robust estimations. We show that utilizing temporal information for 3D hand pose estimation significantly enhances general pose estimations by outperforming state-of-the-art methods in experiments on hand pose estimation benchmarks.","accessed":{"date-parts":[["2023",9,26]]},"author":[{"family":"Yang","given":"John"},{"family":"Chang","given":"Hyung Jin"},{"family":"Lee","given":"Seungeui"},{"family":"Kwak","given":"Nojun"}],"citation-key":"yangSeqHANDRGBSequenceBased3D2020","issued":{"date-parts":[["2020",7,10]]},"number":"arXiv:2007.05168","publisher":"arXiv","source":"arXiv.org","title":"SeqHAND:RGB-Sequence-Based 3D Hand Pose and Shape Estimation","title-short":"SeqHAND","type":"article","URL":"http://arxiv.org/abs/2007.05168"},
  {"id":"yanOptimizationBasedFramework2010","abstract":"In computer vision community, human pose estimation and nonrigid shape recovery have evolved into different subfields. The state-of-the-art optimization techniques have been applied to the problem of deformable surface reconstruction successfully and recent methods in this area have focused on designing formulations that are easier to solve. In general, these techniques lay their success on the assumption that sufficient 2-D-3-D correspondences can be detected. By contrast, confronted with the similar ambiguity problem, many techniques for human pose estimation adopt stochastic searching or discriminative predictions, which allow for more generative image cues. However, the global optimization cannot be guaranteed via the stochastic methods; and discriminative techniques usually suffer from inaccuracy. In this letter, we absorb ideas from both domains and propose a unified approach for articulated human pose estimation. Specifically, we optimize the human pose to account for the discriminative pose prediction, bone length preservation in parallel with the point-topoint image observation. Moreover, the L2 norm minimization is solved iteratively as a linear system with high computational efficiency.","author":[{"family":"Yan","given":"Junchi"},{"family":"Shen","given":"Shuhan"},{"family":"Li","given":"Yin"},{"family":"Liu","given":"Yuncai"}],"citation-key":"yanOptimizationBasedFramework2010","container-title":"IEEE Signal Processing Letters","DOI":"10.1109/LSP.2010.2053845","ISSN":"1558-2361","issue":"8","issued":{"date-parts":[["2010",8]]},"page":"766-769","source":"IEEE Xplore","title":"An Optimization Based Framework for Human Pose Estimation","type":"article-journal","volume":"17"},
  {"id":"yanOptimizationBasedFramework2010a","abstract":"In computer vision community, human pose estimation and nonrigid shape recovery have evolved into different subfields. The state-of-the-art optimization techniques have been applied to the problem of deformable surface reconstruction successfully and recent methods in this area have focused on designing formulations that are easier to solve. In general, these techniques lay their success on the assumption that sufficient 2-D-3-D correspondences can be detected. By contrast, confronted with the similar ambiguity problem, many techniques for human pose estimation adopt stochastic searching or discriminative predictions, which allow for more generative image cues. However, the global optimization cannot be guaranteed via the stochastic methods; and discriminative techniques usually suffer from inaccuracy. In this letter, we absorb ideas from both domains and propose a unified approach for articulated human pose estimation. Specifically, we optimize the human pose to account for the discriminative pose prediction, bone length preservation in parallel with the point-topoint image observation. Moreover, the L2 norm minimization is solved iteratively as a linear system with high computational efficiency.","author":[{"family":"Yan","given":"Junchi"},{"family":"Shen","given":"Shuhan"},{"family":"Li","given":"Yin"},{"family":"Liu","given":"Yuncai"}],"citation-key":"yanOptimizationBasedFramework2010a","container-title":"IEEE Signal Processing Letters","DOI":"10.1109/LSP.2010.2053845","ISSN":"1558-2361","issue":"8","issued":{"date-parts":[["2010",8]]},"page":"766-769","source":"IEEE Xplore","title":"An Optimization Based Framework for Human Pose Estimation","type":"article-journal","volume":"17"},
  {"id":"yaoMONETMultiviewSemiSupervised2019","abstract":"This paper presents MONET-an end-to-end semi-supervised learning frameworkfor a keypoint detector using multiview image streams. In particular, we consider general subjects such as non-human species where attaining a large scale annotated dataset is challenging. While multiview geometry can be used to self-supervise the unlabeled data, integrating the geometry into learning a keypoint detector is challenging due to representation mismatch. We address this mismatch by formulating a new differentiable representation of the epipolar constraint called epipolar divergence-a generalized distance from the epipolar lines to the corresponding keypoint distribution. Epipolar divergence characterizes when two view keypoint distributions produce zero reprojection error. We design a twin network that minimizes the epipolar divergence through stereo rectification that can significantly alleviate computational complexity and sampling aliasing in training. We demonstrate that our framework can localize customized keypoints of diverse species, e.g., humans, dogs, and monkeys.","author":[{"family":"Yao","given":"Yuan"},{"family":"Jafarian","given":"Yasamin"},{"family":"Park","given":"Hyun Soo"}],"citation-key":"yaoMONETMultiviewSemiSupervised2019","container-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV.2019.00084","event-title":"2019 IEEE/CVF International Conference on Computer Vision (ICCV)","ISSN":"2380-7504","issued":{"date-parts":[["2019",10]]},"page":"753-762","source":"IEEE Xplore","title":"MONET: Multiview Semi-Supervised Keypoint Detection via Epipolar Divergence","title-short":"MONET","type":"paper-conference"},
  {"id":"yaoMONETMultiviewSemiSupervised2019a","accessed":{"date-parts":[["2022",2,14]]},"author":[{"family":"Yao","given":"Yuan"},{"family":"Jafarian","given":"Yasamin"},{"family":"Park","given":"Hyun Soo"}],"citation-key":"yaoMONETMultiviewSemiSupervised2019a","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"753-762","source":"openaccess.thecvf.com","title":"MONET: Multiview Semi-Supervised Keypoint Detection via Epipolar Divergence","title-short":"MONET","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Yao_MONET_Multiview_Semi-Supervised_Keypoint_Detection_via_Epipolar_Divergence_ICCV_2019_paper.html"},
  {"id":"yeAffordanceDiffusionSynthesizing2023","abstract":"Recent successes in image synthesis are powered by large-scale diffusion models. However, most methods are currently limited to either text- or image-conditioned generation for synthesizing an entire image, texture transfer or inserting objects into a user-specified region. In contrast, in this work we focus on synthesizing complex interactions (ie, an articulated hand) with a given object. Given an RGB image of an object, we aim to hallucinate plausible images of a human hand interacting with it. We propose a two-step generative approach: a LayoutNet that samples an articulation-agnostic hand-object-interaction layout, and a ContentNet that synthesizes images of a hand grasping the object given the predicted layout. Both are built on top of a large-scale pretrained diffusion model to make use of its latent representation. Compared to baselines, the proposed method is shown to generalize better to novel objects and perform surprisingly well on out-of-distribution in-the-wild scenes of portable-sized objects. The resulting system allows us to predict descriptive affordance information, such as hand articulation and approaching orientation. Project page: https://judyye.github.io/affordiffusion-www","accessed":{"date-parts":[["2023",12,9]]},"author":[{"family":"Ye","given":"Yufei"},{"family":"Li","given":"Xueting"},{"family":"Gupta","given":"Abhinav"},{"family":"De Mello","given":"Shalini"},{"family":"Birchfield","given":"Stan"},{"family":"Song","given":"Jiaming"},{"family":"Tulsiani","given":"Shubham"},{"family":"Liu","given":"Sifei"}],"citation-key":"yeAffordanceDiffusionSynthesizing2023","issued":{"date-parts":[["2023",5,20]]},"number":"arXiv:2303.12538","publisher":"arXiv","source":"arXiv.org","title":"Affordance Diffusion: Synthesizing Hand-Object Interactions","title-short":"Affordance Diffusion","type":"article","URL":"http://arxiv.org/abs/2303.12538"},
  {"id":"yeDiffusionGuidedReconstructionEveryday","author":[{"family":"Ye","given":"Yufei"},{"family":"Hebbar","given":"Poorvi"},{"family":"Gupta","given":"Abhinav"},{"family":"Tulsiani","given":"Shubham"}],"citation-key":"yeDiffusionGuidedReconstructionEveryday","language":"en","source":"Zotero","title":"Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips","type":"article-journal"},
  {"id":"yingRgbFusionPointCloudBased3d2021","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Ying","given":"Jiaming"},{"family":"Zhao","given":"Xu"}],"citation-key":"yingRgbFusionPointCloudBased3d2021","container-title":"2021 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP42928.2021.9506588","event-place":"Anchorage, AK, USA","event-title":"2021 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-6654-4115-5","issued":{"date-parts":[["2021",9,19]]},"language":"en","note":"ZSCC:00005","page":"3108-3112","publisher":"IEEE","publisher-place":"Anchorage, AK, USA","source":"DOI.org (Crossref)","title":"Rgb-D Fusion For Point-Cloud-Based 3d Human Pose Estimation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9506588/"},
  {"id":"yuAlleviatingModelingAmbiguity2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Yu","given":"Zhenbo"},{"family":"Ni","given":"Bingbing"},{"family":"Xu","given":"Jingwei"},{"family":"Wang","given":"Junjie"},{"family":"Zhao","given":"Chenglong"},{"family":"Zhang","given":"Wenjun"}],"citation-key":"yuAlleviatingModelingAmbiguity2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"8651-8660","source":"openaccess.thecvf.com","title":"Towards Alleviating the Modeling Ambiguity of Unsupervised Monocular 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Yu_Towards_Alleviating_the_Modeling_Ambiguity_of_Unsupervised_Monocular_3D_Human_ICCV_2021_paper.html"},
  {"id":"yuanEgoPoseEstimationForecasting2019","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Yuan","given":"Ye"},{"family":"Kitani","given":"Kris"}],"citation-key":"yuanEgoPoseEstimationForecasting2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"10082-10092","source":"openaccess.thecvf.com","title":"Ego-Pose Estimation and Forecasting As Real-Time PD Control","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Yuan_Ego-Pose_Estimation_and_Forecasting_As_Real-Time_PD_Control_ICCV_2019_paper.html"},
  {"id":"yuanSimPoESimulatedCharacter2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Yuan","given":"Ye"},{"family":"Wei","given":"Shih-En"},{"family":"Simon","given":"Tomas"},{"family":"Kitani","given":"Kris"},{"family":"Saragih","given":"Jason"}],"citation-key":"yuanSimPoESimulatedCharacter2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"7159-7169","source":"openaccess.thecvf.com","title":"SimPoE: Simulated Character Control for 3D Human Pose Estimation","title-short":"SimPoE","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Yuan_SimPoE_Simulated_Character_Control_for_3D_Human_Pose_Estimation_CVPR_2021_paper.html"},
  {"id":"yuBidirectionallyDeformableMotion2023","abstract":"Video-based human pose transfer is a video-to-video generation task that animates a plain source human image based on a series of target human poses. Considering the difficulties in transferring highly structural patterns on the garments and discontinuous poses, existing methods often generate unsatisfactory results such as distorted textures and flickering artifacts. To address these issues, we propose a novel Deformable Motion Modulation (DMM) that utilizes geometric kernel offset with adaptive weight modulation to simultaneously perform feature alignment and style transfer. Different from normal style modulation used in style transfer, the proposed modulation mechanism adaptively reconstructs smoothed frames from style codes according to the object shape through an irregular receptive field of view. To enhance the spatio-temporal consistency, we leverage bidirectional propagation to extract the hidden motion information from a warped image sequence generated by noisy poses. The proposed feature propagation significantly enhances the motion prediction ability by forward and backward propagation. Both quantitative and qualitative experimental results demonstrate superiority over the state-of-the-arts in terms of image fidelity and visual continuity. The source code is publicly available at github.com/rocketappslab/bdmm.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Yu","given":"Wing-Yin"},{"family":"Po","given":"Lai-Man"},{"family":"Cheung","given":"Ray C. C."},{"family":"Zhao","given":"Yuzhi"},{"family":"Xue","given":"Yu"},{"family":"Li","given":"Kun"}],"citation-key":"yuBidirectionallyDeformableMotion2023","DOI":"10.48550/arXiv.2307.07754","issued":{"date-parts":[["2023",7,18]]},"number":"arXiv:2307.07754","publisher":"arXiv","source":"arXiv.org","title":"Bidirectionally Deformable Motion Modulation For Video-based Human Pose Transfer","type":"article","URL":"http://arxiv.org/abs/2307.07754"},
  {"id":"yuGLAGCNGloballocalAdaptive2023","abstract":"3D human pose estimation has been researched for decades with promising fruits. 3D human pose lifting is one of the promising research directions toward the task where both estimated pose and ground truth pose data are used for training. Existing pose lifting works mainly focus on improving the performance of estimated pose, but they usually underperform when testing on the ground truth pose data. We observe that the performance of the estimated pose can be easily improved by preparing good quality 2D pose, such as fine-tuning the 2D pose or using advanced 2D pose detectors. As such, we concentrate on improving the 3D human pose lifting via ground truth data for the future improvement of more quality estimated pose data. Towards this goal, a simple yet effective model called Global-local Adaptive Graph Convolutional Network (GLA-GCN) is proposed in this work. Our GLA-GCN globally models the spatiotemporal structure via a graph representation and backtraces local joint features for 3D human pose estimation via individually connected layers. To validate our model design, we conduct extensive experiments on three benchmark datasets: Human3.6M, HumanEva-I, and MPI-INF-3DHP. Experimental results show that our GLA-GCN implemented with ground truth 2D poses significantly outperforms state-of-the-art methods (e.g., up to around 3%, 17%, and 14% error reductions on Human3.6M, HumanEva-I, and MPI-INF-3DHP, respectively). GitHub: https://github.com/bruceyo/GLA-GCN.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Yu","given":"Bruce X. B."},{"family":"Zhang","given":"Zhi"},{"family":"Liu","given":"Yongxu"},{"family":"Zhong","given":"Sheng-hua"},{"family":"Liu","given":"Yan"},{"family":"Chen","given":"Chang Wen"}],"citation-key":"yuGLAGCNGloballocalAdaptive2023","DOI":"10.48550/arXiv.2307.05853","issued":{"date-parts":[["2023",7,21]]},"number":"arXiv:2307.05853","publisher":"arXiv","source":"arXiv.org","title":"GLA-GCN: Global-local Adaptive Graph Convolutional Network for 3D Human Pose Estimation from Monocular Video","title-short":"GLA-GCN","type":"article","URL":"http://arxiv.org/abs/2307.05853"},
  {"id":"yuLiteHRNetLightweightHighResolution2021","accessed":{"date-parts":[["2024",3,11]]},"author":[{"family":"Yu","given":"Changqian"},{"family":"Xiao","given":"Bin"},{"family":"Gao","given":"Changxin"},{"family":"Yuan","given":"Lu"},{"family":"Zhang","given":"Lei"},{"family":"Sang","given":"Nong"},{"family":"Wang","given":"Jingdong"}],"citation-key":"yuLiteHRNetLightweightHighResolution2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"10440-10450","source":"openaccess.thecvf.com","title":"Lite-HRNet: A Lightweight High-Resolution Network","title-short":"Lite-HRNet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Yu_Lite-HRNet_A_Lightweight_High-Resolution_Network_CVPR_2021_paper.html"},
  {"id":"yuPCLsGeometryAwareNeural2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Yu","given":"Frank"},{"family":"Salzmann","given":"Mathieu"},{"family":"Fua","given":"Pascal"},{"family":"Rhodin","given":"Helge"}],"citation-key":"yuPCLsGeometryAwareNeural2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"9064-9073","source":"openaccess.thecvf.com","title":"PCLs: Geometry-Aware Neural Reconstruction of 3D Pose With Perspective Crop Layers","title-short":"PCLs","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Yu_PCLs_Geometry-Aware_Neural_Reconstruction_of_3D_Pose_With_Perspective_Crop_CVPR_2021_paper.html"},
  {"id":"zanfirMonocular3DPose2018","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Zanfir","given":"Andrei"},{"family":"Marinoiu","given":"Elisabeta"},{"family":"Sminchisescu","given":"Cristian"}],"citation-key":"zanfirMonocular3DPose2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"2148-2157","source":"openaccess.thecvf.com","title":"Monocular 3D Pose and Shape Estimation of Multiple People in Natural Scenes - The Importance of Multiple Scene Constraints","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Zanfir_Monocular_3D_Pose_CVPR_2018_paper.html"},
  {"id":"zanfirNeuralDescentVisual2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Zanfir","given":"Andrei"},{"family":"Bazavan","given":"Eduard Gabriel"},{"family":"Zanfir","given":"Mihai"},{"family":"Freeman","given":"William T."},{"family":"Sukthankar","given":"Rahul"},{"family":"Sminchisescu","given":"Cristian"}],"citation-key":"zanfirNeuralDescentVisual2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"14484-14493","source":"openaccess.thecvf.com","title":"Neural Descent for Visual 3D Human Pose and Shape","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Zanfir_Neural_Descent_for_Visual_3D_Human_Pose_and_Shape_CVPR_2021_paper.html"},
  {"id":"zengLearningSkeletalGraph2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Zeng","given":"Ailing"},{"family":"Sun","given":"Xiao"},{"family":"Yang","given":"Lei"},{"family":"Zhao","given":"Nanxuan"},{"family":"Liu","given":"Minhao"},{"family":"Xu","given":"Qiang"}],"citation-key":"zengLearningSkeletalGraph2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11436-11445","source":"openaccess.thecvf.com","title":"Learning Skeletal Graph Neural Networks for Hard 3D Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Zeng_Learning_Skeletal_Graph_Neural_Networks_for_Hard_3D_Pose_Estimation_ICCV_2021_paper.html"},
  {"id":"zengNeuralArchitectureSearch2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Zeng","given":"Dan"},{"family":"Huang","given":"Yuhang"},{"family":"Bao","given":"Qian"},{"family":"Zhang","given":"Junjie"},{"family":"Su","given":"Chi"},{"family":"Liu","given":"Wu"}],"citation-key":"zengNeuralArchitectureSearch2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11385-11394","source":"openaccess.thecvf.com","title":"Neural Architecture Search for Joint Human Parsing and Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Zeng_Neural_Architecture_Search_for_Joint_Human_Parsing_and_Pose_Estimation_ICCV_2021_paper.html"},
  {"id":"zhaiHopFIRHopwiseGraphFormer2023","abstract":"2D-to-3D human pose lifting is fundamental for 3D human pose estimation (HPE), for which graph convolutional networks (GCNs) have proven inherently suitable for modeling the human skeletal topology. However, the current GCN-based 3D HPE methods update the node features by aggregating their neighbors' information without considering the interaction of joints in different joint synergies. Although some studies have proposed importing limb information to learn the movement patterns, the latent synergies among joints, such as maintaining balance are seldom investigated. We propose the Hop-wise GraphFormer with Intragroup Joint Refinement (HopFIR) architecture to tackle the 3D HPE problem. HopFIR mainly consists of a novel hop-wise GraphFormer (HGF) module and an intragroup joint refinement (IJR) module. The HGF module groups the joints by k-hop neighbors and applies a hopwise transformer-like attention mechanism to these groups to discover latent joint synergies. The IJR module leverages the prior limb information for peripheral joint refinement. Extensive experimental results show that HopFIR outperforms the SOTA methods by a large margin, with a mean per-joint position error (MPJPE) on the Human3.6M dataset of 32.67 mm. We also demonstrate that the state-of-the-art GCN-based methods can benefit from the proposed hop-wise attention mechanism with a significant improvement in performance: SemGCN and MGCN are improved by 8.9% and 4.5%, respectively.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Zhai","given":"Kai"},{"family":"Nie","given":"Qiang"},{"family":"Ouyang","given":"Bo"},{"family":"Li","given":"Xiang"},{"family":"Yang","given":"Shanlin"}],"citation-key":"zhaiHopFIRHopwiseGraphFormer2023","DOI":"10.48550/arXiv.2302.14581","issued":{"date-parts":[["2023",8,19]]},"number":"arXiv:2302.14581","publisher":"arXiv","source":"arXiv.org","title":"HopFIR: Hop-wise GraphFormer with Intragroup Joint Refinement for 3D Human Pose Estimation","title-short":"HopFIR","type":"article","URL":"http://arxiv.org/abs/2302.14581"},
  {"id":"zhang3DAwareNeuralBody2023","abstract":"Regression-based methods for 3D human pose estimation directly predict the 3D pose parameters from a 2D image using deep networks. While achieving state-of-the-art performance on standard benchmarks, their performance degrades under occlusion. In contrast, optimization-based methods fit a parametric body model to 2D features in an iterative manner. The localized reconstruction loss can potentially make them robust to occlusion, but they suffer from the 2D-3D ambiguity. Motivated by the recent success of generative models in rigid object pose estimation, we propose 3D-aware Neural Body Fitting (3DNBF) - an approximate analysis-by-synthesis approach to 3D human pose estimation with SOTA performance and occlusion robustness. In particular, we propose a generative model of deep features based on a volumetric human representation with Gaussian ellipsoidal kernels emitting 3D pose-dependent feature vectors. The neural features are trained with contrastive learning to become 3D-aware and hence to overcome the 2D-3D ambiguity. Experiments show that 3DNBF outperforms other approaches on both occluded and standard benchmarks. Code is available at https://github.com/edz-o/3DNBF","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Zhang","given":"Yi"},{"family":"Ji","given":"Pengliang"},{"family":"Wang","given":"Angtian"},{"family":"Mei","given":"Jieru"},{"family":"Kortylewski","given":"Adam"},{"family":"Yuille","given":"Alan"}],"citation-key":"zhang3DAwareNeuralBody2023","DOI":"10.48550/arXiv.2308.10123","issued":{"date-parts":[["2023",8,19]]},"number":"arXiv:2308.10123","publisher":"arXiv","source":"arXiv.org","title":"3D-Aware Neural Body Fitting for Occlusion Robust 3D Human Pose Estimation","type":"article","URL":"http://arxiv.org/abs/2308.10123"},
  {"id":"zhang3DILGIrregularLatent","abstract":"We propose a new representation for encoding 3D shapes as neural ﬁelds. The representation is designed to be compatible with the transformer architecture and to beneﬁt both shape reconstruction and shape generation. Existing works on neural ﬁelds are grid-based representations with latents deﬁned on a regular grid. In contrast, we deﬁne latents on irregular grids, enabling our representation to be sparse and adaptive. In the context of shape reconstruction from point clouds, our shape representation built on irregular grids improves upon grid-based methods in terms of reconstruction accuracy. For shape generation, our representation promotes high-quality shape generation using auto-regressive probabilistic models. We show different applications that improve over the current state of the art. First, we show results for probabilistic shape reconstruction from a single higher resolution image. Second, we train a probabilistic model conditioned on very low resolution images. Third, we apply our model to category-conditioned generation. All probabilistic experiments conﬁrm that we are able to generate detailed and high quality shapes to yield the new state of the art in generative 3D shape modeling.","author":[{"family":"Zhang","given":"Biao"},{"family":"Nießner","given":"Matthias"},{"family":"Wonka","given":"Peter"}],"citation-key":"zhang3DILGIrregularLatent","language":"en","source":"Zotero","title":"3DILG: Irregular Latent Grids for 3D Generative Modeling","type":"article-journal"},
  {"id":"zhangAdaFuseAdaptiveMultiview2021","abstract":"Occlusion is probably the biggest challenge for human pose estimation in the wild. Typical solutions often rely on intrusive sensors such as IMUs to detect occluded joints. To make the task truly unconstrained, we present AdaFuse, an adaptive multiview fusion method, which can enhance the features in occluded views by leveraging those in visible views. The core of AdaFuse is to determine the point-point correspondence between two views which we solve effectively by exploring the sparsity of the heatmap representation. We also learn an adaptive fusion weight for each camera view to reflect its feature quality in order to reduce the chance that good features are undesirably corrupted by ``bad'' views. The fusion model is trained end-to-end with the pose estimation network, and can be directly applied to new camera configurations without additional adaptation. We extensively evaluate the approach on three public datasets including Human3.6M, Total Capture and CMU Panoptic. It outperforms the state-of-the-arts on all of them. We also create a large scale synthetic dataset Occlusion-Person, which allows us to perform numerical evaluation on the occluded joints, as it provides occlusion labels for every joint in the images. The dataset and code are released at https://github.com/zhezh/adafuse-3d-human-pose.","accessed":{"date-parts":[["2023",6,14]]},"author":[{"family":"Zhang","given":"Zhe"},{"family":"Wang","given":"Chunyu"},{"family":"Qiu","given":"Weichao"},{"family":"Qin","given":"Wenhu"},{"family":"Zeng","given":"Wenjun"}],"citation-key":"zhangAdaFuseAdaptiveMultiview2021","container-title":"International Journal of Computer Vision","container-title-short":"Int J Comput Vis","DOI":"10.1007/s11263-020-01398-9","ISSN":"0920-5691, 1573-1405","issue":"3","issued":{"date-parts":[["2021",3]]},"page":"703-718","source":"arXiv.org","title":"AdaFuse: Adaptive Multiview Fusion for Accurate Human Pose Estimation in the Wild","title-short":"AdaFuse","type":"article-journal","URL":"http://arxiv.org/abs/2010.13302","volume":"129"},
  {"id":"zhangAdaFuseAdaptiveMultiview2021a","abstract":"Occlusion is probably the biggest challenge for human pose estimation in the wild. Typical solutions often rely on intrusive sensors such as IMUs to detect occluded joints. To make the task truly unconstrained, we present AdaFuse, an adaptive multiview fusion method, which can enhance the features in occluded views by leveraging those in visible views. The core of AdaFuse is to determine the point-point correspondence between two views which we solve effectively by exploring the sparsity of the heatmap representation. We also learn an adaptive fusion weight for each camera view to reflect its feature quality in order to reduce the chance that good features are undesirably corrupted by ``bad'' views. The fusion model is trained end-to-end with the pose estimation network, and can be directly applied to new camera configurations without additional adaptation. We extensively evaluate the approach on three public datasets including Human3.6M, Total Capture and CMU Panoptic. It outperforms the state-of-the-arts on all of them. We also create a large scale synthetic dataset Occlusion-Person, which allows us to perform numerical evaluation on the occluded joints, as it provides occlusion labels for every joint in the images. The dataset and code are released at https://github.com/zhezh/adafuse-3d-human-pose.","accessed":{"date-parts":[["2023",6,12]]},"author":[{"family":"Zhang","given":"Zhe"},{"family":"Wang","given":"Chunyu"},{"family":"Qiu","given":"Weichao"},{"family":"Qin","given":"Wenhu"},{"family":"Zeng","given":"Wenjun"}],"citation-key":"zhangAdaFuseAdaptiveMultiview2021a","container-title":"International Journal of Computer Vision","container-title-short":"Int J Comput Vis","DOI":"10.1007/s11263-020-01398-9","ISSN":"0920-5691, 1573-1405","issue":"3","issued":{"date-parts":[["2021",3]]},"page":"703-718","source":"arXiv.org","title":"AdaFuse: Adaptive Multiview Fusion for Accurate Human Pose Estimation in the Wild","title-short":"AdaFuse","type":"article-journal","URL":"http://arxiv.org/abs/2010.13302","volume":"129"},
  {"id":"zhangCATCornerAided2021","abstract":"Single object tracking in visual media is an important yet challenging task. Various challenges, especially target scale variation, shape deformation and occlusion, can have large effects on the performances of trackers. Current deep regression based trackers only pay close attention to regression on the center key point of the tracking target, meanwhile employ the image pyramid based multi-scale testing method to deal with scale estimation. Such procedure can not properly handle the three challenges. We address these challenges in a principled way by the aid of auxiliary regressions on the four bounding box corners of the tracking target. In this work, we propose the novel Corner Aided Tracker with deep regression network, abbreviated as CAT. Different from RPN-based trackers, in CAT, four corners along with the center key point of the bounding box for tracking target are simultaneously obtained by ﬁve corresponding response maps. Furthermore, to robustly and accurately generate tight bounding boxes for the tracking target and collect reliable samples for online training of the network, we propose an adaptive key point selection method to select the subset of reliable key points and drop the unreliable ones, based on the qualities of their corresponding response maps as well as the constraints from shape, scale and location. We demonstrate that the regressed corners can help naturally locate the tracking target with tight bounding boxes. The challenges of scale variation, shape deformation and occlusion can be handled explicitly. The commonly used time-consuming image pyramid based multi-scale testing method can also be discarded. Extensive experiments on OTB2013, OTB2015, UAV123, LaSOT, VOT2016 and VOT2018 datasets are conducted to report new state-of-theart performances and demonstrate the effectiveness of CAT.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhang","given":"Shiquan"},{"family":"Zhao","given":"Xu"},{"family":"Fang","given":"Liangji"}],"citation-key":"zhangCATCornerAided2021","container-title":"IEEE Transactions on Multimedia","container-title-short":"IEEE Trans. Multimedia","DOI":"10.1109/TMM.2020.2990089","ISSN":"1520-9210, 1941-0077","issued":{"date-parts":[["2021"]]},"language":"en","note":"ZSCC:00004","page":"859-870","source":"DOI.org (Crossref)","title":"CAT: Corner Aided Tracking With Deep Regression Network","title-short":"CAT","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9076834/","volume":"23"},
  {"id":"zhangDetailedAccurateHuman2017","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Zhang","given":"Chao"},{"family":"Pujades","given":"Sergi"},{"family":"Black","given":"Michael J."},{"family":"Pons-Moll","given":"Gerard"}],"citation-key":"zhangDetailedAccurateHuman2017","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2017"]]},"page":"4191-4200","source":"openaccess.thecvf.com","title":"Detailed, Accurate, Human Shape Estimation From Clothed 3D Scan Sequences","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2017/html/Zhang_Detailed_Accurate_Human_CVPR_2017_paper.html"},
  {"id":"zhangDistributionAwareCoordinateRepresentation2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Zhang","given":"Feng"},{"family":"Zhu","given":"Xiatian"},{"family":"Dai","given":"Hanbin"},{"family":"Ye","given":"Mao"},{"family":"Zhu","given":"Ce"}],"citation-key":"zhangDistributionAwareCoordinateRepresentation2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"7093-7102","source":"openaccess.thecvf.com","title":"Distribution-Aware Coordinate Representation for Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Distribution-Aware_Coordinate_Representation_for_Human_Pose_Estimation_CVPR_2020_paper.html"},
  {"id":"zhangExploitingOffsetguidedNetwork","abstract":"Human pose estimation has witnessed a signiﬁcant advance thanks to the development of deep learning. Recent human pose estimation approaches tend to directly predict the location heatmaps, which causes quantization errors and inevitably deteriorates the performance within the reduced network output. Aim at solving it, we revisit the heatmap-offset aggregation method and propose the Offsetguided Network (OGN) with an intuitive but effective fusion strategy for both two-stages pose estimation and Mask R-CNN. For two-stages pose estimation, a greedy box generation strategy is also proposed to keep more necessary candidates while performing person detection. For mask R-CNN, ratio-consistent is adopted to improve the generalization ability of the network. State-of-the-art results on COCO and PoseTrack dataset verify the effectiveness of our offset-guided pose estimation and tracking.","author":[{"family":"Zhang","given":"Rui"},{"family":"Zhu","given":"Zheng"},{"family":"Li","given":"Peng"},{"family":"Wu","given":"Rui"},{"family":"Guo","given":"Chaoxu"},{"family":"Huang","given":"Guan"},{"family":"Xia","given":"Hailun"}],"citation-key":"zhangExploitingOffsetguidedNetwork","language":"en","page":"9","source":"Zotero","title":"Exploiting Offset-guided Network for Pose Estimation and Tracking","type":"article-journal"},
  {"id":"zhangFusingWearableIMUs2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Zhang","given":"Zhe"},{"family":"Wang","given":"Chunyu"},{"family":"Qin","given":"Wenhu"},{"family":"Zeng","given":"Wenjun"}],"citation-key":"zhangFusingWearableIMUs2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"2200-2209","source":"openaccess.thecvf.com","title":"Fusing Wearable IMUs With Multi-View Images for Human Pose Estimation: A Geometric Approach","title-short":"Fusing Wearable IMUs With Multi-View Images for Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Fusing_Wearable_IMUs_With_Multi-View_Images_for_Human_Pose_Estimation_CVPR_2020_paper.html"},
  {"id":"zhangIntegratedGlobalLocalMetric2017","abstract":"The task of person re-identification (re-id) is to match images of people observed in different camera views. Recent researches mainly focus on feature representation and metric learning. Many global metric learning approaches have achieved good performance. Since comparing all of the samples with a single global metric is inappropriate to handle heterogeneous data, some local metric learning approaches are proposed. But most of them cannot be used on re-id directly due to some research challenges. Also, they usually need complicated computation to solve the optimization problems with numerous parameters. In order to improve the performance of global metric learning and avoid complex computation, we propose to simultaneously learn local metrics on clusters of samples softly partitioned by Gaussian Mixture Model (GMM) and a global metric on the entire training set. Then the local metrics are combined with the global metric by their posterior probabilities of GMM to obtain an integrated metric for similarity evaluation. Experiments on three challenging datasets (VIPeR, PRID450S and QMUL GRID) verify the effectiveness of the proposed method.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhang","given":"Jing"},{"family":"Zhao","given":"Xu"}],"citation-key":"zhangIntegratedGlobalLocalMetric2017","container-title":"2017 IEEE Winter Conference on Applications of Computer Vision (WACV)","DOI":"10.1109/WACV.2017.72","event-place":"Santa Rosa, CA, USA","event-title":"2017 IEEE Winter Conference on Applications of Computer Vision (WACV)","ISBN":"978-1-5090-4822-9","issued":{"date-parts":[["2017",3]]},"language":"en","page":"596-604","publisher":"IEEE","publisher-place":"Santa Rosa, CA, USA","source":"DOI.org (Crossref)","title":"Integrated Global-Local Metric Learning for Person Re-identification","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7926655/"},
  {"id":"zhangLearningBasedDistortionCorrection2022","abstract":"Camera calibration is a crucial technique which signiﬁcantly inﬂuences the performance of many robotic systems. Robustness and high precision have always been the pursuit of diverse calibration methods. State-of-the-art calibration techniques, however, still suffer from inexact corner detection, radial lens distortion and unstable parameter estimation. Therefore, in this paper, we improve the precision and robustness of calibration by widening these bottlenecks. In particular, effective distortion correction is performed by a learning-based method. Then, accurate sub-pixel feature location is achieved by the combination of robust learning detection, exact reﬁnement and complete post-processing. To obtain stable parameter estimation, an image-level RANSACbased calibration procedure is proposed. Ultimately, we assemble these methods into a novel and practical calibration framework. Compared with state-of-art methods, experiment results on both real and synthetic datasets under noise, bad lighting and distortion manifest the better robustness and higher precision of the proposed framework.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhang","given":"Yesheng"},{"family":"Zhao","given":"Xu"},{"family":"Qian","given":"Dahong"}],"citation-key":"zhangLearningBasedDistortionCorrection2022","container-title":"IEEE Robotics and Automation Letters","container-title-short":"IEEE Robot. Autom. Lett.","DOI":"10.1109/LRA.2022.3192610","ISSN":"2377-3766, 2377-3774","issue":"4","issued":{"date-parts":[["2022",10]]},"language":"en","note":"ZSCC:00000","number":"4","page":"10470-10477","source":"DOI.org (Crossref)","title":"Learning-Based Distortion Correction and Feature Detection for High Precision and Robust Camera Calibration","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/9834080/","volume":"7"},
  {"id":"zhangLearningCausalRepresentation2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Zhang","given":"Xiheng"},{"family":"Wong","given":"Yongkang"},{"family":"Wu","given":"Xiaofei"},{"family":"Lu","given":"Juwei"},{"family":"Kankanhalli","given":"Mohan"},{"family":"Li","given":"Xiangdong"},{"family":"Geng","given":"Weidong"}],"citation-key":"zhangLearningCausalRepresentation2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11270-11280","source":"openaccess.thecvf.com","title":"Learning Causal Representation for Training Cross-Domain Pose Estimator via Generative Interventions","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Zhang_Learning_Causal_Representation_for_Training_Cross-Domain_Pose_Estimator_via_Generative_ICCV_2021_paper.html"},
  {"id":"zhangLedLocalizationQualityEstimation2018","abstract":"Classiﬁcation subnetwork and box regression subnetwork are essential components in deep networks for object detection. However, we observe a contradiction that before NMS, some better localized detections do not correspond to higher classiﬁcation conﬁdences, and vice versa. This contradiction exists because classiﬁcation conﬁdences can not fully reﬂect the localization-quality (loc-quality) of each detection. In this work, we propose the Localization-quality Estimation embedded Detector abbreviated as LED, and a corresponding detection pipeline. In this detection pipeline, we ﬁrst propose an accurate loc-quality estimation method for each detection, then combine the loc-quality with the corresponding classiﬁcation conﬁdence during inference to make each detection more reasonable and accurate. For efﬁciency, LED is designed as an one-stage network. Extensive experiments are conducted on Pascal VOC 2007 and KITTI car detection datasets to demonstrate the effectiveness of LED.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhang","given":"Shiquan"},{"family":"Zhao","given":"Xu"},{"family":"Fang","given":"Liangji"},{"family":"Fei","given":"Haiping"},{"family":"Song","given":"Haitao"}],"citation-key":"zhangLedLocalizationQualityEstimation2018","container-title":"2018 25th IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2018.8451206","event-place":"Athens","event-title":"2018 25th IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-4799-7061-2","issued":{"date-parts":[["2018",10]]},"language":"en","note":"ZSCC:00018","page":"584-588","publisher":"IEEE","publisher-place":"Athens","source":"DOI.org (Crossref)","title":"Led: Localization-Quality Estimation Embedded Detector","title-short":"Led","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/8451206/"},
  {"id":"zhangMixSTESeq2seqMixed2022","accessed":{"date-parts":[["2022",11,17]]},"author":[{"family":"Zhang","given":"Jinlu"},{"family":"Tu","given":"Zhigang"},{"family":"Yang","given":"Jianyu"},{"family":"Chen","given":"Yujin"},{"family":"Yuan","given":"Junsong"}],"citation-key":"zhangMixSTESeq2seqMixed2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13232-13242","source":"openaccess.thecvf.com","title":"MixSTE: Seq2seq Mixed Spatio-Temporal Encoder for 3D Human Pose Estimation in Video","title-short":"MixSTE","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Zhang_MixSTE_Seq2seq_Mixed_Spatio-Temporal_Encoder_for_3D_Human_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"zhangNeuralDomeNeuralModeling2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Zhang","given":"Juze"},{"family":"Luo","given":"Haimin"},{"family":"Yang","given":"Hongdi"},{"family":"Xu","given":"Xinru"},{"family":"Wu","given":"Qianyang"},{"family":"Shi","given":"Ye"},{"family":"Yu","given":"Jingyi"},{"family":"Xu","given":"Lan"},{"family":"Wang","given":"Jingya"}],"citation-key":"zhangNeuralDomeNeuralModeling2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8834-8845","source":"openaccess.thecvf.com","title":"NeuralDome: A Neural Modeling Pipeline on Multi-View Human-Object Interactions","title-short":"NeuralDome","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Zhang_NeuralDome_A_Neural_Modeling_Pipeline_on_Multi-View_Human-Object_Interactions_CVPR_2023_paper.html"},
  {"id":"zhangObjectOccludedHumanShape2020","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Zhang","given":"Tianshu"},{"family":"Huang","given":"Buzhen"},{"family":"Wang","given":"Yangang"}],"citation-key":"zhangObjectOccludedHumanShape2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"7376-7385","source":"openaccess.thecvf.com","title":"Object-Occluded Human Shape and Pose Estimation From a Single Color Image","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Zhang_Object-Occluded_Human_Shape_and_Pose_Estimation_From_a_Single_Color_CVPR_2020_paper.html"},
  {"id":"zhangPyMAF3DHuman2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Zhang","given":"Hongwen"},{"family":"Tian","given":"Yating"},{"family":"Zhou","given":"Xinchi"},{"family":"Ouyang","given":"Wanli"},{"family":"Liu","given":"Yebin"},{"family":"Wang","given":"Limin"},{"family":"Sun","given":"Zhenan"}],"citation-key":"zhangPyMAF3DHuman2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11446-11456","source":"openaccess.thecvf.com","title":"PyMAF: 3D Human Pose and Shape Regression With Pyramidal Mesh Alignment Feedback Loop","title-short":"PyMAF","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Zhang_PyMAF_3D_Human_Pose_and_Shape_Regression_With_Pyramidal_Mesh_ICCV_2021_paper.html"},
  {"id":"zhangSemisupervisedLearningMultilabel2022","abstract":"Semi-supervised multi-label video action detection aims to locate all the persons and recognize their multiple action labels by leveraging both labeled and unlabeled videos. Compared to the single-label scenario, semi-supervised learning in multi-label video action detection is more challenging due to two significant issues: generation of multiple pseudo labels and class-imbalanced data distribution. In this paper, we propose an effective semi-supervised learning method to tackle these challenges. Firstly, to make full use of the informative unlabeled data for better training, we design an effective multiple pseudo labeling strategy by setting dynamic learnable threshold for each class. Secondly, to handle the long-tailed distribution for each class, we propose the unlabeled class balancing strategy. We select training samples according to the multiple pseudo labels generated during the training iteration, instead of the usual data re-sampling that requires label information before training. Then the balanced re-weighting is leveraged to mitigate the class imbalance caused by multi-label co-occurrence. Extensive experiments conducted on two challenging benchmarks, AVA and UCF101-24, demonstrate the effectiveness of our proposed designs. By using the unlabeled data effectively, our method achieves the state-of-the-art performance in video action detection on both AVA and UCF101-24 datasets. Besides, it can still achieve competitive performance compared with fully-supervised methods when using limited annotations on AVA dataset.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhang","given":"Hongcheng"},{"family":"Zhao","given":"Xu"},{"family":"Wang","given":"Dongqi"}],"citation-key":"zhangSemisupervisedLearningMultilabel2022","container-title":"Proceedings of the 30th ACM International Conference on Multimedia","DOI":"10.1145/3503161.3547980","event-place":"Lisboa Portugal","event-title":"MM '22: The 30th ACM International Conference on Multimedia","ISBN":"978-1-4503-9203-7","issued":{"date-parts":[["2022",10,10]]},"language":"en","note":"ZSCC:00000","page":"2124-2134","publisher":"ACM","publisher-place":"Lisboa Portugal","source":"DOI.org (Crossref)","title":"Semi-supervised Learning for Multi-label Video Action Detection","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3503161.3547980"},
  {"id":"zhangSpatioTemporalMotionAggregation2022","abstract":"Recognizing action patterns and detecting action instances are vital for spatial temporal action detection task, which aims to recognize the actions of interest in untrimmed videos and localize them in both space and time. The mainstream action tubelet detectors, however, ignore the conﬂicts in features between localization and classiﬁcation, and use localization features for temporal modeling, which leads to ineffective action classiﬁcation. In this paper, we propose the Spatio-Temporal Motion Aggregation mechanism for integrating the local motion feature from a short term snippet and the longer spatio-temporal information to predict the action category. We design the Class-Agnostic Center Localization module to perform action instance center localization in the Class-Agnostic manner. Besides, Movement and Size Regression is proposed for movement estimation and spatial extent detection by using Gaussian kernels to encode training samples. These three modules work together to generate the tubelet detection results, which could be further linked to yield video-level tubes with a matching strategy. Our detector achieves the state-of-the-art performance in both frame-mAP and video-mAP metrics, on the UCF-24 and JHMDB datasets.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhang","given":"Hongcheng"},{"family":"Zhao","given":"Xu"}],"citation-key":"zhangSpatioTemporalMotionAggregation2022","container-title":"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","DOI":"10.1109/ICASSP43922.2022.9746817","event-place":"Singapore, Singapore","event-title":"ICASSP 2022 - 2022 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)","ISBN":"978-1-6654-0540-9","issued":{"date-parts":[["2022",5,23]]},"language":"en","note":"ZSCC:00001","page":"2180-2184","publisher":"IEEE","publisher-place":"Singapore, Singapore","source":"DOI.org (Crossref)","title":"Spatio-Temporal Motion Aggregation Network for Video Action Detection","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9746817/"},
  {"id":"zhangStatisticalBackgroundSubtraction2014","abstract":"In this paper, we study the class imbalance problem in statistical background subtraction. Firstly, we discuss the imbalance essence in background subtraction, and conclude that foreground and background are inherently imbalanced. Secondly, following the imbalanced learning strategy in machine learning, we present a spatio-temporal over-sampling method to resolve the class imbalance in background subtraction. Our method densely generate synthesized foreground samples in compact 3D spatio-temporal domain. Those generated samples could reduce the imbalance level between foreground and background from both quantity and quality, and therefore contribute to improvement of detection performance. We also deﬁne a new index to measure the change of imbalance level during over-sampling. Experiments are conducted on public datasets to demonstrate the effectiveness of our method.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhang","given":"Xiang"},{"family":"Liu","given":"Zhi"},{"family":"Li","given":"Hongsheng"},{"family":"Zhao","given":"Xu"},{"family":"Zhang","given":"Ping"}],"citation-key":"zhangStatisticalBackgroundSubtraction2014","container-title":"2014 IEEE International Conference on Multimedia and Expo (ICME)","DOI":"10.1109/ICME.2014.6890245","event-place":"Chengdu, China","event-title":"2014 IEEE International Conference on Multimedia and Expo (ICME)","ISBN":"978-1-4799-4761-4","issued":{"date-parts":[["2014",7]]},"language":"en","note":"ZSCC:00007","page":"1-6","publisher":"IEEE","publisher-place":"Chengdu, China","source":"DOI.org (Crossref)","title":"Statistical background subtraction based on imbalanced learning","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/6890245"},
  {"id":"zhanRay3DRayBased3D2022","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Zhan","given":"Yu"},{"family":"Li","given":"Fenghai"},{"family":"Weng","given":"Renliang"},{"family":"Choi","given":"Wongun"}],"citation-key":"zhanRay3DRayBased3D2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"13116-13125","source":"openaccess.thecvf.com","title":"Ray3D: Ray-Based 3D Human Pose Estimation for Monocular Absolute 3D Localization","title-short":"Ray3D","type":"paper-conference"},
  {"id":"zhaoBACNetBoundaryAnchorComplementary2022","abstract":"The task of temporal action detection aims to locate and classify action segments in untrimmed videos. Most existing works usually consist of two components: snippet-level boundary segmentation and anchor-level action evaluation. These two components, however, are typically designed irrelevantly, so the detection accuracy is undermined due to vague boundaries and complex video content. To tackle this problem, we design two supplementary modules. One module, termed as Anchor Aware Module (AAM), uses temporal and semantic related anchors to enhance snippet feature. The other module, named Boundary Aware Module (BAM), endows anchor feature with structured representation using intermediate supervision. Moreover, the ConvLSTM is applied to establish temporal relation in BAM with the structured representation. These two modules are integrated as the Boundary-Anchor Complementary Network (BACNet), which achieves the state-of-the-art performance on both THUMOS-14 and ActivityNet-1.3 datasets.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhao","given":"Zixuan"},{"family":"Wang","given":"Dongqi"},{"family":"Zhao","given":"Xu"}],"citation-key":"zhaoBACNetBoundaryAnchorComplementary2022","container-title":"2022 IEEE International Conference on Multimedia and Expo (ICME)","DOI":"10.1109/ICME52920.2022.9859756","event-place":"Taipei, Taiwan","event-title":"2022 IEEE International Conference on Multimedia and Expo (ICME)","ISBN":"978-1-6654-8563-0","issued":{"date-parts":[["2022",7,18]]},"language":"en","note":"ZSCC:00000","page":"01-06","publisher":"IEEE","publisher-place":"Taipei, Taiwan","source":"DOI.org (Crossref)","title":"BACNet: Boundary-Anchor Complementary Network for Temporal Action Detection","title-short":"BACNet","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9859756/"},
  {"id":"zhaoCapturing3DHuman2007","abstract":"In this paper, we present an Orthogonal Locality Preserving Projection based (OLPP) approach to capture three-dimensional human motion from monocular images. From the motion capture data residing in high dimension space of human activities, we extract the motion base space in which human pose can be described essentially and concisely by more controllable way. This is actually a dimensionality reduction process completed in the framework of OLPP. And then, the structure of this space corresponding to special activity such as walking motion is explored with data clustering. Pose recovering is performed in the generative framework. For the single image, Gaussian mixture model is used to generate candidates of the 3D pose. The shape context is the common descriptor of image silhouette feature and synthetical feature of human model. We get the shortlist of 3D poses by measuring the shape contexts matching cost between image features and the synthetical features. In tracking situation, an AR model trained by the example sequence produces almost accurate pose predictions. Experiments demonstrate that the proposed approach works well.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zhaoCapturing3DHuman2007","container-title":"HCI","DOI":"10.1007/978-3-540-73321-8_36","event-place":"Berlin, Heidelberg","ISBN":"978-3-540-73318-8 978-3-540-73321-8","issued":{"date-parts":[["2007"]]},"language":"en","note":"ZSCC:00000","page":"304-313","publisher":"Springer Berlin Heidelberg","publisher-place":"Berlin, Heidelberg","source":"DOI.org (Crossref)","title":"Capturing 3D Human Motion from Monocular Images Using Orthogonal Locality Preserving Projection","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-540-73321-8_36","volume":"4561"},
  {"id":"zhaoExploringDiscriminativePose2013","abstract":"Articulated conﬁguration of human body parts is an essential representation of human motion, therefore is well suited for classifying human actions. In this work, we propose a novel approach to exploring the discriminative pose sub-patterns for effective action classiﬁcation. These pose sub-patterns are extracted from a predeﬁned set of 3D poses represented by hierarchical motion angles. The basic idea is motivated by the two observations: (1) There exist representative sub-patterns in each action class, from which the action class can be easily differentiated. (2) These sub-patterns frequently appear in the action class. By constructing a connection between frequent sub-patterns and the discriminative measure, we develop the SSPI, namely, the Support Sub-Pattern Induced learning algorithm for simultaneous feature selection and feature learning. Based on the algorithm, discriminative pose sub-patterns can be identiﬁed and used as a series of “magnetic centers” on the surface of normalized super-sphere for feature transform. The “attractive forces” from the sub-patterns determine the direction and steplength of the transform. This transformation makes a feature more discriminative while maintaining dimensionality invariance. Comprehensive experimental studies conducted on a large scale motion capture dataset demonstrate the effectiveness of the proposed approach for action classiﬁcation and the superior performance over the state-of-the-art techniques.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"},{"family":"Fu","given":"Yun"}],"citation-key":"zhaoExploringDiscriminativePose2013","container-title":"Proceedings of the 21st ACM international conference on Multimedia","DOI":"10.1145/2502081.2502094","event-place":"Barcelona Spain","event-title":"MM '13: ACM Multimedia Conference","ISBN":"978-1-4503-2404-5","issued":{"date-parts":[["2013",10,21]]},"language":"en","note":"ZSCC:00015","page":"273-282","publisher":"ACM","publisher-place":"Barcelona Spain","source":"DOI.org (Crossref)","title":"Exploring discriminative pose sub-patterns for effective action classification","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/2502081.2502094"},
  {"id":"zhaoGenerativeEstimation3D2007","abstract":"We present a method for 3D pose estimation of human motion in generative framework. For the generalization of application scenario, the observation information we utilized comes from monocular silhouettes. We distill prior information of human motion by performing conventional PCA on single motion capture data sequence. In doing so, the aims for both reducing dimensionality and extracting the prior knowledge of human motion are achieved simultaneously. We adopt the shape contexts descriptor to construct the matching function, by which the validity and the robustness of the matching between image features and synthesized model features can be ensured. To explore the solution space eÆciently, we design the Annealed Genetic Algorithm (AGA) and Hierarchical Annealed Genetic Algorithm (HAGA) that searches the optimal solutions e«ectively by utilizing the characteristics of state space. Results of pose estimation on di«erent motion sequences demonstrate that the novel generative method can achieves viewpoint invariant 3D pose estimation.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zhaoGenerativeEstimation3D2007","container-title":"Computer Vision – ACCV 2007","DOI":"10.1007/978-3-540-76386-4_39","event-place":"Berlin, Heidelberg","ISBN":"978-3-540-76385-7","issued":{"date-parts":[["2007"]]},"language":"en","note":"ZSCC:00011","page":"419-429","publisher":"Springer Berlin Heidelberg","publisher-place":"Berlin, Heidelberg","source":"DOI.org (Crossref)","title":"Generative Estimation of 3D Human Pose Using Shape Contexts Matching","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-540-76386-4_39","volume":"4843"},
  {"id":"zhaoGenerativeTracking3D2008","abstract":"We present a generative method for reconstructing 3D human motion from single images and monocular image sequences. Inadequate observation information in monocular images and the complicated nature of human motion make the 3D human pose reconstruction challenging. In order to mine more prior knowledge about human motion, we extract the motion subspace by performing conventional principle component analysis (PCA) on small sample set of motion capture data. In doing so, we also reduce the problem dimensionality so that the generative pose recovering can be performed more effectively. And, the extracted subspace is naturally hierarchical. This allows us to explore the solution space efﬁciently. We design an annealed genetic algorithm (AGA) and hierarchical annealed genetic algorithm (HAGA) for human motion analysis that searches the optimal solutions by utilizing the hierarchical characteristics of state space. In tracking scenario, we embed the evolutionary mechanism of AGA into the framework of evolution strategy for adapting the local characteristics of ﬁtness function. We adopt the robust shape contexts descriptor to construct the matching function. Our methods are demonstrated in different motion types and different image sequences. Results of human motion estimation show that our novel generative method can achieve viewpoint invariant 3D pose reconstruction.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zhaoGenerativeTracking3D2008","container-title":"Pattern Recognition","container-title-short":"Pattern Recognition","DOI":"10.1016/j.patcog.2008.01.004","ISSN":"00313203","issue":"8","issued":{"date-parts":[["2008",8]]},"language":"en","note":"ZSCC:00066","number":"8","page":"2470-2483","source":"DOI.org (Crossref)","title":"Generative tracking of 3D human motion by hierarchical annealed genetic algorithm","type":"article-journal","URL":"https://linkinghub.elsevier.com/retrieve/pii/S0031320308000071","volume":"41"},
  {"id":"zhaoGraFormerGraphOrientedTransformer2022","accessed":{"date-parts":[["2022",6,21]]},"author":[{"family":"Zhao","given":"Weixi"},{"family":"Wang","given":"Weiqiang"},{"family":"Tian","given":"Yunjie"}],"citation-key":"zhaoGraFormerGraphOrientedTransformer2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"20438-20447","source":"openaccess.thecvf.com","title":"GraFormer: Graph-Oriented Transformer for 3D Pose Estimation","title-short":"GraFormer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Zhao_GraFormer_Graph-Oriented_Transformer_for_3D_Pose_Estimation_CVPR_2022_paper.html"},
  {"id":"zhaoHumanBlurHuman2023","abstract":"We propose a method to estimate 3D human poses from substantially blurred images. The key idea is to tackle the inverse problem of image deblurring by modeling the forward problem with a 3D human model, a texture map, and a sequence of poses to describe human motion. The blurring process is then modeled by a temporal image aggregation step. Using a differentiable renderer, we can solve the inverse problem by backpropagating the pixel-wise reprojection error to recover the best human motion representation that explains a single or multiple input images. Since the image reconstruction loss alone is insufficient, we present additional regularization terms. To the best of our knowledge, we present the first method to tackle this problem. Our method consistently outperforms other methods on significantly blurry inputs since they lack one or multiple key functionalities that our method unifies, i.e. image deblurring with sub-frame accuracy and explicit 3D modeling of non-rigid human motion.","accessed":{"date-parts":[["2023",9,14]]},"author":[{"family":"Zhao","given":"Yiming"},{"family":"Rozumnyi","given":"Denys"},{"family":"Song","given":"Jie"},{"family":"Hilliges","given":"Otmar"},{"family":"Pollefeys","given":"Marc"},{"family":"Oswald","given":"Martin R."}],"citation-key":"zhaoHumanBlurHuman2023","DOI":"10.48550/arXiv.2303.17209","issued":{"date-parts":[["2023",8,29]]},"number":"arXiv:2303.17209","publisher":"arXiv","source":"arXiv.org","title":"Human from Blur: Human Pose Tracking from Blurry Images","title-short":"Human from Blur","type":"article","URL":"http://arxiv.org/abs/2303.17209"},
  {"id":"zhaoOnlineLearningDynamic2017","abstract":"Person re-identification receives increasing attentions in computer vision due to its potential applications in video surveillance. In order to alleviate wrong matches caused by misalignment or missing features among cameras, we propose to learn a multi-view gallery of frequently appearing objects in a relatively closed environment. The gallery contains appearance models of these objects from different cameras and viewpoints. The strength of the learned appearance models lies in that they are invariant to viewpoint and illumination changes. To automatically estimate the number of frequently appearing objects in the environment and update their appearance models online, we propose a dynamic gallery learning algorithm. We specifically build up two datasets to validate the effectiveness of our approach in realistic scenarios. Comparisons with benchmark methods demonstrate promising performance in accuracy and efficiency of re-identification.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhao","given":"Yanna"},{"family":"Zhao","given":"Xu"},{"family":"Xiang","given":"Zongjie"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zhaoOnlineLearningDynamic2017","container-title":"Multimedia Tools and Applications","container-title-short":"Multimed Tools Appl","DOI":"10.1007/s11042-015-3015-5","ISSN":"1380-7501, 1573-7721","issue":"1","issued":{"date-parts":[["2017",1]]},"language":"en","note":"ZSCC:00009","number":"1","page":"217-241","source":"DOI.org (Crossref)","title":"Online learning of dynamic multi-view gallery for person Re-identification","type":"article-journal","URL":"http://link.springer.com/10.1007/s11042-015-3015-5","volume":"76"},
  {"id":"zhaoPersonReidentificationEncoding2016","abstract":"Recognizing objects from disjoint camera views, known as person re-identification, is an important and challenging problem in the field of computer vision. Recent progress in person re-identification is due to new visual features and models that deal with cross-view differences. Existing appearance models focus on visual features in the normal sense, e.g., color histogram, Scale-invariant Feature Transform (SIFT) and Histogram of Oriented Gradients (HOG). In this paper, we propose a new appearance based method using the generative information of local image features and their encoding. In this paradigm, local image features which capture the color and structural cues of the human images are first extracted. A Gaussian Mixture Model (GMM) is then learned to approximate the generation process of these features. It provides a relatively comprehensive statistical representation. Finally, discriminative feature maps are obtained by calculating Free Energy Score Space (FESS) for GMM. The obtained feature maps are concatenated and encoded into a fixed-length feature vector for person re-identification. Our approach demonstrates promising performance on challenging datasets. It is also very practical: it has low computational cost both at training and testing. A GMM trained on images with different imaging conditions can be applied to other images without any significant loss in performance.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhao","given":"Yanna"},{"family":"Zhao","given":"Xu"},{"family":"Luo","given":"Ruotian"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zhaoPersonReidentificationEncoding2016","container-title":"Multimedia Tools and Applications","container-title-short":"Multimed Tools Appl","DOI":"10.1007/s11042-015-2503-y","ISSN":"1380-7501, 1573-7721","issue":"8","issued":{"date-parts":[["2016",4]]},"language":"en","note":"ZSCC:00006","number":"8","page":"4795-4813","source":"DOI.org (Crossref)","title":"Person Re-identification by encoding free energy feature maps","type":"article-journal","URL":"http://link.springer.com/10.1007/s11042-015-2503-y","volume":"75"},
  {"id":"zhaoPersonReidentificationFree2014","abstract":"Person re-identiﬁcation is an important and challenging computer vision problem. Recent progress in this area is due to new visual features and models that deals with crossview variations. Instead of working towards more complex models, we focus on low level features and their encoding. Low level features capturing the color and structural information are ﬁrst extracted from human images. Gaussian Mixture Model (GMM) is then employed to approximate the distribution of the features, providing a relatively comprehensive statistical representation. Finally, low level features are mapped to a space by computing free energy score of the GMM. The mapped features are encoded into a ﬁxed-length feature vector for person re-identiﬁcation. Extensive experiments are conducted on several public datasets. Comparisons with benchmark person re-identiﬁcation methods show the promising performance of our approach.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhao","given":"Yanna"},{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zhaoPersonReidentificationFree2014","container-title":"2014 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2014.7025496","event-place":"Paris, France","event-title":"2014 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-4799-5751-4","issued":{"date-parts":[["2014",10]]},"language":"en","note":"ZSCC:00003","page":"2452-2456","publisher":"IEEE","publisher-place":"Paris, France","source":"DOI.org (Crossref)","title":"Person re-identification by free energy score space encoding","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7025496/"},
  {"id":"zhaoPoseFormerV2ExploringFrequency2023","accessed":{"date-parts":[["2023",6,10]]},"author":[{"family":"Zhao","given":"Qitao"},{"family":"Zheng","given":"Ce"},{"family":"Liu","given":"Mengyuan"},{"family":"Wang","given":"Pichao"},{"family":"Chen","given":"Chen"}],"citation-key":"zhaoPoseFormerV2ExploringFrequency2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8877-8886","source":"openaccess.thecvf.com","title":"PoseFormerV2: Exploring Frequency Domain for Efficient and Robust 3D Human Pose Estimation","title-short":"PoseFormerV2","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_PoseFormerV2_Exploring_Frequency_Domain_for_Efficient_and_Robust_3D_Human_CVPR_2023_paper.html"},
  {"id":"zhaoSemanticGraphConvolutional2019","accessed":{"date-parts":[["2023",3,16]]},"author":[{"family":"Zhao","given":"Long"},{"family":"Peng","given":"Xi"},{"family":"Tian","given":"Yu"},{"family":"Kapadia","given":"Mubbasir"},{"family":"Metaxas","given":"Dimitris N."}],"citation-key":"zhaoSemanticGraphConvolutional2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"3425-3435","source":"openaccess.thecvf.com","title":"Semantic Graph Convolutional Networks for 3D Human Pose Regression","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Semantic_Graph_Convolutional_Networks_for_3D_Human_Pose_Regression_CVPR_2019_paper.html"},
  {"id":"zhaoSemanticGraphConvolutional2019a","accessed":{"date-parts":[["2021",11,3]]},"author":[{"family":"Zhao","given":"Long"},{"family":"Peng","given":"Xi"},{"family":"Tian","given":"Yu"},{"family":"Kapadia","given":"Mubbasir"},{"family":"Metaxas","given":"Dimitris N."}],"citation-key":"zhaoSemanticGraphConvolutional2019a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"3425-3435","source":"openaccess.thecvf.com","title":"Semantic Graph Convolutional Networks for 3D Human Pose Regression","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Zhao_Semantic_Graph_Convolutional_Networks_for_3D_Human_Pose_Regression_CVPR_2019_paper.html"},
  {"id":"zhaoTemporalSpatialLocalGaussian2010","abstract":"Within a discriminative framework for human pose estimation, modeling the mapping from feature space to pose space is challenging as we are required to handle the multimodal conditional distribution in a high-dimensional space. However, to build the mapping, current techniques usually involve a large set of training samples in the learning process but are limited in their capability to deal with multimodality. In this work, we propose a novel online sparse Gaussian Process (GP) regression model combining both temporal and spatial information. We exploit the fact that for a given test input, its output is mainly determined by the training samples potentially residing in its neighbor domain in the inputoutput uniﬁed space. This leads to a local mixture GP experts system, where the GP experts are deﬁned in the local neighborhoods with the variational covariance function adapting to the speciﬁc regions. For the nonlinear human motion series, we integrate the temporal and spatial experts into a seamless system to handle multimodality. All the local experts are deﬁned online within very small neighborhoods, so learning and inference are extremely eﬃcient. We conduct extensive experiments on the real HumanEva database to verify the eﬃcacy of the proposed model, obtaining signiﬁcant improvement against the previous models.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhao","given":"Xu"},{"family":"Fu","given":"Yun"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zhaoTemporalSpatialLocalGaussian2010","container-title":"Computer Vision – ACCV 2009","DOI":"10.1007/978-3-642-12307-8_34","event-place":"Berlin, Heidelberg","ISBN":"978-3-642-12306-1 978-3-642-12307-8","issued":{"date-parts":[["2010"]]},"language":"en","note":"ZSCC:00008","page":"364-373","publisher":"Springer Berlin Heidelberg","publisher-place":"Berlin, Heidelberg","source":"DOI.org (Crossref)","title":"Temporal-Spatial Local Gaussian Process Experts for Human Pose Estimation","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-642-12307-8_34","volume":"5994"},
  {"id":"zhaoTracking3DHuman2007","abstract":"In this study, we present an efﬁcient approach to recover 3D human motion from monocular image sequences in generative reconstruction framework. This approach is based on the extracting of motion base space. From the motion capture data with bothersome high dimension characteristic of human activity, we extract the motion base space in which human pose can be described essentially and concisely by a more controllable way. And then, the structure of this space corresponding to some special activities such as walking motion is explored with data clustering. For the single image, Gaussian mixture model is used to generate the candidates of 3D pose. The shape context is the common descriptor of image silhouette feature and synthetical feature of human model. We get the shortlist of 3D poses by measuring the shape contexts matching cost between image feature and the synthetical features. In tracking situation, an AR model trained by the example sequences produces almost accurate pose predictions. Experiments demonstrate that the proposed approach works well.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zhaoTracking3DHuman2007","container-title":"2007 IEEE Workshop on Applications of Computer Vision (WAC V '07)","DOI":"10.1109/WACV.2007.60","event-place":"Austin, TX, USA","event-title":"2007 IEEE Workshop on Applications of Computer Vision (WAC V '07)","ISBN":"978-0-7695-2794-9","ISSN":"1550-5790","issued":{"date-parts":[["2007",2]]},"language":"en","note":"ZSCC:00008","page":"39-39","publisher":"IEEE","publisher-place":"Austin, TX, USA","source":"DOI.org (Crossref)","title":"Tracking 3D Human Motion in Compact Base Space","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/4118768/"},
  {"id":"zheng3DHumanPose2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Zheng","given":"Ce"},{"family":"Zhu","given":"Sijie"},{"family":"Mendieta","given":"Matias"},{"family":"Yang","given":"Taojiannan"},{"family":"Chen","given":"Chen"},{"family":"Ding","given":"Zhengming"}],"citation-key":"zheng3DHumanPose2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11656-11665","source":"openaccess.thecvf.com","title":"3D Human Pose Estimation With Spatial and Temporal Transformers","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Zheng_3D_Human_Pose_Estimation_With_Spatial_and_Temporal_Transformers_ICCV_2021_paper.html"},
  {"id":"zhengFeatEREfficientNetwork2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Zheng","given":"Ce"},{"family":"Mendieta","given":"Matias"},{"family":"Yang","given":"Taojiannan"},{"family":"Qi","given":"Guo-Jun"},{"family":"Chen","given":"Chen"}],"citation-key":"zhengFeatEREfficientNetwork2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"13945-13954","source":"openaccess.thecvf.com","title":"FeatER: An Efficient Network for Human Reconstruction via Feature Map-Based TransformER","title-short":"FeatER","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_FeatER_An_Efficient_Network_for_Human_Reconstruction_via_Feature_Map-Based_CVPR_2023_paper.html"},
  {"id":"ZhengHeQuanJuJuBuDuLiangXueXiDeRenTiMuBiaoZaiShiBiepdf2017","citation-key":"ZhengHeQuanJuJuBuDuLiangXueXiDeRenTiMuBiaoZaiShiBiepdf2017","container-title":"中国图象图形学报","issued":{"date-parts":[["2017"]]},"title":"整合全局-局部度量学习的人体目标再识别.pdf","type":"article-journal"},
  {"id":"zhengLearningVisibilityField2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Zheng","given":"Ruichen"},{"family":"Li","given":"Peng"},{"family":"Wang","given":"Haoqian"},{"family":"Yu","given":"Tao"}],"citation-key":"zhengLearningVisibilityField2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"216-226","source":"openaccess.thecvf.com","title":"Learning Visibility Field for Detailed 3D Human Reconstruction and Relighting","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_Learning_Visibility_Field_for_Detailed_3D_Human_Reconstruction_and_Relighting_CVPR_2023_paper.html"},
  {"id":"zhouHEMletsPoseLearning2019","accessed":{"date-parts":[["2021",11,2]]},"author":[{"family":"Zhou","given":"Kun"},{"family":"Han","given":"Xiaoguang"},{"family":"Jiang","given":"Nianjuan"},{"family":"Jia","given":"Kui"},{"family":"Lu","given":"Jiangbo"}],"citation-key":"zhouHEMletsPoseLearning2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"2344-2353","source":"openaccess.thecvf.com","title":"HEMlets Pose: Learning Part-Centric Heatmap Triplets for Accurate 3D Human Pose Estimation","title-short":"HEMlets Pose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Zhou_HEMlets_Pose_Learning_Part-Centric_Heatmap_Triplets_for_Accurate_3D_Human_ICCV_2019_paper.html"},
  {"id":"zhouParallelizedDeformablePart2016","abstract":"As a typical machine-learning based detection technique, deformable part models (DPM) achieve great success in detecting complex object categories. The heavy computational burden of DPM, however, severely restricts their utilization in many real world applications. In this work, we accelerate DPM via parallelization and hypothesis pruning. Firstly, we implement the original DPM approach on a GPU platform and parallelize it, making it 136 times faster than DPM release 5 without loss of detection accuracy. Furthermore, we use a mixture root template as a preﬁlter for hypothesis pruning, and achieve more than 200 times speedup over DPM release 5, apparently the fastest implementation of DPM yet. The performance of our method has been validated on the Pascal VOC 2007 and INRIA pedestrian datasets, and compared to other state-of-the-art techniques.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhou","given":"Zhi-Min"},{"family":"Zhao","given":"Xu"}],"citation-key":"zhouParallelizedDeformablePart2016","container-title":"Computational Visual Media","container-title-short":"Comp. Visual Media","DOI":"10.1007/s41095-016-0051-7","ISSN":"2096-0433, 2096-0662","issue":"3","issued":{"date-parts":[["2016",9]]},"language":"en","note":"ZSCC:00001","number":"3","page":"245-256","source":"DOI.org (Crossref)","title":"Parallelized deformable part models with effective hypothesis pruning","type":"article-journal","URL":"http://link.springer.com/10.1007/s41095-016-0051-7","volume":"2"},
  {"id":"zhuConstrainedOptimizationHuman2007","abstract":"A new 2-step method is presented for human upper-body pose estimation from depth sequences, in which coarse human part labeling takes place first, followed by more precise joint position estimation as the second phase. In the first step, a number of constraints are extracted from notable image features such as the head and torso. The problem of pose estimation is cast as that of label assignment with these constraints. Major parts of the human upper body are labeled by this process. The second step estimates joint positions optimally based on kinematic constraints using dense correspondences between depth profile and human model parts. The proposed framework is shown to overcome some issues of existing approaches for human pose tracking using similar types of data streams. Performance comparison with motion capture data is presented to demonstrate the accuracy of our approach.","author":[{"family":"Zhu","given":"Youding"},{"family":"Fujimura","given":"Kikuo"}],"citation-key":"zhuConstrainedOptimizationHuman2007","collection-title":"Lecture Notes in Computer Science","container-title":"Computer Vision – ACCV 2007","DOI":"10.1007/978-3-540-76386-4_38","editor":[{"family":"Yagi","given":"Yasushi"},{"family":"Kang","given":"Sing Bing"},{"family":"Kweon","given":"In So"},{"family":"Zha","given":"Hongbin"}],"event-place":"Berlin, Heidelberg","ISBN":"978-3-540-76386-4","issued":{"date-parts":[["2007"]]},"language":"en","page":"408-418","publisher":"Springer","publisher-place":"Berlin, Heidelberg","source":"Springer Link","title":"Constrained Optimization for Human Pose Estimation from Depth Sequences","type":"paper-conference"},
  {"id":"zhuSparseCodingLocal2011","abstract":"By extracting local spatial-temporal features from videos, many recently proposed approaches for action recognition achieve promising performance. The Bag-of-Words (BoW) model is commonly used in the approaches to obtain the video level representations. However, BoW model roughly assigns each feature vector to its closest visual word, therefore inevitably causing nontrivial quantization errors and impairing further improvements on classiﬁcation rates. To obtain a more accurate and discriminative representation, in this paper, we propose an approach for action recognition by encoding local 3D spatial-temporal gradient features within the sparse coding framework. In so doing, each local spatial-temporal feature is transformed to a linear combination of a few “atoms” in a trained dictionary. In addition, we also investigate the construction of the dictionary under the guidance of transfer learning. We collect a large set of diverse video clips of sport games and movies, from which a set of universal atoms composed of the dictionary are learned by an online learning strategy. We test our approach on KTH dataset and UCF sports dataset. Experimental results demonstrate that our approach outperforms the state-of-art techniques on KTH dataset and achieves the comparable performance on UCF sports dataset.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zhu","given":"Yan"},{"family":"Zhao","given":"Xu"},{"family":"Fu","given":"Yun"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zhuSparseCodingLocal2011","container-title":"Computer Vision – ACCV 2010","DOI":"10.1007/978-3-642-19309-5_51","event-place":"Berlin, Heidelberg","ISBN":"978-3-642-19308-8 978-3-642-19309-5","issued":{"date-parts":[["2011"]]},"language":"en","note":"ZSCC:00106","page":"660-671","publisher":"Springer Berlin Heidelberg","publisher-place":"Berlin, Heidelberg","source":"DOI.org (Crossref)","title":"Sparse Coding on Local Spatial-Temporal Volumes for Human Action Recognition","type":"paper-conference","URL":"http://link.springer.com/10.1007/978-3-642-19309-5_51","volume":"6493"},
  {"id":"zonghengzhouConnectedKcoverageProblem2004","abstract":"In overdeployed sensor networks, one approach to conserve energy is to keep only a small subset of sensors active at any instant. In this article, we consider the problem of selecting a minimum size connected K-cover, which is defined as a set of sensors M such that each point in the sensor network is \"covered\" by at least K different sensors in M, and the communication graph induced by M is connected. For the above optimization problem, we design a centralized approximation algorithm that delivers a near-optimal (within a factor of O(lg n)) solution, and present a distributed version of the algorithm. We also present a communication-efficient localized distributed algorithm which is empirically shown to perform well","author":[{"literal":"Zongheng Zhou"},{"family":"Das","given":"S."},{"family":"Gupta","given":"H."}],"citation-key":"zonghengzhouConnectedKcoverageProblem2004","container-title":"Proceedings. 13th International Conference on Computer Communications and Networks (IEEE Cat. No.04EX969)","DOI":"10.1109/ICCCN.2004.1401672","event-title":"Proceedings. 13th International Conference on Computer Communications and Networks (IEEE Cat. No.04EX969)","ISSN":"1095-2055","issued":{"date-parts":[["2004",10]]},"page":"373-378","source":"IEEE Xplore","title":"Connected K-coverage problem in sensor networks","type":"paper-conference"},
  {"id":"zouCLOTH4DDatasetClothed2023","accessed":{"date-parts":[["2023",9,4]]},"author":[{"family":"Zou","given":"Xingxing"},{"family":"Han","given":"Xintong"},{"family":"Wong","given":"Waikeung"}],"citation-key":"zouCLOTH4DDatasetClothed2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12847-12857","source":"openaccess.thecvf.com","title":"CLOTH4D: A Dataset for Clothed Human Reconstruction","title-short":"CLOTH4D","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Zou_CLOTH4D_A_Dataset_for_Clothed_Human_Reconstruction_CVPR_2023_paper.html"},
  {"id":"zouDetectCoherentMotions2015","abstract":"Coherent motion is a very common motion pattern in crowded scenes. Coherent Filter is a very effective and robust tool to detect coherent motions based on point trajectories, the performance of coherent filter depends on point trajectories’ property. In this work, we present a two-stage strategy to extract dense, accurate and longterm point trajectories from crowded scenes. The method includes a tracklets acquisition procedure and a tracklets association procedure. We use LDOF tracker to acquire dense tracklets, and then formulate tracklets association as a linear assignment problem (LAP). Experiments conducted on challenging crowd datasets show that our trajectories are very suitable for detecting coherent motions in crowded scenes.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zou","given":"Yi"},{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zouDetectCoherentMotions2015","container-title":"2015 IEEE International Conference on Image Processing (ICIP)","DOI":"10.1109/ICIP.2015.7351649","event-place":"Quebec City, QC, Canada","event-title":"2015 IEEE International Conference on Image Processing (ICIP)","ISBN":"978-1-4799-8339-1","issued":{"date-parts":[["2015",9]]},"language":"en","note":"ZSCC:00005","page":"4456-4460","publisher":"IEEE","publisher-place":"Quebec City, QC, Canada","source":"DOI.org (Crossref)","title":"Detect coherent motions in crowd scenes based on tracklets association","type":"paper-conference","URL":"http://ieeexplore.ieee.org/document/7351649/"},
  {"id":"zouEventHPEEventBased3D2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Zou","given":"Shihao"},{"family":"Guo","given":"Chuan"},{"family":"Zuo","given":"Xinxin"},{"family":"Wang","given":"Sen"},{"family":"Wang","given":"Pengyu"},{"family":"Hu","given":"Xiaoqin"},{"family":"Chen","given":"Shoushun"},{"family":"Gong","given":"Minglun"},{"family":"Cheng","given":"Li"}],"citation-key":"zouEventHPEEventBased3D2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"10996-11005","source":"openaccess.thecvf.com","title":"EventHPE: Event-Based 3D Human Pose and Shape Estimation","title-short":"EventHPE","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Zou_EventHPE_Event-Based_3D_Human_Pose_and_Shape_Estimation_ICCV_2021_paper.html"},
  {"id":"zouMeasuringCrowdCollectiveness2018","abstract":"As a scene-independent descriptor of crowd motions, crowd collectiveness quantiﬁes the degree of constituent individuals moving as a union in a crowd scene. An effective measurement on crowd collectiveness is of great importance for applications in surveillance of public safety, human dynamics, and other areas. To this end, we propose a novel framework to measure crowd collectiveness by combining macroscopic and microscopic motion consistency and deﬁne quantitatively the global and local consistency of crowd motions. The deﬁned global consistency represents the likelihood of pairwise individuals belonging to the same collective group, while the local consistency reﬂects the degree of conformity in a local region. Based on the proposed collectiveness measure, a new algorithm, named Group Mining, is proposed to detect collective groups from a crowd. We validate the effectiveness of the proposed method on several synthetic particle systems and a real world crowd database with humanlabeled collectiveness. Experimental results show that, comparing with the previous approaches, our collectiveness measure is more consistent with human perception, and the collective groups detected by our Group Mining algorithm are more accurate and robust.","accessed":{"date-parts":[["2023",4,1]]},"author":[{"family":"Zou","given":"Yi"},{"family":"Zhao","given":"Xu"},{"family":"Liu","given":"Yuncai"}],"citation-key":"zouMeasuringCrowdCollectiveness2018","container-title":"IEEE Transactions on Multimedia","container-title-short":"IEEE Trans. Multimedia","DOI":"10.1109/TMM.2018.2832601","ISSN":"1520-9210, 1941-0077","issue":"12","issued":{"date-parts":[["2018",12]]},"language":"en","note":"ZSCC:00002","number":"12","page":"3311-3323","source":"DOI.org (Crossref)","title":"Measuring Crowd Collectiveness by Macroscopic and Microscopic Motion Consistencies","type":"article-journal","URL":"https://ieeexplore.ieee.org/document/8355688/","volume":"20"},
  {"id":"zouModulatedGraphConvolutional2021","accessed":{"date-parts":[["2021",10,28]]},"author":[{"family":"Zou","given":"Zhiming"},{"family":"Tang","given":"Wei"}],"citation-key":"zouModulatedGraphConvolutional2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11477-11487","source":"openaccess.thecvf.com","title":"Modulated Graph Convolutional Network for 3D Human Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Zou_Modulated_Graph_Convolutional_Network_for_3D_Human_Pose_Estimation_ICCV_2021_paper.html"}
]
