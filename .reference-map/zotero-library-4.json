[
  {"id":"aichDataFreeClassIncrementalHand2023","abstract":"This paper investigates data-free class-incremental learning (DFCIL) for hand gesture recognition from 3D skeleton sequences. In this class-incremental learning (CIL) setting, while incrementally registering the new classes, we do not have access to the training samples (i.e. data-free) of the already known classes due to privacy. Existing DFCIL methods primarily focus on various forms of knowledge distillation for model inversion to mitigate catastrophic forgetting. Unlike SOTA methods, we delve deeper into the choice of the best samples for inversion. Inspired by the well-grounded theory of max-margin classification, we find that the best samples tend to lie close to the approximate decision boundary within a reasonable margin. To this end, we propose BOAT-MI -- a simple and effective boundary-aware prototypical sampling mechanism for model inversion for DFCIL. Our sampling scheme outperforms SOTA methods significantly on two 3D skeleton gesture datasets, the publicly available SHREC 2017, and EgoGesture3D -- which we extract from a publicly available RGBD dataset. Both our codebase and the EgoGesture3D skeleton dataset are publicly available: https://github.com/humansensinglab/dfcil-hgr","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Aich","given":"Shubhra"},{"family":"Ruiz-Santaquiteria","given":"Jesus"},{"family":"Lu","given":"Zhenyu"},{"family":"Garg","given":"Prachi"},{"family":"Joseph","given":"K. J."},{"family":"Garcia","given":"Alvaro Fernandez"},{"family":"Balasubramanian","given":"Vineeth N."},{"family":"Kin","given":"Kenrick"},{"family":"Wan","given":"Chengde"},{"family":"Camgoz","given":"Necati Cihan"},{"family":"Ma","given":"Shugao"},{"family":"De la Torre","given":"Fernando"}],"citation-key":"aichDataFreeClassIncrementalHand2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"20958-20967","source":"openaccess.thecvf.com","title":"Data-Free Class-Incremental Hand Gesture Recognition","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Aich_Data-Free_Class-Incremental_Hand_Gesture_Recognition_ICCV_2023_paper.html"},
  {"id":"ambergOptimalStepNonrigid2007","abstract":"We show how to extend the ICP framework to nonrigid registration, while retaining the convergence properties of the original algorithm. The resulting optimal step nonrigid ICP framework allows the use of different regularisations, as long as they have an adjustable stiffness parameter. The registration loops over a series of decreasing stiffness weights, and incrementally deforms the template towards the target, recovering the whole range of global and local deformations. To find the optimal deformation for a given stiffness, optimal iterative closest point steps are used. Preliminary correspondences are estimated by a nearest-point search. Then the optimal deformation of the template for these fixed correspondences and the active stiffness is calculated. Afterwards the process continues with new correspondences found by searching from the displaced template vertices. We present an algorithm using a locally affine regularisation which assigns an affine transformation to each vertex and minimises the difference in the transformation of neighbouring vertices. It is shown that for this regularisation the optimal deformation for fixed correspondences and fixed stiffness can be determined exactly and efficiently. The method succeeds for a wide range of initial conditions, and handles missing data robustly. It is compared qualitatively and quantitatively to other algorithms using synthetic examples and real world data.","accessed":{"date-parts":[["2024",11,5]]},"author":[{"family":"Amberg","given":"Brian"},{"family":"Romdhani","given":"Sami"},{"family":"Vetter","given":"Thomas"}],"citation-key":"ambergOptimalStepNonrigid2007","container-title":"2007 IEEE Conference on Computer Vision and Pattern Recognition","DOI":"10.1109/CVPR.2007.383165","event-title":"2007 IEEE Conference on Computer Vision and Pattern Recognition","ISSN":"1063-6919","issued":{"date-parts":[["2007",6]]},"page":"1-8","source":"IEEE Xplore","title":"Optimal Step Nonrigid ICP Algorithms for Surface Registration","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/4270190"},
  {"id":"ansuiniObjectActionSame2008","abstract":"Objects can be grasped in several ways due to their physical properties, the context surrounding the object, and the goal of the grasping agent. The aim of the present study was to investigate whether the prior-to-contact grasping kinematics of the same object vary as a result of different goals of the person grasping it. Subjects were requested to reach toward and grasp a bottle filled with water, and then complete one of the following tasks: (1) Grasp it without performing any subsequent action; (2) Lift and throw it; (3) Pour the water into a container; (4) Place it accurately on a target area; (5) Pass it to another person. We measured the angular excursions at both metacarpal-phalangeal (mcp) and proximal interphalangeal (pip) joints of all digits, and abduction angles of adjacent digit pairs by means of resistive sensors embedded in a glove. The results showed that the presence and the nature of the task to be performed following grasping affect the positioning of the fingers during the reaching phase. We contend that a one-to-one association between a sensory stimulus and a motor response does not capture all the aspects involved in grasping. The theoretical approach within which we frame our discussion considers internal models of anticipatory control which may provide a suitable explanation of our results.","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Ansuini","given":"Caterina"},{"family":"Giosa","given":"Livia"},{"family":"Turella","given":"Luca"},{"family":"Alto√®","given":"Gianmarco"},{"family":"Castiello","given":"Umberto"}],"citation-key":"ansuiniObjectActionSame2008","container-title":"Experimental Brain Research","container-title-short":"Exp Brain Res","DOI":"10.1007/s00221-007-1136-4","ISSN":"1432-1106","issue":"1","issued":{"date-parts":[["2008",2,1]]},"language":"en","page":"111-119","source":"Springer Link","title":"An object for an action, the same object for other actions: effects on hand shaping","title-short":"An object for an action, the same object for other actions","type":"article-journal","URL":"https://doi.org/10.1007/s00221-007-1136-4","volume":"185"},
  {"id":"aziziOcclusionHandling3D","abstract":"Understanding human behavior fundamentally relies on accurate 3D human pose estimation. Graph Convolutional Networks (GCNs) have recently shown promising advancements, delivering state-of-the-art performance with rather lightweight architectures. In the context of graph-structured data, leveraging the eigenvectors of the graph Laplacian matrix for positional encoding is effective. Yet, the approach does not specify how to handle scenarios where edges in the input graph are missing. To this end, we propose a novel positional encoding technique, PerturbPE, that extracts consistent and regular components from the eigenbasis. Our method involves applying multiple perturbations and taking their average to extract the consistent and regular component from the eigenbasis. PerturbPE leverages the Rayleigh-Schrodinger Perturbation Theorem (RSPT) for calculating the perturbed eigenvectors. Employing this labeling technique enhances the robustness and generalizability of the model. Our results support our theoretical findings, e.g. our experimental analysis observed a performance enhancement of up to 12% on the Human3.6M dataset in instances where occlusion resulted in the absence of one edge. Furthermore, our novel approach significantly enhances performance in scenarios where two edges are missing, setting a new benchmark for state-of-the-art.","author":[{"family":"Azizi","given":"Niloofar"},{"family":"Fayyaz","given":"Mohsen"},{"family":"Bischof","given":"Horst"}],"citation-key":"aziziOcclusionHandling3D","language":"en","source":"Zotero","title":"Occlusion Handling in 3D Human Pose Estimation with Perturbed Positional Encoding","type":"article-journal"},
  {"id":"baoUncertaintyawareStateSpace2023","abstract":"Hand trajectory forecasting from egocentric views is crucial for enabling a prompt understanding of human intentions when interacting with AR/VR systems. However, existing methods handle this problem in a 2D image space which is inadequate for 3D real-world applications. In this paper, we set up an egocentric 3D hand trajectory forecasting task that aims to predict hand trajectories in a 3D space from early observed RGB videos in a first-person view. To fulfill this goal, we propose an uncertainty-aware state space Transformer (USST) that takes the merits of the attention mechanism and aleatoric uncertainty within the framework of the classical state-space model. The model can be further enhanced by the velocity constraint and visual prompt tuning (VPT) on large vision transformers. Moreover, we develop an annotation workflow to collect 3D hand trajectories with high quality. Experimental results on H2O and EgoPAT3D datasets demonstrate the superiority of USST for both 2D and 3D trajectory forecasting. The code and datasets are publicly released: https://actionlab-cv.github.io/EgoHandTrajPred.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Bao","given":"Wentao"},{"family":"Chen","given":"Lele"},{"family":"Zeng","given":"Libing"},{"family":"Li","given":"Zhong"},{"family":"Xu","given":"Yi"},{"family":"Yuan","given":"Junsong"},{"family":"Kong","given":"Yu"}],"citation-key":"baoUncertaintyawareStateSpace2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"13702-13711","source":"openaccess.thecvf.com","title":"Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Bao_Uncertainty-aware_State_Space_Transformer_for_Egocentric_3D_Hand_Trajectory_Forecasting_ICCV_2023_paper.html"},
  {"id":"brahmbhattContactDBAnalyzingPredicting2019","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Brahmbhatt","given":"Samarth"},{"family":"Ham","given":"Cusuh"},{"family":"Kemp","given":"Charles C."},{"family":"Hays","given":"James"}],"citation-key":"brahmbhattContactDBAnalyzingPredicting2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"8709-8719","source":"openaccess.thecvf.com","title":"ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging","title-short":"ContactDB","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Brahmbhatt_ContactDB_Analyzing_and_Predicting_Grasp_Contact_via_Thermal_Imaging_CVPR_2019_paper.html"},
  {"id":"brahmbhattContactGraspFunctionalMultifinger2019","abstract":"Grasping and manipulating objects is an important human skill. Since most objects are designed to be manipulated by human hands, anthropomorphic hands can enable richer human-robot interaction. Desirable grasps are not only stable, but also functional: they enable post-grasp actions with the object. However, functional grasp synthesis for high degree-of-freedom anthropomorphic hands from object shape alone is challenging because of the large optimization space. We present ContactGrasp, a framework for functional grasp synthesis from object shape and contact on the object surface. Contact can be manually specified or obtained through demonstrations. Our contact representation is object-centric and allows functional grasp synthesis even for hand models different than the one used for demonstration. Using a dataset of contact demonstrations from humans grasping diverse household objects, we synthesize functional grasps for three hand models and two functional intents. The project webpage is https://contactdb.cc.gatech.edu/contactgrasp.html.","accessed":{"date-parts":[["2024",11,22]]},"author":[{"family":"Brahmbhatt","given":"Samarth"},{"family":"Handa","given":"Ankur"},{"family":"Hays","given":"James"},{"family":"Fox","given":"Dieter"}],"citation-key":"brahmbhattContactGraspFunctionalMultifinger2019","DOI":"10.48550/arXiv.1904.03754","issued":{"date-parts":[["2019",7,25]]},"number":"arXiv:1904.03754","publisher":"arXiv","source":"arXiv.org","title":"ContactGrasp: Functional Multi-finger Grasp Synthesis from Contact","title-short":"ContactGrasp","type":"article","URL":"http://arxiv.org/abs/1904.03754"},
  {"id":"brahmbhattContactPoseDatasetGrasps2020","abstract":"Grasping is natural for humans. However, it involves complex hand configurations and soft tissue deformation that can result in complicated regions of contact between the hand and the object. Understanding and modeling this contact can potentially improve hand models, AR/VR experiences, and robotic grasping. Yet, we currently lack datasets of hand-object contact paired with other data modalities, which is crucial for developing and evaluating contact modeling techniques. We introduce ContactPose, the first dataset of hand-object contact paired with hand pose, object pose, and RGB-D images. ContactPose has 2306 unique grasps of 25 household objects grasped with 2 functional intents by 50 participants, and more than 2.9 M RGB-D grasp images. Analysis of ContactPose data reveals interesting relationships between hand pose and contact. We use this data to rigorously evaluate various data representations, heuristics from the literature, and learning methods for contact modeling. Data, code, and trained models are available at https://contactpose.cc.gatech.edu.","author":[{"family":"Brahmbhatt","given":"Samarth"},{"family":"Tang","given":"Chengcheng"},{"family":"Twigg","given":"Christopher D."},{"family":"Kemp","given":"Charles C."},{"family":"Hays","given":"James"}],"citation-key":"brahmbhattContactPoseDatasetGrasps2020","container-title":"Computer Vision ‚Äì ECCV 2020","DOI":"10.1007/978-3-030-58601-0_22","editor":[{"family":"Vedaldi","given":"Andrea"},{"family":"Bischof","given":"Horst"},{"family":"Brox","given":"Thomas"},{"family":"Frahm","given":"Jan-Michael"}],"event-place":"Cham","ISBN":"978-3-030-58601-0","issued":{"date-parts":[["2020"]]},"language":"en","page":"361-378","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"ContactPose: A Dataset of Grasps with Object Contact and Hand Pose","title-short":"ContactPose","type":"paper-conference"},
  {"id":"castielloNeuroscienceGrasping2005","abstract":"Considerable advances in our knowledge of human and non-human primate grasping control have been made during the past decade, using a combination of behavioural, neuroimaging and electrophysiological approaches. As a result, the neural circuitry and mechanisms of grasping are being elucidated. However, few attempts have been made to reconcile findings across species.Various experiments using kinematic techniques indicate that the mechanics of grasping in humans vary depending on object attributes such as fragility, size, shape, texture and weight. By contrast, kinematic studies in monkeys have been limited and confined to the testing of an object's size and shape.Single-unit physiology studies indicate that grasping might be represented by neurons in a network of brain areas, including the motor, premotor and parietal cortices. Because these areas contain representations of hand actions and the structures of objects, it has been suggested that their integrity might be crucial for successful grasping.Findings from patients with brain damage who have difficulty in grasping objects are difficult to reconcile with neurophysiological findings, as the patients' lesions are confined to regions that, in monkeys, do not seem to be involved in grasping-related visuomotor transformations.Recent positron emission tomography and functional MRI studies in humans have indicated possible human homologues of the brain regions that seem to be involved in grasping in monkeys. However, because of the difficulty of studying real grasping in the neuroimaging environment, many laboratories have taken to studying rather unnatural tasks. So, there are inconsistencies in the experimental protocols that make these results difficult to compare and interpret.One consistent region that has been identified as being involved in grasping tasks is the human homologue of the monkey anterior intraparietal area. However, in most of the studies that show such activation, participants were constrained to a single type of grasp (a precision grip), which raises the question of whether different grasping patterns are represented in this area.Contextual information that is involved in the unfolding of the grasping action (for example, using the same object for different purposes) has been largely neglected. A fundamental step is to uncover whether and to what extent contextual factors affect movement organization in humans and monkeys.To make progress in this field, it would be useful to implement a coordinated series of experiments in which similar protocols are applied to monkeys and humans, using various techniques. This multi-pronged approach should ideally combine functional imaging with MRI-compatible electrophysiological and kinematic recordings.","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Castiello","given":"Umberto"}],"citation-key":"castielloNeuroscienceGrasping2005","container-title":"Nature Reviews Neuroscience","container-title-short":"Nat Rev Neurosci","DOI":"10.1038/nrn1744","ISSN":"1471-0048","issue":"9","issued":{"date-parts":[["2005",9]]},"language":"en","license":"2005 Springer Nature Limited","page":"726-736","publisher":"Nature Publishing Group","source":"www.nature.com","title":"The neuroscience of grasping","type":"article-journal","URL":"https://www.nature.com/articles/nrn1744","volume":"6"},
  {"id":"chaoDexYCBBenchmarkCapturing2021","accessed":{"date-parts":[["2024",9,30]]},"author":[{"family":"Chao","given":"Yu-Wei"},{"family":"Yang","given":"Wei"},{"family":"Xiang","given":"Yu"},{"family":"Molchanov","given":"Pavlo"},{"family":"Handa","given":"Ankur"},{"family":"Tremblay","given":"Jonathan"},{"family":"Narang","given":"Yashraj S."},{"family":"Van Wyk","given":"Karl"},{"family":"Iqbal","given":"Umar"},{"family":"Birchfield","given":"Stan"},{"family":"Kautz","given":"Jan"},{"family":"Fox","given":"Dieter"}],"citation-key":"chaoDexYCBBenchmarkCapturing2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"9044-9053","source":"openaccess.thecvf.com","title":"DexYCB: A Benchmark for Capturing Hand Grasping of Objects","title-short":"DexYCB","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Chao_DexYCB_A_Benchmark_for_Capturing_Hand_Grasping_of_Objects_CVPR_2021_paper.html"},
  {"id":"chaText2HOITextguided3D2024","abstract":"This paper introduces the first text-guided work for generating the sequence of hand-object interaction in 3D. The main challenge arises from the lack of labeled data where existing ground-truth datasets are nowhere near generalizable in interaction type and object category which inhibits the modeling of diverse 3D hand-object interaction with the correct physical implication (e.g. contacts and semantics) from text prompts. To address this challenge we propose to decompose the interaction generation task into two subtasks: hand-object contact generation; and hand-object motion generation. For contact generation a VAE-based network takes as input a text and an object mesh and generates the probability of contacts between the surfaces of hands and the object during the interaction. The network learns a variety of local geometry structure of diverse objects that is independent of the objects' category and thus it is applicable to general objects. For motion generation a Transformer-based diffusion model utilizes this 3D contact map as a strong prior for generating physically plausible hand-object motion as a function of text prompts by learning from the augmented labeled dataset; where we annotate text labels from many existing 3D hand and object motion data. Finally we further introduce a hand refiner module that minimizes the distance between the object surface and hand joints to improve the temporal stability of the object-hand contacts and to suppress the penetration artifacts. In the experiments we demonstrate that our method can generate more realistic and diverse interactions compared to other baseline methods. We also show that our method is applicable to unseen objects. We will release our model and newly labeled data as a strong foundation for future research. Codes and data are available in: https://github.com/JunukCha/Text2HOI.","accessed":{"date-parts":[["2024",10,22]]},"author":[{"family":"Cha","given":"Junuk"},{"family":"Kim","given":"Jihyeon"},{"family":"Yoon","given":"Jae Shin"},{"family":"Baek","given":"Seungryul"}],"citation-key":"chaText2HOITextguided3D2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"1577-1585","source":"openaccess.thecvf.com","title":"Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction","title-short":"Text2HOI","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Cha_Text2HOI_Text-guided_3D_Motion_Generation_for_Hand-Object_Interaction_CVPR_2024_paper.html"},
  {"id":"chenFSNetFastShapebased2021","abstract":"In this paper, we focus on category-level 6D pose and size estimation from monocular RGB-D image. Previous methods suffer from inefficient category-level pose feature extraction which leads to low accuracy and inference speed. To tackle this problem, we propose a fast shape-based network (FS-Net) with efficient category-level feature extraction for 6D pose estimation. First, we design an orientation aware autoencoder with 3D graph convolution for latent feature extraction. The learned latent feature is insensitive to point shift and object size thanks to the shift and scale-invariance properties of the 3D graph convolution. Then, to efficiently decode category-level rotation information from the latent feature, we propose a novel decoupled rotation mechanism that employs two decoders to complementarily access the rotation information. Meanwhile, we estimate translation and size by two residuals, which are the difference between the mean of object points and ground truth translation, and the difference between the mean size of the category and ground truth size, respectively. Finally, to increase the generalization ability of FS-Net, we propose an online box-cage based 3D deformation mechanism to augment the training data. Extensive experiments on two benchmark datasets show that the proposed method achieves state-of-the-art performance in both category- and instance-level 6D object pose estimation. Especially in category-level pose estimation, without extra synthetic data, our method outperforms existing methods by 6.3% on the NOCS-REAL dataset.","accessed":{"date-parts":[["2024",11,6]]},"author":[{"family":"Chen","given":"Wei"},{"family":"Jia","given":"Xi"},{"family":"Chang","given":"Hyung Jin"},{"family":"Duan","given":"Jinming"},{"family":"Shen","given":"Linlin"},{"family":"Leonardis","given":"Ales"}],"citation-key":"chenFSNetFastShapebased2021","DOI":"10.48550/arXiv.2103.07054","issued":{"date-parts":[["2021",6,6]]},"number":"arXiv:2103.07054","publisher":"arXiv","source":"arXiv.org","title":"FS-Net: Fast Shape-based Network for Category-Level 6D Object Pose Estimation with Decoupled Rotation Mechanism","title-short":"FS-Net","type":"article","URL":"http://arxiv.org/abs/2103.07054"},
  {"id":"cheng4Diff3DAwareDiffusion","abstract":"We present 4Diff, a 3D-aware diffusion model addressing the exo-to-ego viewpoint translation task ‚Äî generating first-person (egocentric) view images from the corresponding third-person (exocentric) images. Building on the diffusion model‚Äôs ability to generate photorealistic images, we propose a transformer-based diffusion model that incorporates geometry priors through two mechanisms: (i) egocentric point cloud rasterization and (ii) 3D-aware rotary cross-attention. Egocentric point cloud rasterization converts the input exocentric image into an egocentric layout, which is subsequently used by a diffusion image transformer. As a component of the diffusion transformer‚Äôs denoiser block, the 3D-aware rotary cross-attention further incorporates 3D information and semantic features from the source exocentric view. Our 4Diff achieves stateof-the-art results on the challenging and diverse Ego-Exo4D multiview dataset and exhibits robust generalization to novel environments not encountered during training. Our code, processed data, and pretrained models are publicly available at https://klauscc.github.io/4diff.","author":[{"family":"Cheng","given":"Feng"},{"family":"Luo","given":"Mi"},{"family":"Wang","given":"Huiyu"},{"family":"Dimakis","given":"Alex"},{"family":"Torresani","given":"Lorenzo"},{"family":"Bertasius","given":"Gedas"},{"family":"Grauman","given":"Kristen"}],"citation-key":"cheng4Diff3DAwareDiffusion","language":"en","source":"Zotero","title":"4Diff: 3D-Aware Diffusion Model for Third-to-First Viewpoint Translation","type":"article-journal"},
  {"id":"chengHandDAGTDenoisingAdaptive2024","abstract":"The extraction of keypoint positions from input hand frames, known as 3D hand pose estimation, is crucial for various human-computer interaction applications. However, current approaches often struggle with the dynamic nature of self-occlusion of hands and intra-occlusion with interacting objects. To address this challenge, this paper proposes the Denoising Adaptive Graph Transformer, HandDAGT, for hand pose estimation. The proposed HandDAGT leverages a transformer structure to thoroughly explore effective geometric features from input patches. Additionally, it incorporates a novel attention mechanism to adaptively weigh the contribution of kinematic correspondence and local geometric features for the estimation of specific keypoints. This attribute enables the model to adaptively employ kinematic and local information based on the occlusion situation, enhancing its robustness and accuracy. Furthermore, we introduce a novel denoising training strategy aimed at improving the model's robust performance in the face of occlusion challenges. Experimental results show that the proposed model significantly outperforms the existing methods on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandDAGT.","accessed":{"date-parts":[["2024",10,22]]},"author":[{"family":"Cheng","given":"Wencan"},{"family":"Kim","given":"Eunji"},{"family":"Ko","given":"Jong Hwan"}],"citation-key":"chengHandDAGTDenoisingAdaptive2024","DOI":"10.48550/arXiv.2407.20542","issued":{"date-parts":[["2024",7,30]]},"number":"arXiv:2407.20542","publisher":"arXiv","source":"arXiv.org","title":"HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation","title-short":"HandDAGT","type":"article","URL":"http://arxiv.org/abs/2407.20542"},
  {"id":"chengHandR2N2Iterative3D2023","abstract":"3D hand pose estimation is a critical task in various human-computer interaction applications. Numerous deep learning based estimation models in this domain have been actively explored. However, the existing models follows a non-recurrent scheme and thus require complex architectures or redundant parameters in order to achieve acceptable model capacity. To tackle this limitation, this paper proposes HandR2N2, a compact neural network that iteratively regresses the hand pose using a novel residual recurrent unit. The recurrent design allows recursive exploitation of partial layers to gradually optimize previously estimated joint locations. In addition, we exploit graph reasoning to capture kinematic dependencies between joints for better performance. Experimental results show that the proposed model significantly outperforms the existing methods on three hand pose benchmark datasets in terms of both accuracy and efficiency. Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandR2N2.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Cheng","given":"Wencan"},{"family":"Ko","given":"Jong Hwan"}],"citation-key":"chengHandR2N2Iterative3D2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"20904-20913","source":"openaccess.thecvf.com","title":"HandR2N2: Iterative 3D Hand Pose Estimation Using a Residual Recurrent Neural Network","title-short":"HandR2N2","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_HandR2N2_Iterative_3D_Hand_Pose_Estimation_Using_a_Residual_Recurrent_ICCV_2023_paper.html"},
  {"id":"chengRicher2DUnderstanding2023","accessed":{"date-parts":[["2024",10,4]]},"author":[{"family":"Cheng","given":"Tianyi"},{"family":"Shan","given":"Dandan"},{"family":"Hassen","given":"Ayda"},{"family":"Higgins","given":"Richard"},{"family":"Fouhey","given":"David"}],"citation-key":"chengRicher2DUnderstanding2023","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2023",12,15]]},"language":"en","page":"30453-30465","source":"proceedings.neurips.cc","title":"Towards A Richer 2D Understanding of Hands at Scale","type":"article-journal","URL":"https://proceedings.neurips.cc/paper_files/paper/2023/hash/612a7948f3294a02a63d970566ca8536-Abstract-Conference.html","volume":"36"},
  {"id":"chenGSDFGeometryDrivenSigned2023","abstract":"Signed distance functions (SDFs) is an attractive framework that has recently shown promising results for 3D shape reconstruction from images. SDFs seamlessly generalize to different shape resolutions and topologies but lack explicit modelling of the underlying 3D geometry. In this work, we exploit the hand structure and use it as guidance for SDF-based shape reconstruction. In particular, we address reconstruction of hands and manipulated objects from monocular RGB images. To this end, we estimate poses of hands and objects and use them to guide 3D reconstruction. More specifically, we predict kinematic chains of pose transformations and align SDFs with highly-articulated hand poses. We improve the visual features of 3D points with geometry alignment and further leverage temporal information to enhance the robustness to occlusion and motion blurs. We conduct extensive experiments on the challenging ObMan and DexYCB benchmarks and demonstrate significant improvements of the proposed method over the state of the art.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Chen","given":"Zerui"},{"family":"Chen","given":"Shizhe"},{"family":"Schmid","given":"Cordelia"},{"family":"Laptev","given":"Ivan"}],"citation-key":"chenGSDFGeometryDrivenSigned2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12890-12900","source":"openaccess.thecvf.com","title":"gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction","title-short":"gSDF","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Chen_gSDF_Geometry-Driven_Signed_Distance_Functions_for_3D_Hand-Object_Reconstruction_CVPR_2023_paper.html"},
  {"id":"chenHandAvatarFreePose","author":[{"family":"Chen","given":"Xingyu"},{"family":"Wang","given":"Baoyuan"},{"family":"Shum","given":"Heung-Yeung"},{"family":"Ai","given":"Xiaobing"}],"citation-key":"chenHandAvatarFreePose","language":"en","source":"Zotero","title":"Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video ‚Äì Supplementary Material","type":"article-journal"},
  {"id":"chenHandAvatarFreePose2023","abstract":"We present HandAvatar, a novel representation for hand animation and rendering, which can generate smoothly compositional geometry and self-occlusion-aware texture. Specifically, we first develop a MANO-HD model as a high-resolution mesh topology to fit personalized hand shapes. Sequentially, we decompose hand geometry into per-bone rigid parts, and then re-compose paired geometry encodings to derive an across-part consistent occupancy field. As for texture modeling, we propose a self-occlusion-aware shading field (SelF). In SelF, drivable anchors are paved on the MANO-HD surface to record albedo information under a wide variety of hand poses. Moreover, directed soft occupancy is designed to describe the ray-to-surface relation, which is leveraged to generate an illumination field for the disentanglement of pose-independent albedo and pose-dependent illumination. Trained from monocular video data, our HandAvatar can perform free-pose hand animation and rendering while at the same time achieving superior appearance fidelity. We also demonstrate that HandAvatar provides a route for hand appearance editing.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Chen","given":"Xingyu"},{"family":"Wang","given":"Baoyuan"},{"family":"Shum","given":"Heung-Yeung"}],"citation-key":"chenHandAvatarFreePose2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8683-8693","source":"openaccess.thecvf.com","title":"Hand Avatar: Free-Pose Hand Animation and Rendering From Monocular Video","title-short":"Hand Avatar","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Hand_Avatar_Free-Pose_Hand_Animation_and_Rendering_From_Monocular_Video_CVPR_2023_paper.html"},
  {"id":"chenMobReconMobileFriendlyHand2022","accessed":{"date-parts":[["2024",9,30]]},"author":[{"family":"Chen","given":"Xingyu"},{"family":"Liu","given":"Yufeng"},{"family":"Dong","given":"Yajiao"},{"family":"Zhang","given":"Xiong"},{"family":"Ma","given":"Chongyang"},{"family":"Xiong","given":"Yanmin"},{"family":"Zhang","given":"Yuan"},{"family":"Guo","given":"Xiaoyan"}],"citation-key":"chenMobReconMobileFriendlyHand2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"20544-20554","source":"openaccess.thecvf.com","title":"MobRecon: Mobile-Friendly Hand Mesh Reconstruction From Monocular Image","title-short":"MobRecon","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Chen_MobRecon_Mobile-Friendly_Hand_Mesh_Reconstruction_From_Monocular_Image_CVPR_2022_paper.html"},
  {"id":"chenModelbased3DHand2021","abstract":"Reconstructing a 3D hand from a single-view RGB image is challenging due to various hand configurations and depth ambiguity. To reliably reconstruct a 3D hand from a monocular image, most state-of-the-art methods heavily rely on 3D annotations at the training stage, but obtaining 3D annotations is expensive. To alleviate reliance on labeled training data, we propose S2HAND, a self-supervised 3D hand reconstruction network that can jointly estimate pose, shape, texture, and the camera viewpoint. Specifically, we obtain geometric cues from the input image through easily accessible 2D detected keypoints. To learn an accurate hand reconstruction model from these noisy geometric cues, we utilize the consistency between 2D and 3D representations and propose a set of novel losses to rationalize outputs of the neural network. For the first time, we demonstrate the feasibility of training an accurate 3D hand reconstruction network without relying on manual annotations. Our experiments show that the proposed method achieves comparable performance with recent fully-supervised methods while using fewer supervision data.","accessed":{"date-parts":[["2024",11,20]]},"author":[{"family":"Chen","given":"Yujin"},{"family":"Tu","given":"Zhigang"},{"family":"Kang","given":"Di"},{"family":"Bao","given":"Linchao"},{"family":"Zhang","given":"Ying"},{"family":"Zhe","given":"Xuefei"},{"family":"Chen","given":"Ruizhi"},{"family":"Yuan","given":"Junsong"}],"citation-key":"chenModelbased3DHand2021","DOI":"10.48550/arXiv.2103.11703","issued":{"date-parts":[["2021",3,22]]},"number":"arXiv:2103.11703","publisher":"arXiv","source":"arXiv.org","title":"Model-based 3D Hand Reconstruction via Self-Supervised Learning","type":"article","URL":"http://arxiv.org/abs/2103.11703"},
  {"id":"chenModelbased3DHand2021a","abstract":"Reconstructing a 3D hand from a single-view RGB image is challenging due to various hand configurations and depth ambiguity. To reliably reconstruct a 3D hand from a monocular image, most state-of-the-art methods heavily rely on 3D annotations at the training stage, but obtaining 3D annotations is expensive. To alleviate reliance on labeled training data, we propose S2HAND, a self-supervised 3D hand reconstruction network that can jointly estimate pose, shape, texture, and the camera viewpoint. Specifically, we obtain geometric cues from the input image through easily accessible 2D detected keypoints. To learn an accurate hand reconstruction model from these noisy geometric cues, we utilize the consistency between 2D and 3D representations and propose a set of novel losses to rationalize outputs of the neural network. For the first time, we demonstrate the feasibility of training an accurate 3D hand reconstruction network without relying on manual annotations. Our experiments show that the proposed method achieves comparable performance with recent fully-supervised methods while using fewer supervision data.","accessed":{"date-parts":[["2024",11,20]]},"author":[{"family":"Chen","given":"Yujin"},{"family":"Tu","given":"Zhigang"},{"family":"Kang","given":"Di"},{"family":"Bao","given":"Linchao"},{"family":"Zhang","given":"Ying"},{"family":"Zhe","given":"Xuefei"},{"family":"Chen","given":"Ruizhi"},{"family":"Yuan","given":"Junsong"}],"citation-key":"chenModelbased3DHand2021a","issued":{"date-parts":[["2021",3,22]]},"number":"arXiv:2103.11703","publisher":"arXiv","source":"arXiv.org","title":"Model-based 3D Hand Reconstruction via Self-Supervised Learning","type":"article","URL":"http://arxiv.org/abs/2103.11703"},
  {"id":"chenTrackingReconstructingHand2023","abstract":"In this work, we tackle the challenging task of jointly tracking hand object poses and reconstructing their shapes from depth point cloud sequences in the wild, given the initial poses at frame 0. We for the first time propose a point cloud-based hand joint tracking network, HandTrackNet, to estimate the inter-frame hand joint motion. Our HandTrackNet proposes a novel hand pose canonicalization module to ease the tracking task, yielding accurate and robust hand joint tracking. Our pipeline then reconstructs the full hand via converting the predicted hand joints into a MANO hand. For object tracking, we devise a simple yet effective module that estimates the object SDF from the first frame and performs optimization-based tracking. Finally, a joint optimization step is adopted to perform joint hand and object reasoning, which alleviates the occlusion-induced ambiguity and further refines the hand pose. During training, the whole pipeline only sees purely synthetic data, which are synthesized with sufficient variations and by depth simulation for the ease of generalization. The whole pipeline is pertinent to the generalization gaps and thus directly transferable to real in-the-wild data. We evaluate our method on two real hand object interaction datasets, e.g. HO3D and DexYCB, without any fine-tuning. Our experiments demonstrate that the proposed method significantly outperforms the previous state-of-the-art depth-based hand and object pose estimation and tracking methods, running at a frame rate of 9 FPS. We have released our code on https://github.com/PKU-EPIC/HOTrack.","accessed":{"date-parts":[["2024",10,9]]},"author":[{"family":"Chen","given":"Jiayi"},{"family":"Yan","given":"Mi"},{"family":"Zhang","given":"Jiazhao"},{"family":"Xu","given":"Yinzhen"},{"family":"Li","given":"Xiaolong"},{"family":"Weng","given":"Yijia"},{"family":"Yi","given":"Li"},{"family":"Song","given":"Shuran"},{"family":"Wang","given":"He"}],"citation-key":"chenTrackingReconstructingHand2023","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","DOI":"10.1609/aaai.v37i1.25103","ISSN":"2374-3468","issue":"1","issued":{"date-parts":[["2023",6,26]]},"language":"en","license":"Copyright (c) 2023 Association for the Advancement of Artificial Intelligence","number":"1","page":"304-312","source":"ojs.aaai.org","title":"Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild","type":"article-journal","URL":"https://ojs.aaai.org/index.php/AAAI/article/view/25103","volume":"37"},
  {"id":"choCrossAttentionDisentangledModalities2022","abstract":"Transformer encoder architectures have recently achieved state-of-the-art results on monocular 3D human mesh reconstruction, but they require a substantial number of parameters and expensive computations. Due to the large memory overhead and slow inference speed, it is difficult to deploy such models for practical use. In this paper, we propose a novel transformer encoder-decoder architecture for 3D human mesh reconstruction from a single image, called FastMETRO. We identify the performance bottleneck in the encoder-based transformers is caused by the token design which introduces high complexity interactions among input tokens. We disentangle the interactions via an encoder-decoder architecture, which allows our model to demand much fewer parameters and shorter inference time. In addition, we impose the prior knowledge of human body‚Äôs morphological relationship via attention masking and mesh upsampling operations, which leads to faster convergence with higher accuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency, and clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore, we validate its generalizability on FreiHAND.","author":[{"family":"Cho","given":"Junhyeong"},{"family":"Youwang","given":"Kim"},{"family":"Oh","given":"Tae-Hyun"}],"citation-key":"choCrossAttentionDisentangledModalities2022","container-title":"Computer Vision ‚Äì ECCV 2022","DOI":"10.1007/978-3-031-19769-7_20","editor":[{"family":"Avidan","given":"Shai"},{"family":"Brostow","given":"Gabriel"},{"family":"Ciss√©","given":"Moustapha"},{"family":"Farinella","given":"Giovanni Maria"},{"family":"Hassner","given":"Tal"}],"event-place":"Cham","ISBN":"978-3-031-19769-7","issued":{"date-parts":[["2022"]]},"language":"en","page":"342-359","publisher":"Springer Nature Switzerland","publisher-place":"Cham","source":"Springer Link","title":"Cross-Attention of¬†Disentangled Modalities for¬†3D Human Mesh Recovery with¬†Transformers","type":"paper-conference"},
  {"id":"choDenseHandObjectHOGraspNet","abstract":"Existing datasets for 3D hand-object interaction are limited either in the data cardinality, data variations in interaction scenarios, or the quality of annotations. In this work, we present a comprehensive new training dataset for hand-object interaction called HOGraspNet. It is the only real dataset that captures full grasp taxonomies, providing grasp annotation and wide intraclass variations. Using grasp taxonomies as atomic actions, their space and time combinatorial can represent complex hand activities around objects. We select 22 rigid objects from the YCB dataset and 8 other compound objects using shape and size taxonomies, ensuring coverage of all hand grasp configurations. The dataset includes diverse hand shapes from 99 participants aged 10 to 74, continuous video frames, and a 1.5M RGB-Depth of sparse frames with annotations. It offers labels for 3D hand and object meshes, 3D keypoints, contact maps, and grasp labels. Accurate hand and object 3D meshes are obtained by fitting the hand parametric model (MANO) and the hand implicit function (HALO) to multi-view RGBD frames, with the MoCap system only for objects. Note that HALO fitting does not require any parameter tuning, enabling scalability to the dataset‚Äôs size with comparable accuracy to MANO. We evaluate HOGraspNet on relevant tasks: grasp classification and 3D hand pose estimation. The result shows performance variations based on grasp type and object class, indicating the potential importance of the interaction space captured by our dataset. The provided data aims at learning universal shape priors or foundation models for 3D hand-object interaction. Our dataset and code are available at https://hograspnet2024.github.io/.","author":[{"family":"Cho","given":"Woojin"},{"family":"Lee","given":"Jihyun"},{"family":"Yi","given":"Minjae"},{"family":"Kim","given":"Minje"},{"family":"Woo","given":"Taeyun"},{"family":"Kim","given":"Donghwan"},{"family":"Ha","given":"Taewook"},{"family":"Lee","given":"Hyokeun"},{"family":"Ryu","given":"Je-Hwan"},{"family":"Woo","given":"Woontack"},{"family":"Kim","given":"Tae-Kyun"}],"citation-key":"choDenseHandObjectHOGraspNet","language":"en","source":"Zotero","title":"Dense Hand-Object(HO) GraspNet with Full Grasping Taxonomy and Dynamics","type":"article-journal"},
  {"id":"coronaGanHandPredictingHuman2020","abstract":"The rise of deep learning has brought remarkable progress in estimating hand geometry from images where the hands are part of the scene. This paper focuses on a new problem not explored so far, consisting in predicting how a human would grasp one or several objects, given a single RGB image of these objects. This is a problem with enormous potential in e.g. augmented reality, robotics or prosthetic design. In order to predict feasible grasps, we need to understand the semantic content of the image, its geometric structure and all potential interactions with a hand physical model. To this end, we introduce a generative model that jointly reasons in all these levels and 1) regresses the 3D shape and pose of the objects in the scene; 2) estimates the grasp types; and 3) reÔ¨Ånes the 51-DoF of a 3D hand model that minimize a graspability loss. To train this model we build the YCB-Affordance dataset, that contains more than 133k images of 21 objects in the YCB-Video dataset [69]. We have annotated these images with more than 28M plausible 3D human grasps according to a 33-class taxonomy. A thorough evaluation in synthetic and real images shows that our model can robustly predict realistic grasps, even in cluttered scenes with multiple objects in close contact.","accessed":{"date-parts":[["2024",10,29]]},"author":[{"family":"Corona","given":"Enric"},{"family":"Pumarola","given":"Albert"},{"family":"Alenya","given":"Guillem"},{"family":"Moreno-Noguer","given":"Francesc"},{"family":"Rogez","given":"Gregory"}],"citation-key":"coronaGanHandPredictingHuman2020","container-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR42600.2020.00508","event-place":"Seattle, WA, USA","event-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-72817-168-5","issued":{"date-parts":[["2020",6]]},"language":"en","license":"https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","page":"5030-5040","publisher":"IEEE","publisher-place":"Seattle, WA, USA","source":"DOI.org (Crossref)","title":"GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes","title-short":"GanHand","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9156512/"},
  {"id":"coronaLISALearningImplicit2022a","abstract":"This paper proposes a do-it-all neural model of human hands, named LISA. The model can capture accurate hand shape and appearance, generalize to arbitrary hand subjects, provide dense surface correspondences, be reconstructed from images in the wild and easily animated. We train LISA by minimizing the shape and appearance losses on a large set of multi-view RGB image sequences annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand local coordinate, our model predicts the color and the signed distance with respect to each hand bone independently, and then combines the per-bone predictions using predicted skinning weights. The shape, color and pose representations are disentangled by design, allowing to estimate or animate only selected parameters. We experimentally demonstrate that LISA can accurately reconstruct a dynamic hand from monocular or multi-view sequences, achieving a noticeably higher quality of reconstructed hand shapes compared to baseline approaches. Project page: https://www.iri.upc.edu/people/ecorona/lisa/.","accessed":{"date-parts":[["2024",11,4]]},"author":[{"family":"Corona","given":"Enric"},{"family":"Hodan","given":"Tomas"},{"family":"Vo","given":"Minh"},{"family":"Moreno-Noguer","given":"Francesc"},{"family":"Sweeney","given":"Chris"},{"family":"Newcombe","given":"Richard"},{"family":"Ma","given":"Lingni"}],"citation-key":"coronaLISALearningImplicit2022a","issued":{"date-parts":[["2022",4,4]]},"number":"arXiv:2204.01695","publisher":"arXiv","source":"arXiv.org","title":"LISA: Learning Implicit Shape and Appearance of Hands","title-short":"LISA","type":"article","URL":"http://arxiv.org/abs/2204.01695"},
  {"id":"daoTransformersAreSSMs2024","abstract":"While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.","accessed":{"date-parts":[["2024",10,23]]},"author":[{"family":"Dao","given":"Tri"},{"family":"Gu","given":"Albert"}],"citation-key":"daoTransformersAreSSMs2024","DOI":"10.48550/arXiv.2405.21060","issued":{"date-parts":[["2024",5,31]]},"number":"arXiv:2405.21060","publisher":"arXiv","source":"arXiv.org","title":"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality","title-short":"Transformers are SSMs","type":"article","URL":"http://arxiv.org/abs/2405.21060"},
  {"id":"dongHambaSingleview3D2024","abstract":"3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, but they fail to achieve robust and accurate performance due to insufficient modeling of joint spatial relations. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to learn the joint relations and spatial sequences for enhancing the reconstruction performance. Specifically, we design a novel Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space modeling features and jointly considers global and local features to improve performance. Extensive experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. Hamba is currently Rank 1 in two challenging competition leaderboards on 3D hand reconstruction. The code will be available upon acceptance. [Website](https://humansensinglab.github.io/Hamba/).","accessed":{"date-parts":[["2024",7,23]]},"author":[{"family":"Dong","given":"Haoye"},{"family":"Chharia","given":"Aviral"},{"family":"Gou","given":"Wenbo"},{"family":"Carrasco","given":"Francisco Vicente"},{"family":"De la Torre","given":"Fernando"}],"citation-key":"dongHambaSingleview3D2024","issued":{"date-parts":[["2024",7,12]]},"language":"en","number":"arXiv:2407.09646","publisher":"arXiv","source":"arXiv.org","title":"Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba","title-short":"Hamba","type":"article","URL":"http://arxiv.org/abs/2407.09646"},
  {"id":"dongHambaSingleview3D2024a","abstract":"3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, but they fail to achieve robust and accurate performance due to insufficient modeling of joint spatial relations. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to learn the joint relations and spatial sequences for enhancing the reconstruction performance. Specifically, we design a novel Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space modeling features and jointly considers global and local features to improve performance. Extensive experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. Hamba is currently Rank 1 in two challenging competition leaderboards on 3D hand reconstruction. The code will be available upon acceptance. [Website](https://humansensinglab.github.io/Hamba/).","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Dong","given":"Haoye"},{"family":"Chharia","given":"Aviral"},{"family":"Gou","given":"Wenbo"},{"family":"Carrasco","given":"Francisco Vicente"},{"family":"Torre","given":"Fernando De","dropping-particle":"la"}],"citation-key":"dongHambaSingleview3D2024a","DOI":"10.48550/arXiv.2407.09646","issued":{"date-parts":[["2024",7,12]]},"number":"arXiv:2407.09646","publisher":"arXiv","source":"arXiv.org","title":"Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba","title-short":"Hamba","type":"article","URL":"http://arxiv.org/abs/2407.09646"},
  {"id":"dongHambaSingleview3D2024b","abstract":"3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, but they fail to achieve robust and accurate performance due to insufficient modeling of joint spatial relations. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to learn the joint relations and spatial sequences for enhancing the reconstruction performance. Specifically, we design a novel Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space modeling features and jointly considers global and local features to improve performance. Extensive experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. Hamba is currently Rank 1 in two challenging competition leaderboards on 3D hand reconstruction. The code will be available upon acceptance. [Website](https://humansensinglab.github.io/Hamba/).","accessed":{"date-parts":[["2024",10,17]]},"author":[{"family":"Dong","given":"Haoye"},{"family":"Chharia","given":"Aviral"},{"family":"Gou","given":"Wenbo"},{"family":"Carrasco","given":"Francisco Vicente"},{"family":"Torre","given":"Fernando De","dropping-particle":"la"}],"citation-key":"dongHambaSingleview3D2024b","DOI":"10.48550/arXiv.2407.09646","issued":{"date-parts":[["2024",7,12]]},"number":"arXiv:2407.09646","publisher":"arXiv","source":"arXiv.org","title":"Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba","title-short":"Hamba","type":"article","URL":"http://arxiv.org/abs/2407.09646"},
  {"id":"fanARCTICDatasetDexterous2023","abstract":"Humans intuitively understand that inanimate objects do not move by themselves, but that state changes are typically caused by human manipulation (e.g., the opening of a book). This is not yet the case for machines. In part this is because there exist no datasets with ground-truth 3D annotations for the study of physically consistent and synchronised motion of hands and articulated objects. To this end, we introduce ARCTIC -- a dataset of two hands that dexterously manipulate objects, containing 2.1M video frames paired with accurate 3D hand and object meshes and detailed, dynamic contact information. It contains bi-manual articulation of objects such as scissors or laptops, where hand poses and object states evolve jointly in time. We propose two novel articulated hand-object interaction tasks: (1) Consistent motion reconstruction: Given a monocular video, the goal is to reconstruct two hands and articulated objects in 3D, so that their motions are spatio-temporally consistent. (2) Interaction field estimation: Dense relative hand-object distances must be estimated from images. We introduce two baselines ArcticNet and InterField, respectively and evaluate them qualitatively and quantitatively on ARCTIC. Our code and data are available at https://arctic.is.tue.mpg.de.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Fan","given":"Zicong"},{"family":"Taheri","given":"Omid"},{"family":"Tzionas","given":"Dimitrios"},{"family":"Kocabas","given":"Muhammed"},{"family":"Kaufmann","given":"Manuel"},{"family":"Black","given":"Michael J."},{"family":"Hilliges","given":"Otmar"}],"citation-key":"fanARCTICDatasetDexterous2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12943-12954","source":"openaccess.thecvf.com","title":"ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation","title-short":"ARCTIC","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.html"},
  {"id":"fanBenchmarksChallengesPose","abstract":"We interact with the world with our hands and see it through our own (egocentric) perspective. A holistic 3D understanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation. Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement. To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from different views. Our study further reveals challenging scenarios intractable with state-of-the-art methods, such as fast hand motion, object reconstruction from narrow egocentric views, and close contact between two hands and objects. Our efforts will enrich the community‚Äôs knowledge foundation and facilitate future hand studies on egocentric hand-object interactions.","author":[{"family":"Fan","given":"Zicong"},{"family":"Ohkawa","given":"Takehiko"},{"family":"Yang","given":"Linlin"},{"family":"Lin","given":"Nie"},{"family":"Zhou","given":"Zhishan"},{"family":"Zhou","given":"Shihao"},{"family":"Liang","given":"Jiajun"},{"family":"Gao","given":"Zhong"},{"family":"Zhang","given":"Xuanyang"},{"family":"Zhang","given":"Xue"},{"family":"Li","given":"Fei"},{"family":"Zheng","given":"Liu"},{"family":"Lu","given":"Feng"},{"family":"Zeid","given":"Karim Abou"},{"family":"Leibe","given":"Bastian"},{"family":"On","given":"Jeongwan"},{"family":"Baek","given":"Seungryul"},{"family":"Prakash","given":"Aditya"},{"family":"Gupta","given":"Saurabh"},{"family":"He","given":"Kun"},{"family":"Sato","given":"Yoichi"},{"family":"Hilliges","given":"Otmar"},{"family":"Chang","given":"Hyung Jin"},{"family":"Yao","given":"Angela"}],"citation-key":"fanBenchmarksChallengesPose","language":"en","source":"Zotero","title":"Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects","type":"article-journal"},
  {"id":"fanHOLDCategoryagnostic3D2023","abstract":"Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most existing methods for hand-object reconstruction from RGB either assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To this end, we introduce HOLD -- the first category-agnostic method that reconstructs an articulated hand and object jointly from a monocular interaction video. We develop a compositional articulated implicit model that can reconstruct disentangled 3D hand and object from 2D images. We also further incorporate hand-object constraints to improve hand-object poses and consequently the reconstruction quality. Our method does not rely on 3D hand-object annotations while outperforming fully-supervised baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we qualitatively show its robustness in reconstructing from in-the-wild videos. Code: https://github.com/zc-alexfan/hold","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Fan","given":"Zicong"},{"family":"Parelli","given":"Maria"},{"family":"Kadoglou","given":"Maria Eleni"},{"family":"Kocabas","given":"Muhammed"},{"family":"Chen","given":"Xu"},{"family":"Black","given":"Michael J."},{"family":"Hilliges","given":"Otmar"}],"citation-key":"fanHOLDCategoryagnostic3D2023","DOI":"10.48550/arXiv.2311.18448","issued":{"date-parts":[["2023",11,30]]},"number":"arXiv:2311.18448","publisher":"arXiv","source":"arXiv.org","title":"HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video","title-short":"HOLD","type":"article","URL":"http://arxiv.org/abs/2311.18448"},
  {"id":"fanLearningDisambiguateStrongly2021","abstract":"In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M [33] dataset. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects 3D hand pose estimation.","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Fan","given":"Zicong"},{"family":"Spurr","given":"Adrian"},{"family":"Kocabas","given":"Muhammed"},{"family":"Tang","given":"Siyu"},{"family":"Black","given":"Michael J."},{"family":"Hilliges","given":"Otmar"}],"citation-key":"fanLearningDisambiguateStrongly2021","container-title":"2021 International Conference on 3D Vision (3DV)","DOI":"10.1109/3DV53792.2021.00011","event-title":"2021 International Conference on 3D Vision (3DV)","ISSN":"2475-7888","issued":{"date-parts":[["2021",12]]},"page":"1-10","source":"IEEE Xplore","title":"Learning To Disambiguate Strongly Interacting Hands via Probabilistic Per-Pixel Part Segmentation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/9665878"},
  {"id":"fuDeformerDynamicFusion2023","abstract":"Accurately estimating 3D hand pose is crucial for understanding how humans interact with the world. Despite remarkable progress, existing methods often struggle to generate plausible hand poses when the hand is heavily occluded or blurred. In videos, the movements of the hand allow us to observe various parts of the hand that may be occluded or blurred in a single frame. To adaptively leverage the visual clue before and after the occlusion or blurring for robust hand pose estimation, we propose the Deformer: a framework that implicitly reasons about the relationship between hand parts within the same image (spatial dimension) and different timesteps (temporal dimension). We show that a naive application of the transformer self-attention mechanism is not sufficient because motion blur or occlusions in certain frames can lead to heavily distorted hand features and generate imprecise keys and queries. To address this challenge, we incorporate a Dynamic Fusion Module into Deformer, which predicts the deformation of the hand and warps the hand mesh predictions from nearby frames to explicitly support the current frame estimation. Furthermore, we have observed that errors are unevenly distributed across different hand parts, with vertices around fingertips having disproportionately higher errors than those around the palm. We mitigate this issue by introducing a new loss function called maxMSE that automatically adjusts the weight of every vertex to focus the model on critical hand parts. Extensive experiments show that our method significantly outperforms state-of-the-art methods by 10%, and is more robust to occlusions (over 14%).","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Fu","given":"Qichen"},{"family":"Liu","given":"Xingyu"},{"family":"Xu","given":"Ran"},{"family":"Niebles","given":"Juan Carlos"},{"family":"Kitani","given":"Kris M."}],"citation-key":"fuDeformerDynamicFusion2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"23600-23611","source":"openaccess.thecvf.com","title":"Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation","title-short":"Deformer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Fu_Deformer_Dynamic_Fusion_Transformer_for_Robust_Hand_Pose_Estimation_ICCV_2023_paper.html"},
  {"id":"fuDSCoDualStreamConditional","abstract":"Reconstructing hand-held objects from a single RGB image is a challenging task in computer vision. In contrast to prior works that utilize deterministic modeling paradigms, we employ a point cloud denoising diffusion model to account for the probabilistic nature of this problem. In the core, we introduce centroid-fixed Dual-Stream Conditional diffusion for monocular hand-held object reconstruction (D-SCo), tackling two predominant challenges. First, to avoid the object centroid from deviating, we utilize a novel hand-constrained centroid fixing paradigm, enhancing the stability of diffusion and reverse processes and the precision of feature projection. Second, we introduce a dual-stream denoiser to semantically and geometrically model hand-object interactions with a novel unified hand-object semantic embedding, enhancing the reconstruction performance of the hand-occluded region of the object. Experiments on the synthetic ObMan dataset and three real-world datasets HO3D, MOW and DexYCB demonstrate that our approach can surpass all other state-of-the-art methods.","author":[{"family":"Fu","given":"Bowen"},{"family":"Wang","given":"Gu"},{"family":"Zhang","given":"Chenyangguang"},{"family":"Di","given":"Yan"},{"family":"Huang","given":"Ziqin"},{"family":"Leng","given":"Zhiying"},{"family":"Manhardt","given":"Fabian"},{"family":"Ji","given":"Xiangyang"},{"family":"Tombari","given":"Federico"}],"citation-key":"fuDSCoDualStreamConditional","language":"en","source":"Zotero","title":"D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object Reconstruction","type":"article-journal"},
  {"id":"garcia-hernandoFirstPersonHandAction2018","abstract":"In this work we study the use of 3D hand poses to recognize first-person dynamic hand actions interacting with 3D objects. Towards this goal, we collected RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations. To obtain hand pose annotations, we used our own mo-cap system that automatically infers the 3D location of each of the 21 joints of a hand model via 6 magnetic sensors and inverse kinematics. Additionally, we recorded the 6D object poses and provide 3D object models for a subset of hand-object interaction sequences. To the best of our knowledge, this is the first benchmark that enables the study of first-person hand actions with the use of 3D hand poses. We present an extensive experimental evaluation of RGB-D and pose-based action recognition by 18 baselines/state-of-the-art approaches. The impact of using appearance features, poses, and their combinations are measured, and the different training/testing protocols are evaluated. Finally, we assess how ready the 3D hand pose estimation field is when hands are severely occluded by objects in egocentric views and its influence on action recognition. From the results, we see clear benefits of using hand pose as a cue for action recognition compared to other data modalities. Our dataset and experiments can be of interest to communities of 3D hand pose estimation, 6D object pose, and robotics as well as action recognition.","accessed":{"date-parts":[["2024",10,29]]},"author":[{"family":"Garcia-Hernando","given":"Guillermo"},{"family":"Yuan","given":"Shanxin"},{"family":"Baek","given":"Seungryul"},{"family":"Kim","given":"Tae-Kyun"}],"citation-key":"garcia-hernandoFirstPersonHandAction2018","DOI":"10.48550/arXiv.1704.02463","issued":{"date-parts":[["2018",4,10]]},"number":"arXiv:1704.02463","publisher":"arXiv","source":"arXiv.org","title":"First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations","type":"article","URL":"http://arxiv.org/abs/1704.02463"},
  {"id":"ge3DHandShape","abstract":"This work addresses a novel and challenging problem of estimating the full 3D hand shape and pose from a single RGB image. Most current methods in 3D hand analysis from monocular RGB images only focus on estimating the 3D locations of hand keypoints, which cannot fully express the 3D shape of hand. In contrast, we propose a Graph Convolutional Neural Network (Graph CNN) based method to reconstruct a full 3D mesh of hand surface that contains richer information of both 3D hand shape and pose. To train networks with full supervision, we create a large-scale synthetic dataset containing both ground truth 3D meshes and 3D poses. When Ô¨Åne-tuning the networks on real-world datasets without 3D ground truth, we propose a weakly-supervised approach by leveraging the depth map as a weak supervision in training. Through extensive evaluations on our proposed new datasets and two public datasets, we show that our proposed method can produce accurate and reasonable 3D hand mesh, and can achieve superior 3D hand pose estimation accuracy when compared with state-of-the-art methods.","author":[{"family":"Ge","given":"Liuhao"},{"family":"Ren","given":"Zhou"},{"family":"Li","given":"Yuncheng"},{"family":"Xue","given":"Zehao"},{"family":"Wang","given":"Yingying"},{"family":"Cai","given":"Jianfei"},{"family":"Yuan","given":"Junsong"}],"citation-key":"ge3DHandShape","language":"en","source":"Zotero","title":"3D Hand Shape and Pose Estimation From a Single RGB Image","type":"article-journal"},
  {"id":"goyalHumanHandsProbes2022","accessed":{"date-parts":[["2024",11,22]]},"author":[{"family":"Goyal","given":"Mohit"},{"family":"Modi","given":"Sahil"},{"family":"Goyal","given":"Rishabh"},{"family":"Gupta","given":"Saurabh"}],"citation-key":"goyalHumanHandsProbes2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"3293-3303","source":"openaccess.thecvf.com","title":"Human Hands As Probes for Interactive Object Understanding","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Goyal_Human_Hands_As_Probes_for_Interactive_Object_Understanding_CVPR_2022_paper.html"},
  {"id":"gradyContactOptOptimizingContact2021a","abstract":"Physical contact between hands and objects plays a critical role in human grasps. We show that optimizing the pose of a hand to achieve expected contact with an object can improve hand poses inferred via image-based methods. Given a hand mesh and an object mesh, a deep model trained on ground truth contact data infers desirable contact across the surfaces of the meshes. Then, ContactOpt efÔ¨Åciently optimizes the pose of the hand to achieve desirable contact using a differentiable contact model. Notably, our contact model encourages mesh interpenetration to approximate deformable soft tissue in the hand. In our evaluations, our methods resulted in grasps that better matched ground truth contact, had lower kinematic error, and were signiÔ¨Åcantly preferred by human participants. Code for this work will be publicly released.","accessed":{"date-parts":[["2024",10,15]]},"author":[{"family":"Grady","given":"Patrick"},{"family":"Tang","given":"Chengcheng"},{"family":"Twigg","given":"Christopher D."},{"family":"Vo","given":"Minh"},{"family":"Brahmbhatt","given":"Samarth"},{"family":"Kemp","given":"Charles C."}],"citation-key":"gradyContactOptOptimizingContact2021a","container-title":"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR46437.2021.00152","event-place":"Nashville, TN, USA","event-title":"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-66544-509-2","issued":{"date-parts":[["2021",6]]},"language":"en","license":"https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","page":"1471-1481","publisher":"IEEE","publisher-place":"Nashville, TN, USA","source":"DOI.org (Crossref)","title":"ContactOpt: Optimizing Contact to Improve Grasps","title-short":"ContactOpt","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9578455/"},
  {"id":"graumanEgo4DWorld3000","author":[{"family":"Grauman","given":"Kristen"},{"family":"Westbury","given":"Andrew"},{"family":"Byrne","given":"Eugene"},{"family":"Chavis","given":"Zachary"},{"family":"Furnari","given":"Antonino"},{"family":"Girdhar","given":"Rohit"},{"family":"Hamburger","given":"Jackson"},{"family":"Jiang","given":"Hao"},{"family":"Liu","given":"Miao"},{"family":"Liu","given":"Xingyu"},{"family":"Martin","given":"Miguel"},{"family":"Nagarajan","given":"Tushar"},{"family":"Radosavovic","given":"Ilija"},{"family":"Ramakrishnan","given":"Santhosh Kumar"},{"family":"Ryan","given":"Fiona"},{"family":"Sharma","given":"Jayant"},{"family":"Wray","given":"Michael"},{"family":"Xu","given":"Mengmeng"},{"family":"Xu","given":"Eric Zhongcong"},{"family":"Zhao","given":"Chen"},{"family":"Bansal","given":"Siddhant"},{"family":"Batra","given":"Dhruv"},{"family":"Cartillier","given":"Vincent"},{"family":"Crane","given":"Sean"},{"family":"Do","given":"Tien"},{"family":"Doulaty","given":"Morrie"},{"family":"Erapalli","given":"Akshay"},{"family":"Feichtenhofer","given":"Christoph"},{"family":"Fragomeni","given":"Adriano"},{"family":"Fu","given":"Qichen"},{"family":"Gebreselasie","given":"Abrham"},{"family":"Gonzalez","given":"Cristina"},{"family":"Hillis","given":"James"},{"family":"Huang","given":"Xuhua"},{"family":"Huang","given":"Yifei"},{"family":"Jia","given":"Wenqi"},{"family":"Khoo","given":"Weslie"},{"family":"Kolar","given":"Jachym"},{"family":"Kottur","given":"Satwik"},{"family":"Kumar","given":"Anurag"},{"family":"Landini","given":"Federico"},{"family":"Li","given":"Chao"},{"family":"Li","given":"Yanghao"},{"family":"Li","given":"Zhenqiang"},{"family":"Mangalam","given":"Karttikeya"},{"family":"Modhugu","given":"Raghava"},{"family":"Munro","given":"Jonathan"},{"family":"Murrell","given":"Tullie"},{"family":"Nishiyasu","given":"Takumi"},{"family":"Price","given":"Will"},{"family":"Ruiz","given":"Paola"},{"family":"Ramazanova","given":"Merey"},{"family":"Sari","given":"Leda"},{"family":"Somasundaram","given":"Kiran"},{"family":"Southerland","given":"Audrey"},{"family":"Sugano","given":"Yusuke"},{"family":"Tao","given":"Ruijie"},{"family":"Vo","given":"Minh"},{"family":"Wang","given":"Yuchen"},{"family":"Wu","given":"Xindi"},{"family":"Yagi","given":"Takuma"},{"family":"Zhao","given":"Ziwei"},{"family":"Zhu","given":"Yunyi"},{"family":"Arbelaez","given":"Pablo"},{"family":"Crandall","given":"David"},{"family":"Damen","given":"Dima"},{"family":"Farinella","given":"Giovanni Maria"},{"family":"Fuegen","given":"Christian"},{"family":"Ghanem","given":"Bernard"},{"family":"Ithapu","given":"Vamsi Krishna"},{"family":"Jawahar","given":"C V"},{"family":"Joo","given":"Hanbyul"},{"family":"Kitani","given":"Kris"},{"family":"Li","given":"Haizhou"},{"family":"Newcombe","given":"Richard"},{"family":"Oliva","given":"Aude"},{"family":"Park","given":"Hyun Soo"},{"family":"Rehg","given":"James M"},{"family":"Sato","given":"Yoichi"},{"family":"Shi","given":"Jianbo"},{"family":"Shou","given":"Mike Zheng"},{"family":"Torralba","given":"Antonio"},{"family":"Torresani","given":"Lorenzo"},{"family":"Yan","given":"Mingfei"},{"family":"Malik","given":"Jitendra"}],"citation-key":"graumanEgo4DWorld3000","language":"en","source":"Zotero","title":"Ego4D: Around the World in 3,000 Hours of Egocentric Video","type":"article-journal"},
  {"id":"guMambaLinearTimeSequence2024","abstract":"Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.","accessed":{"date-parts":[["2024",10,17]]},"author":[{"family":"Gu","given":"Albert"},{"family":"Dao","given":"Tri"}],"citation-key":"guMambaLinearTimeSequence2024","DOI":"10.48550/arXiv.2312.00752","issued":{"date-parts":[["2024",5,31]]},"number":"arXiv:2312.00752","publisher":"arXiv","source":"arXiv.org","title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces","title-short":"Mamba","type":"article","URL":"http://arxiv.org/abs/2312.00752"},
  {"id":"hampaliInHand3DObject2023","abstract":"We propose a method for in-hand 3D scanning of an unknown object with a monocular camera. Our method relies on a neural implicit surface representation that captures both the geometry and the appearance of the object, however, by contrast with most NeRF-based methods, we do not assume that the camera-object relative poses are known. Instead, we simultaneously optimize both the object shape and the pose trajectory. As direct optimization over all shape and pose parameters is prone to fail without coarse-level initialization, we propose an incremental approach that starts by splitting the sequence into carefully selected overlapping segments within which the optimization is likely to succeed. We reconstruct the object shape and track its poses independently within each segment, then merge all the segments before performing a global optimization. We show that our method is able to reconstruct the shape and color of both textured and challenging texture-less objects, outperforms classical methods that rely only on appearance features, and that its performance is close to recent methods that assume known camera poses.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Hampali","given":"Shreyas"},{"family":"Hodan","given":"Tomas"},{"family":"Tran","given":"Luan"},{"family":"Ma","given":"Lingni"},{"family":"Keskin","given":"Cem"},{"family":"Lepetit","given":"Vincent"}],"citation-key":"hampaliInHand3DObject2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"17079-17088","source":"openaccess.thecvf.com","title":"In-Hand 3D Object Scanning From an RGB Sequence","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Hampali_In-Hand_3D_Object_Scanning_From_an_RGB_Sequence_CVPR_2023_paper.html"},
  {"id":"hassonLeveragingPhotometricConsistency2020","accessed":{"date-parts":[["2024",10,3]]},"author":[{"family":"Hasson","given":"Yana"},{"family":"Tekin","given":"Bugra"},{"family":"Bogo","given":"Federica"},{"family":"Laptev","given":"Ivan"},{"family":"Pollefeys","given":"Marc"},{"family":"Schmid","given":"Cordelia"}],"citation-key":"hassonLeveragingPhotometricConsistency2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"571-580","source":"openaccess.thecvf.com","title":"Leveraging Photometric Consistency Over Time for Sparsely Supervised Hand-Object Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Hasson_Leveraging_Photometric_Consistency_Over_Time_for_Sparsely_Supervised_Hand-Object_Reconstruction_CVPR_2020_paper.html"},
  {"id":"hassonUnconstrainedJointHandObject2021","abstract":"Our work aims to obtain 3D reconstruction of hands and manipulated objects from monocular videos. Reconstructing hand-object manipulations holds a great potential for robotics and learning from human demonstrations. The supervised learning approach to this problem, however, requires 3D supervision and remains limited to constrained laboratory settings and simulators for which 3D ground truth is available. In this paper we first propose a learning-free fitting approach for hand-object reconstruction which can seamlessly handle two-hand object interactions. Our method relies on cues obtained with common methods for object detection, hand pose estimation and instance segmentation. We quantitatively evaluate our approach and show that it can be applied to datasets with varying levels of difficulty for which training data is unavailable.","accessed":{"date-parts":[["2024",10,3]]},"author":[{"family":"Hasson","given":"Yana"},{"family":"Varol","given":"G√ºl"},{"family":"Schmid","given":"Cordelia"},{"family":"Laptev","given":"Ivan"}],"citation-key":"hassonUnconstrainedJointHandObject2021","container-title":"2021 International Conference on 3D Vision (3DV)","DOI":"10.1109/3DV53792.2021.00075","event-title":"2021 International Conference on 3D Vision (3DV)","ISSN":"2475-7888","issued":{"date-parts":[["2021",12]]},"page":"659-668","source":"IEEE Xplore","title":"Towards Unconstrained Joint Hand-Object Reconstruction From RGB Videos","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/9665955"},
  {"id":"huangNeuralVotingField2023","abstract":"We present a unified framework for camera-space 3D hand pose estimation from a single RGB image based on 3D implicit representation. As opposed to recent works, most of which first adopt holistic or pixel-level dense regression to obtain relative 3D hand pose and then follow with complex second-stage operations for 3D global root or scale recovery, we propose a novel unified 3D dense regression scheme to estimate camera-space 3D hand pose via dense 3D point-wise voting in camera frustum. Through direct dense modeling in 3D domain inspired by Pixel-aligned Implicit Functions for 3D detailed reconstruction, our proposed Neural Voting Field (NVF) fully models 3D dense local evidence and hand global geometry, helping to alleviate common 2D-to-3D ambiguities. Specifically, for a 3D query point in camera frustum and its pixel-aligned image feature, NVF, represented by a Multi-Layer Perceptron, regresses: (i) its signed distance to the hand surface; (ii) a set of 4D offset vectors (1D voting weight and 3D directional vector to each hand joint). Following a vote-casting scheme, 4D offset vectors from near-surface points are selected to calculate the 3D hand joint coordinates by a weighted average. Experiments demonstrate that NVF outperforms existing state-of-the-art algorithms on FreiHAND dataset for camera-space 3D hand pose estimation. We also adapt NVF to the classic task of root-relative 3D hand pose estimation, for which NVF also obtains state-of-the-art results on HO3D dataset.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Huang","given":"Lin"},{"family":"Lin","given":"Chung-Ching"},{"family":"Lin","given":"Kevin"},{"family":"Liang","given":"Lin"},{"family":"Wang","given":"Lijuan"},{"family":"Yuan","given":"Junsong"},{"family":"Liu","given":"Zicheng"}],"citation-key":"huangNeuralVotingField2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8969-8978","source":"openaccess.thecvf.com","title":"Neural Voting Field for Camera-Space 3D Hand Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Neural_Voting_Field_for_Camera-Space_3D_Hand_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"huangPHRITParametricHand2023","abstract":"We propose PHRIT, a novel approach for parametric hand mesh modeling with an implicit template that combines the advantages of both parametric meshes and implicit representations. Our method represents deformable hand shapes using signed distance fields (SDFs) with part-based shape priors, utilizing a deformation field to execute the deformation. The model offers efficient high-fidelity hand reconstruction by deforming the canonical template at infinite resolution. Additionally, it is fully differentiable and can be easily used in hand modeling since it can be driven by the skeleton and shape latent codes. We evaluate PHRIT on multiple downstream tasks, including skeleton-driven hand reconstruction, shapes from point clouds, and single-view 3D reconstruction, demonstrating that our approach achieves realistic and immersive hand modeling with state-of-the-art performance.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Huang","given":"Zhisheng"},{"family":"Chen","given":"Yujin"},{"family":"Kang","given":"Di"},{"family":"Zhang","given":"Jinlu"},{"family":"Tu","given":"Zhigang"}],"citation-key":"huangPHRITParametricHand2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"14974-14984","source":"openaccess.thecvf.com","title":"PHRIT: Parametric Hand Representation with Implicit Template","title-short":"PHRIT","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Huang_PHRIT_Parametric_Hand_Representation_with_Implicit_Template_ICCV_2023_paper.html"},
  {"id":"huHandObjectInteractionController2024","abstract":"Hand manipulating objects is an important interaction motion in our daily activities. We faithfully reconstruct this motion with a single RGBD camera by a novel deep reinforcement learning method to leverage physics. Firstly, we propose object compensation control which establishes direct object control to make the network training more stable. Meanwhile, by leveraging the compensation force and torque, we seamlessly upgrade the simple point contact model to a more physical-plausible surface contact model, further improving the reconstruction accuracy and physical correctness. Experiments indicate that without involving any heuristic physical rules, this work still successfully involves physics in the reconstruction of hand-object interactions which are complex motions hard to imitate with deep reinforcement learning. Our code and data are available at https://github.com/hu-hy17/HOIC.","accessed":{"date-parts":[["2024",10,10]]},"author":[{"family":"Hu","given":"Haoyu"},{"family":"Yi","given":"Xinyu"},{"family":"Cao","given":"Zhe"},{"family":"Yong","given":"Jun-Hai"},{"family":"Xu","given":"Feng"}],"citation-key":"huHandObjectInteractionController2024","DOI":"10.48550/arXiv.2405.02676","issued":{"date-parts":[["2024",5,4]]},"number":"arXiv:2405.02676","publisher":"arXiv","source":"arXiv.org","title":"Hand-Object Interaction Controller (HOIC): Deep Reinforcement Learning for Reconstructing Interactions with Physics","title-short":"Hand-Object Interaction Controller (HOIC)","type":"article","URL":"http://arxiv.org/abs/2405.02676"},
  {"id":"iqbalHandPoseEstimation2018","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Iqbal","given":"Umar"},{"family":"Molchanov","given":"Pavlo"},{"family":"Gall","given":"Thomas Breuel Juergen"},{"family":"Kautz","given":"Jan"}],"citation-key":"iqbalHandPoseEstimation2018","event-title":"Proceedings of the European Conference on Computer Vision (ECCV)","issued":{"date-parts":[["2018"]]},"page":"118-134","source":"openaccess.thecvf.com","title":"Hand Pose Estimation via Latent 2.5D Heatmap Regression","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ECCV_2018/html/Umar_Iqbal_Hand_Pose_Estimation_ECCV_2018_paper.html"},
  {"id":"iwaseRelightableHandsEfficientNeural2023","abstract":"We present the first neural relighting approach for rendering high-fidelity personalized hands that can be animated in real-time under novel illumination. Our approach adopts a teacher-student framework, where the teacher learns appearance under a single point light from images captured in a light-stage, allowing us to synthesize hands in arbitrary illuminations but with heavy compute. Using images rendered by the teacher model as training data, an efficient student model directly predicts appearance under natural illuminations in real-time. To achieve generalization, we condition the student model with physics-inspired illumination features such as visibility, diffuse shading, and specular reflections computed on a coarse proxy geometry, maintaining a small computational overhead. Our key insight is that these features have strong correlation with subsequent global light transport effects, which proves sufficient as conditioning data for the neural relighting network. Moreover, in contrast to bottleneck illumination conditioning, these features are spatially aligned based on underlying geometry, leading to better generalization to unseen illuminations and poses. In our experiments, we demonstrate the efficacy of our illumination feature representations, outperforming baseline approaches. We also show that our approach can photorealistically relight two interacting hands at real-time speeds.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Iwase","given":"Shun"},{"family":"Saito","given":"Shunsuke"},{"family":"Simon","given":"Tomas"},{"family":"Lombardi","given":"Stephen"},{"family":"Bagautdinov","given":"Timur"},{"family":"Joshi","given":"Rohan"},{"family":"Prada","given":"Fabian"},{"family":"Shiratori","given":"Takaaki"},{"family":"Sheikh","given":"Yaser"},{"family":"Saragih","given":"Jason"}],"citation-key":"iwaseRelightableHandsEfficientNeural2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"16663-16673","source":"openaccess.thecvf.com","title":"RelightableHands: Efficient Neural Relighting of Articulated Hand Models","title-short":"RelightableHands","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Iwase_RelightableHands_Efficient_Neural_Relighting_of_Articulated_Hand_Models_CVPR_2023_paper.html"},
  {"id":"jianAffordPoseLargeScaleDataset2023","abstract":"How human interact with objects depends on the functional roles of the target objects, which introduces the problem of affordance-aware hand-object interaction. It requires a large number of human demonstrations for the learning and understanding of plausible and appropriate hand-object interactions. In this work, we present AffordPose, a large-scale dataset of hand-object interactions with affordance-driven hand pose. We first annotate the specific part-level affordance labels for each object, e.g. twist, pull, handle-grasp, etc, instead of the general intents such as use or handover, to indicate the purpose and guide the localization of the hand-object interactions. The fine-grained hand-object interactions reveal the influence of hand-centered affordances on the detailed arrangement of the hand poses, yet also exhibit a certain degree of diversity. We collect a total of 26.7K hand-object interactions, each including the 3D object shape, the part-level affordance label, and the manually adjusted hand poses. The comprehensive data analysis shows the common characteristics and diversity of hand-object interactions per affordance via the parameter statistics and contacting computation. We also conduct experiments on the tasks of hand-object affordance understanding and affordance-oriented hand-object interaction generation, to validate the effectiveness of our dataset in learning the fine-grained hand-object interactions. Project page: https://github.com/GentlesJan/AffordPose .","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Jian","given":"Juntao"},{"family":"Liu","given":"Xiuping"},{"family":"Li","given":"Manyi"},{"family":"Hu","given":"Ruizhen"},{"family":"Liu","given":"Jian"}],"citation-key":"jianAffordPoseLargeScaleDataset2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"14713-14724","source":"openaccess.thecvf.com","title":"AffordPose: A Large-Scale Dataset of Hand-Object Interactions with Affordance-Driven Hand Pose","title-short":"AffordPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Jian_AffordPose_A_Large-Scale_Dataset_of_Hand-Object_Interactions_with_Affordance-Driven_Hand_ICCV_2023_paper.html"},
  {"id":"jiangA2JTransformerAnchortoJointTransformer2023","abstract":"3D interacting hand pose estimation from a single RGB image is a challenging task, due to serious self-occlusion and inter-occlusion towards hands, confusing similar appearance patterns between 2 hands, ill-posed joint position mapping from 2D to 3D, etc.. To address these, we propose to extend A2J-the state-of-the-art depth-based 3D single hand pose estimation method-to RGB domain under interacting hand condition. Our key idea is to equip A2J with strong local-global aware ability to well capture interacting hands' local fine details and global articulated clues among joints jointly. To this end, A2J is evolved under Transformer's non-local encoding-decoding framework to build A2J-Transformer. It holds 3 main advantages over A2J. First, self-attention across local anchor points is built to make them global spatial context aware to better capture joints' articulation clues for resisting occlusion. Secondly, each anchor point is regarded as learnable query with adaptive feature learning for facilitating pattern fitting capacity, instead of having the same local representation with the others. Last but not least, anchor point locates in 3D space instead of 2D as in A2J, to leverage 3D pose prediction. Experiments on challenging InterHand 2.6M demonstrate that, A2J-Transformer can achieve state-of-the-art model-free performance (3.38mm MPJPE advancement in 2-hand case) and can also be applied to depth domain with strong generalization.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Jiang","given":"Changlong"},{"family":"Xiao","given":"Yang"},{"family":"Wu","given":"Cunlin"},{"family":"Zhang","given":"Mingyang"},{"family":"Zheng","given":"Jinghong"},{"family":"Cao","given":"Zhiguo"},{"family":"Zhou","given":"Joey Tianyi"}],"citation-key":"jiangA2JTransformerAnchortoJointTransformer2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8846-8855","source":"openaccess.thecvf.com","title":"A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation From a Single RGB Image","title-short":"A2J-Transformer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_A2J-Transformer_Anchor-to-Joint_Transformer_Network_for_3D_Interacting_Hand_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"jiangComplementingEventStreams2024","abstract":"Reliable hand mesh reconstruction (HMR) from commonly-used color and depth sensors is challenging especially under scenarios with varied illuminations and fast motions. Event camera is a highly promising alternative for its high dynamic range and dense temporal resolution properties but it lacks key texture appearance for hand mesh reconstruction. In this paper we propose EvRGBHand -- the first approach for 3D hand mesh reconstruction with an event camera and an RGB camera compensating for each other. By fusing two modalities of data across time space and information dimensionsEvRGBHand can tackle overexposure and motion blur issues in RGB-based HMR and foreground scarcity and background overflow issues in event-based HMR. We further propose EvRGBDegrader which allows our model to generalize effectively in challenging scenes even when trained solely on standard scenes thus reducing data acquisition costs. Experiments on real-world data demonstrate that EvRGBHand can effectively solve the challenging issues when using either type of camera alone via retaining the merits of both and shows the potential of generalization to outdoor scenes and another type of event camera. Our code models and dataset will be made public after acceptance.","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Jiang","given":"Jianping"},{"family":"Zhou","given":"Xinyu"},{"family":"Wang","given":"Bingxuan"},{"family":"Deng","given":"Xiaoming"},{"family":"Xu","given":"Chao"},{"family":"Shi","given":"Boxin"}],"citation-key":"jiangComplementingEventStreams2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"24944-24954","source":"openaccess.thecvf.com","title":"Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_Complementing_Event_Streams_and_RGB_Frames_for_Hand_Mesh_Reconstruction_CVPR_2024_paper.html"},
  {"id":"jiangHandObjectContactConsistency2021","abstract":"While predicting robot grasps with parallel jaw grippers have been well studied and widely applied in robot manipulation tasks, the study on natural human grasp generation with a multi-finger hand remains a very challenging problem. In this paper, we propose to generate human grasps given a 3D object in the world. Our key observation is that it is crucial to model the consistency between the hand contact points and object contact regions. That is, we encourage the prior hand contact points to be close to the object surface and the object common contact regions to be touched by the hand at the same time. Based on the hand-object contact consistency, we design novel objectives in training the human grasp generation model and also a new self-supervised task which allows the grasp generation network to be adjusted even during test time. Our experiments show significant improvement in human grasp generation over state-of-the-art approaches by a large margin. More interestingly, by optimizing the model during test time with the self-supervised task, it helps achieve larger gain on unseen and out-of-domain objects. Project page: https://hwjiang1510.github.io/GraspTTA/","accessed":{"date-parts":[["2024",11,22]]},"author":[{"family":"Jiang","given":"Hanwen"},{"family":"Liu","given":"Shaowei"},{"family":"Wang","given":"Jiashun"},{"family":"Wang","given":"Xiaolong"}],"citation-key":"jiangHandObjectContactConsistency2021","DOI":"10.48550/arXiv.2104.03304","issued":{"date-parts":[["2021",4,7]]},"number":"arXiv:2104.03304","publisher":"arXiv","source":"arXiv.org","title":"Hand-Object Contact Consistency Reasoning for Human Grasps Generation","type":"article","URL":"http://arxiv.org/abs/2104.03304"},
  {"id":"jiangProbabilisticAttentionModel2023","abstract":"Recently, deep learning based approaches have shown promising results in 3D hand reconstruction from a single RGB image. These approaches can be roughly divided into model-based approaches, which are heavily dependent on the model's parameter space, and model-free approaches, which require large numbers of 3D ground truths to reduce depth ambiguity and struggle in weakly-supervised scenarios. To overcome these issues, we propose a novel probabilistic model to achieve the robustness of model-based approaches and reduced dependence on the model's parameter space of model-free approaches. The proposed probabilistic model incorporates a model-based network as a prior-net to estimate the prior probability distribution of joints and vertices. An Attention-based Mesh Vertices Uncertainty Regression (AMVUR) model is proposed to capture dependencies among vertices and the correlation between joints and mesh vertices to improve their feature representation. We further propose a learning based occlusion-aware Hand Texture Regression model to achieve high-fidelity texture reconstruction. We demonstrate the flexibility of the proposed probabilistic model to be trained in both supervised and weakly-supervised scenarios. The experimental results demonstrate our probabilistic model's state-of-the-art accuracy in 3D hand and texture reconstruction from a single image in both training schemes, including in the presence of severe occlusions.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Jiang","given":"Zheheng"},{"family":"Rahmani","given":"Hossein"},{"family":"Black","given":"Sue"},{"family":"Williams","given":"Bryan M."}],"citation-key":"jiangProbabilisticAttentionModel2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"758-767","source":"openaccess.thecvf.com","title":"A Probabilistic Attention Model With Occlusion-Aware Texture Regression for 3D Hand Reconstruction From a Single RGB Image","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_A_Probabilistic_Attention_Model_With_Occlusion-Aware_Texture_Regression_for_3D_CVPR_2023_paper.html"},
  {"id":"karunratanakulGraspingFieldLearning2020","abstract":"Robotic grasping of house-hold objects has made remarkable progress in recent years. Yet, human grasps are still difficult to synthesize realistically. There are several key reasons: (1) the human hand has many degrees of freedom (more than robotic manipulators); (2) the synthesized hand should conform to the surface of the object; and (3) it should interact with the object in a semantically and physically plausible manner. To make progress in this direction, we draw inspiration from the recent progress on learning-based implicit representations for 3D object reconstruction. Specifically, we propose an expressive representation for human grasp modelling that is efficient and easy to integrate with deep neural networks. Our insight is that every point in a three-dimensional space can be characterized by the signed distances to the surface of the hand and the object, respectively. Consequently, the hand, the object, and the contact area can be represented by implicit surfaces in a common space, in which the proximity between the hand and the object can be modelled explicitly. We name this 3D to 2D mapping as Grasping Field, parameterize it with a deep neural network, and learn it from data. We demonstrate that the proposed grasping field is an effective and expressive representation for human grasp generation. Specifically, our generative model is able to synthesize high-quality human grasps, given only on a 3D object point cloud. The extensive experiments demonstrate that our generative model compares favorably with a strong baseline and approaches the level of natural human grasps. Our method improves the physical plausibility of the hand-object contact reconstruction and achieves comparable performance for 3D hand reconstruction compared to state-of-the-art methods.","accessed":{"date-parts":[["2024",11,22]]},"author":[{"family":"Karunratanakul","given":"Korrawe"},{"family":"Yang","given":"Jinlong"},{"family":"Zhang","given":"Yan"},{"family":"Black","given":"Michael"},{"family":"Muandet","given":"Krikamol"},{"family":"Tang","given":"Siyu"}],"citation-key":"karunratanakulGraspingFieldLearning2020","DOI":"10.48550/arXiv.2008.04451","issued":{"date-parts":[["2020",11,26]]},"number":"arXiv:2008.04451","publisher":"arXiv","source":"arXiv.org","title":"Grasping Field: Learning Implicit Representations for Human Grasps","title-short":"Grasping Field","type":"article","URL":"http://arxiv.org/abs/2008.04451"},
  {"id":"karunratanakulGuidedMotionDiffusion2023","abstract":"Denoising diffusion models have shown great promise in human motion synthesis conditioned on natural language descriptions. However, integrating spatial constraints, such as pre-defined motion trajectories and obstacles, remains a challenge despite being essential for bridging the gap between isolated human motion and its surrounding environment. To address this issue, we propose Guided Motion Diffusion (GMD), a method that incorporates spatial constraints into the motion generation process. Specifically, we propose an effective feature projection scheme that manipulates motion representation to enhance the coherency between spatial information and local poses. Together with a new imputation formulation, the generated motion can reliably conform to spatial constraints such as global motion trajectories. Furthermore, given sparse spatial constraints (e.g. sparse keyframes), we introduce a new dense guidance approach to turn a sparse signal, which is susceptible to being ignored during the reverse steps, into denser signals to guide the generated motion to the given constraints. Our extensive experiments justify the development of \\methodname, which achieves a significant improvement over state-of-the-art methods in text-based motion generation while allowing control of the synthesized motions with spatial constraints.","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Karunratanakul","given":"Korrawe"},{"family":"Preechakul","given":"Konpat"},{"family":"Suwajanakorn","given":"Supasorn"},{"family":"Tang","given":"Siyu"}],"citation-key":"karunratanakulGuidedMotionDiffusion2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"2151-2162","source":"openaccess.thecvf.com","title":"Guided Motion Diffusion for Controllable Human Motion Synthesis","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Karunratanakul_Guided_Motion_Diffusion_for_Controllable_Human_Motion_Synthesis_ICCV_2023_paper.html"},
  {"id":"karunratanakulHARPPersonalizedHand2023","abstract":"We present HARP (HAnd Reconstruction and Personalization), a personalized hand avatar creation approach that takes a short monocular RGB video of a human hand as input and reconstructs a faithful hand avatar exhibiting a high-fidelity appearance and geometry. In contrast to the major trend of neural implicit representations, HARP models a hand with a mesh-based parametric hand model, a vertex displacement map, a normal map, and an albedo without any neural components. The explicit nature of our representation enables a truly scalable, robust, and efficient approach to hand avatar creation as validated by our experiments. HARP is optimized via gradient descent from a short sequence captured by a hand-held mobile phone and can be directly used in AR/VR applications with real-time rendering capability. To enable this, we carefully design and implement a shadow-aware differentiable rendering scheme that is robust to high degree articulations and self-shadowing regularly present in hand motions, as well as challenging lighting conditions. It also generalizes to unseen poses and novel viewpoints, producing photo-realistic renderings of hand animations. Furthermore, the learned HARP representation can be used for improving 3D hand pose estimation quality in challenging viewpoints. The key advantages of HARP are validated by the in-depth analyses on appearance reconstruction, novel view and novel pose synthesis, and 3D hand pose refinement. It is an AR/VR-ready personalized hand representation that shows superior fidelity and scalability.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Karunratanakul","given":"Korrawe"},{"family":"Prokudin","given":"Sergey"},{"family":"Hilliges","given":"Otmar"},{"family":"Tang","given":"Siyu"}],"citation-key":"karunratanakulHARPPersonalizedHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12802-12813","source":"openaccess.thecvf.com","title":"HARP: Personalized Hand Reconstruction From a Monocular RGB Video","title-short":"HARP","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Karunratanakul_HARP_Personalized_Hand_Reconstruction_From_a_Monocular_RGB_Video_CVPR_2023_paper.html"},
  {"id":"karunratanakulSkeletonDrivenNeuralOccupancy2021","abstract":"We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference.","accessed":{"date-parts":[["2024",10,25]]},"author":[{"family":"Karunratanakul","given":"Korrawe"},{"family":"Spurr","given":"Adrian"},{"family":"Fan","given":"Zicong"},{"family":"Hilliges","given":"Otmar"},{"family":"Tang","given":"Siyu"}],"citation-key":"karunratanakulSkeletonDrivenNeuralOccupancy2021","issued":{"date-parts":[["2021",9,23]]},"number":"arXiv:2109.11399","publisher":"arXiv","source":"arXiv.org","title":"A Skeleton-Driven Neural Occupancy Representation for Articulated Hands","type":"article","URL":"http://arxiv.org/abs/2109.11399"},
  {"id":"khirodkarSapiensFoundationHuman2024","abstract":"We present Sapiens, a family of models for four fundamental human-centric vision tasks -- 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability -- model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error. Project page: https://about.meta.com/realitylabs/codecavatars/sapiens.","accessed":{"date-parts":[["2024",10,8]]},"author":[{"family":"Khirodkar","given":"Rawal"},{"family":"Bagautdinov","given":"Timur"},{"family":"Martinez","given":"Julieta"},{"family":"Zhaoen","given":"Su"},{"family":"James","given":"Austin"},{"family":"Selednik","given":"Peter"},{"family":"Anderson","given":"Stuart"},{"family":"Saito","given":"Shunsuke"}],"citation-key":"khirodkarSapiensFoundationHuman2024","DOI":"10.48550/arXiv.2408.12569","issued":{"date-parts":[["2024",8,26]]},"number":"arXiv:2408.12569","publisher":"arXiv","source":"arXiv.org","title":"Sapiens: Foundation for Human Vision Models","title-short":"Sapiens","type":"article","URL":"http://arxiv.org/abs/2408.12569"},
  {"id":"kulonWeaklySupervisedMeshConvolutionalHand2020","accessed":{"date-parts":[["2024",10,15]]},"author":[{"family":"Kulon","given":"Dominik"},{"family":"Guler","given":"Riza Alp"},{"family":"Kokkinos","given":"Iasonas"},{"family":"Bronstein","given":"Michael M."},{"family":"Zafeiriou","given":"Stefanos"}],"citation-key":"kulonWeaklySupervisedMeshConvolutionalHand2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"4990-5000","source":"openaccess.thecvf.com","title":"Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Kulon_Weakly-Supervised_Mesh-Convolutional_Hand_Reconstruction_in_the_Wild_CVPR_2020_paper.html"},
  {"id":"leeIm2HandsLearningAttentive2023","abstract":"We present Implicit Two Hands (Im2Hands), the first neural implicit representation of two interacting hands. Unlike existing methods on two-hand reconstruction that rely on a parametric hand model and/or low-resolution meshes, Im2Hands can produce fine-grained geometry of two hands with high hand-to-hand and hand-to-image coherency. To handle the shape complexity and interaction context between two hands, Im2Hands models the occupancy volume of two hands -- conditioned on an RGB image and coarse 3D keypoints -- by two novel attention-based modules responsible for (1) initial occupancy estimation and (2) context-aware occupancy refinement, respectively. Im2Hands first learns per-hand neural articulated occupancy in the canonical space designed for each hand using query-image attention. It then refines the initial two-hand occupancy in the posed space to enhance the coherency between the two hand shapes using query-anchor attention. In addition, we introduce an optional keypoint refinement module to enable robust two-hand shape estimation from predicted hand keypoints in a single-image reconstruction scenario. We experimentally demonstrate the effectiveness of Im2Hands on two-hand reconstruction in comparison to related methods, where ours achieves state-of-the-art results. Our code is publicly available at https://github.com/jyunlee/Im2Hands.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Lee","given":"Jihyun"},{"family":"Sung","given":"Minhyuk"},{"family":"Choi","given":"Honggyu"},{"family":"Kim","given":"Tae-Kyun"}],"citation-key":"leeIm2HandsLearningAttentive2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"21169-21178","source":"openaccess.thecvf.com","title":"Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes","title-short":"Im2Hands","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Lee_Im2Hands_Learning_Attentive_Implicit_Representation_of_Interacting_Two-Hand_Shapes_CVPR_2023_paper.html"},
  {"id":"leeInterHandGenTwoHandInteraction2024","abstract":"We present InterHandGen a novel framework that learns the generative prior of two-hand interaction. Sampling from our model yields plausible and diverse two-hand shapes in close interaction with or without an object. Our prior can be incorporated into any optimization or learning methods to reduce ambiguity in an ill-posed setup. Our key observation is that directly modeling the joint distribution of multiple instances imposes high learning complexity due to its combinatorial nature. Thus we propose to decompose the modeling of joint distribution into the modeling of factored unconditional and conditional single instance distribution. In particular we introduce a diffusion model that learns the single-hand distribution unconditional and conditional to another hand via conditioning dropout. For sampling we combine anti-penetration and classifier-free guidance to enable plausible generation. Furthermore we establish the rigorous evaluation protocol of two-hand synthesis where our method significantly outperforms baseline generative models in terms of plausibility and diversity. We also demonstrate that our diffusion prior can boost the performance of two-hand reconstruction from monocular in-the-wild images achieving new state-of-the-art accuracy.","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Lee","given":"Jihyun"},{"family":"Saito","given":"Shunsuke"},{"family":"Nam","given":"Giljoo"},{"family":"Sung","given":"Minhyuk"},{"family":"Kim","given":"Tae-Kyun"}],"citation-key":"leeInterHandGenTwoHandInteraction2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"527-537","source":"openaccess.thecvf.com","title":"InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion","title-short":"InterHandGen","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Lee_InterHandGen_Two-Hand_Interaction_Generation_via_Cascaded_Reverse_Diffusion_CVPR_2024_paper.html"},
  {"id":"lengDynamicHyperbolicAttention2023","abstract":"Reconstructing both objects and hands in 3D from a single RGB image is complex. Existing methods rely on manually defined hand-object constraints in Euclidean space, leading to suboptimal feature learning. Compared with Euclidean space, hyperbolic space better preserves the geometric properties of meshes thanks to its exponentially-growing space distance, which amplifies the differences between the features based on similarity. In this work, we propose the first precise hand-object reconstruction method in hyperbolic space, namely Dynamic Hyperbolic Attention Network (DHANet), which leverages intrinsic properties of hyperbolic space to learn representative features. Our method that projects mesh and image features into a unified hyperbolic space includes two modules, i.e. dynamic hyperbolic graph convolution and image-attention hyperbolic graph convolution. With these two modules, our method learns mesh features with rich geometry-image multi-modal information and models better hand-object interaction. Our method provides a promising alternative for fine hand-object reconstruction in hyperbolic space. Extensive experiments on three public datasets demonstrate that our method outperforms most state-of-the-art methods.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Leng","given":"Zhiying"},{"family":"Wu","given":"Shun-Cheng"},{"family":"Saleh","given":"Mahdi"},{"family":"Montanaro","given":"Antonio"},{"family":"Yu","given":"Hao"},{"family":"Wang","given":"Yin"},{"family":"Navab","given":"Nassir"},{"family":"Liang","given":"Xiaohui"},{"family":"Tombari","given":"Federico"}],"citation-key":"lengDynamicHyperbolicAttention2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"14894-14904","source":"openaccess.thecvf.com","title":"Dynamic Hyperbolic Attention Network for Fine Hand-object Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Leng_Dynamic_Hyperbolic_Attention_Network_for_Fine_Hand-object_Reconstruction_ICCV_2023_paper.html"},
  {"id":"leonardiAreSyntheticData","abstract":"In this study, we investigate the effectiveness of synthetic data in enhancing egocentric hand-object interaction detection. Via extensive experiments and comparative analyses on three egocentric datasets, VISOR, EgoHOS, and ENIGMA-51, our findings reveal how to exploit synthetic data for the HOI detection task when real labeled data are scarce or unavailable. Specifically, by leveraging only 10% of real labeled data, we achieve improvements in Overall AP compared to baselines trained exclusively on real data of: +5.67% on EPIC-KITCHENS VISOR, +8.24% on EgoHOS, and +11.69% on ENIGMA-51. Our analysis is supported by a novel data generation pipeline and the newly introduced HOI-Synth benchmark which augments existing datasets with synthetic images of hand-object interactions automatically labeled with hand-object contact states, bounding boxes, and pixel-wise segmentation masks. Data, code, and data generation tools to support future research are released at: https://fpv-iplab.github.io/HOI-Synth/.","author":[{"family":"Leonardi","given":"Rosario"},{"family":"Furnari","given":"Antonino"},{"family":"Ragusa","given":"Francesco"},{"family":"Farinella","given":"Giovanni Maria"}],"citation-key":"leonardiAreSyntheticData","language":"en","source":"Zotero","title":"Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection?","type":"article-journal"},
  {"id":"liCHORDCategorylevelHandheld2023","abstract":"In daily life, humans utilize hands to manipulate objects. Modeling the shape of objects that are manipulated by the hand is essential for AI to comprehend daily tasks and to learn manipulation skills. However, previous approaches have encountered difficulties in reconstructing the precise shapes of hand-held objects, primarily owing to a deficiency in prior shape knowledge and inadequate data for training. As illustrated, given a particular type of tool, such as a mug, despite its infinite variations in shape and appearance, humans have a limited number of 'effective' modes and poses for its manipulation. This can be attributed to the fact that humans have mastered the shape prior of the 'mug' category, and can quickly establish the corresponding relations between different mug instances and the prior, such as where the rim and handle are located. In light of this, we propose a new method, CHORD, for Category-level Hand-held Object Reconstruction via shape Deformation. CHORD deforms a categorical shape prior for reconstructing the intra-class objects. To ensure accurate reconstruction, we empower CHORD with three types of awareness: appearance, shape, and interacting pose. In addition, we have constructed a new dataset, COMIC, of category-level hand-object interaction. COMIC contains a rich array of object instances, materials, hand interactions, and viewing directions. Extensive evaluation shows that CHORD outperforms state-of-the-art approaches in both quantitative and qualitative measures. Code, model, and datasets are available at https://kailinli.github.io/CHORD.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Li","given":"Kailin"},{"family":"Yang","given":"Lixin"},{"family":"Zhen","given":"Haoyu"},{"family":"Lin","given":"Zenan"},{"family":"Zhan","given":"Xinyu"},{"family":"Zhong","given":"Licheng"},{"family":"Xu","given":"Jian"},{"family":"Wu","given":"Kejian"},{"family":"Lu","given":"Cewu"}],"citation-key":"liCHORDCategorylevelHandheld2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"9444-9454","source":"openaccess.thecvf.com","title":"CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation","title-short":"CHORD","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Li_CHORD_Category-level_Hand-held_Object_Reconstruction_via_Shape_Deformation_ICCV_2023_paper.html"},
  {"id":"liHHMRHolisticHand2024","abstract":"Recent years have witnessed a trend of the deep integration of the generation and reconstruction paradigms. In this paper we extend the ability of controllable generative models for a more comprehensive hand mesh recovery task: direct hand mesh generation inpainting reconstruction and fitting in a single framework which we name as Holistic Hand Mesh Recovery (HHMR). Our key observation is that different kinds of hand mesh recovery tasks can be achieved by a single generative model with strong multimodal controllability and in such a framework realizing different tasks only requires giving different signals as conditions. To achieve this goal we propose an all-in-one diffusion framework based on graph convolution and attention mechanisms for holistic hand mesh recovery. In order to achieve strong control generation capability while ensuring the decoupling of multimodal control signals we map different modalities to a share feature space and apply cross-scale random masking in both modality and feature levels. In this way the correlation between different modalities can be fully exploited during the learning of hand priors. Furthermore we propose Condition-aligned Gradient Guidance to enhance the alignment of the generated model with the control signals which significantly improves the accuracy of the hand mesh reconstruction and fitting. Experiments show that our novel framework can realize multiple hand mesh recovery tasks simultaneously and outperform the existing methods in different tasks which provides more possibilities for subsequent downstream applications including gesture recognition pose generation mesh editing and so on.","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Li","given":"Mengcheng"},{"family":"Zhang","given":"Hongwen"},{"family":"Zhang","given":"Yuxiang"},{"family":"Shao","given":"Ruizhi"},{"family":"Yu","given":"Tao"},{"family":"Liu","given":"Yebin"}],"citation-key":"liHHMRHolisticHand2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"645-654","source":"openaccess.thecvf.com","title":"HHMR: Holistic Hand Mesh Recovery by Enhancing the Multimodal Controllability of Graph Diffusion Models","title-short":"HHMR","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Li_HHMR_Holistic_Hand_Mesh_Recovery_by_Enhancing_the_Multimodal_Controllability_CVPR_2024_paper.html"},
  {"id":"liInteractingAttentionGraph2022","abstract":"Graph convolutional network (GCN) has achieved great success in single hand reconstruction task, while interacting two-hand reconstruction by GCN remains unexplored. In this paper, we present Interacting Attention Graph Hand (IntagHand), the first graph convolution based network that reconstructs two interacting hands from a single RGB image. To solve occlusion and interaction challenges of two-hand reconstruction, we introduce two novel attention based modules in each upsampling step of the original GCN. The first module is the pyramid image feature attention (PIFA) module, which utilizes multiresolution features to implicitly obtain vertex-to-image alignment. The second module is the cross hand attention (CHA) module that encodes the coherence of interacting hands by building dense cross-attention between two hand vertices. As a result, our model outperforms all existing two-hand reconstruction methods by a large margin on InterHand2.6M benchmark. Moreover, ablation studies verify the effectiveness of both PIFA and CHA modules for improving the reconstruction accuracy. Results on in-the-wild images and live video streams further demonstrate the generalization ability of our network. Our code is available at https://github.com/Dw1010/IntagHand.","accessed":{"date-parts":[["2024",11,15]]},"author":[{"family":"Li","given":"Mengcheng"},{"family":"An","given":"Liang"},{"family":"Zhang","given":"Hongwen"},{"family":"Wu","given":"Lianpeng"},{"family":"Chen","given":"Feng"},{"family":"Yu","given":"Tao"},{"family":"Liu","given":"Yebin"}],"citation-key":"liInteractingAttentionGraph2022","container-title":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR52688.2022.00278","event-place":"New Orleans, LA, USA","event-title":"2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-66546-946-3","issued":{"date-parts":[["2022",6]]},"language":"en","license":"https://doi.org/10.15223/policy-029","page":"2751-2760","publisher":"IEEE","publisher-place":"New Orleans, LA, USA","source":"DOI.org (Crossref)","title":"Interacting Attention Graph for Single Image Two-Hand Reconstruction","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9880324/"},
  {"id":"linCrossDomain3DHand2023","abstract":"Recent advances in hand pose estimation have shed light on utilizing synthetic data to train neural networks, which however inevitably hinders generalization to real-world data due to domain gaps. To solve this problem, we present a framework for cross-domain semi-supervised hand pose estimation and target the challenging scenario of learning models from labelled multi-modal synthetic data and unlabelled real-world data. To that end, we propose a dual-modality network that exploits synthetic RGB and synthetic depth images. For pre-training, our network uses multi-modal contrastive learning and attention-fused supervision to learn effective representations of the RGB images. We then integrate a novel self-distillation technique during fine-tuning to reduce pseudo-label noise. Experiments show that the proposed method significantly improves 3D hand pose estimation and 2D keypoint detection on benchmarks.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Lin","given":"Qiuxia"},{"family":"Yang","given":"Linlin"},{"family":"Yao","given":"Angela"}],"citation-key":"linCrossDomain3DHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"17184-17193","source":"openaccess.thecvf.com","title":"Cross-Domain 3D Hand Pose Estimation With Dual Modalities","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Cross-Domain_3D_Hand_Pose_Estimation_With_Dual_Modalities_CVPR_2023_paper.html"},
  {"id":"linHarmoniousFeatureLearning2023","abstract":"Joint hand and object pose estimation from a single image is extremely challenging as serious occlusion often occurs when the hand and object interact. Existing approaches typically first extract coarse hand and object features from a single backbone, then further enhance them with reference to each other via interaction modules. However, these works usually ignore that the hand and object are competitive in feature learning, since the backbone takes both of them as foreground and they are usually mutually occluded. In this paper, we propose a novel Harmonious Feature Learning Network (HFL-Net). HFL-Net introduces a new framework that combines the advantages of single- and double-stream backbones: it shares the parameters of the low- and high-level convolutional layers of a common ResNet-50 model for the hand and object, leaving the middle-level layers unshared. This strategy enables the hand and the object to be extracted as the sole targets by the middle-level layers, avoiding their competition in feature learning. The shared high-level layers also force their features to be harmonious, thereby facilitating their mutual feature enhancement. In particular, we propose to enhance the feature of the hand via concatenation with the feature in the same location from the object stream. A subsequent self-attention layer is adopted to deeply fuse the concatenated feature. Experimental results show that our proposed approach consistently outperforms state-of-the-art methods on the popular HO3D and Dex-YCB databases. Notably, the performance of our model on hand pose estimation even surpasses that of existing works that only perform the single-hand pose estimation task. Code is available at https://github.com/lzfff12/HFL-Net.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Lin","given":"Zhifeng"},{"family":"Ding","given":"Changxing"},{"family":"Yao","given":"Huan"},{"family":"Kuang","given":"Zengsheng"},{"family":"Huang","given":"Shaoli"}],"citation-key":"linHarmoniousFeatureLearning2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12989-12998","source":"openaccess.thecvf.com","title":"Harmonious Feature Learning for Interactive Hand-Object Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Harmonious_Feature_Learning_for_Interactive_Hand-Object_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"linMeshGraphormer2021","accessed":{"date-parts":[["2024",9,30]]},"author":[{"family":"Lin","given":"Kevin"},{"family":"Wang","given":"Lijuan"},{"family":"Liu","given":"Zicheng"}],"citation-key":"linMeshGraphormer2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"12939-12948","source":"openaccess.thecvf.com","title":"Mesh Graphormer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Lin_Mesh_Graphormer_ICCV_2021_paper.html"},
  {"id":"liRenderIHLargeScaleSynthetic2023","abstract":"The current interacting hand (IH) datasets are relatively simplistic in terms of background and texture, with hand joints being annotated by a machine annotator, which may result in inaccuracies, and the diversity of pose distribution is limited. However, the variability of background, pose distribution, and texture can greatly influence the generalization ability. Therefore, we present a large-scale synthetic dataset --RenderIH-- for interacting hands with accurate and diverse pose annotations. The dataset contains 1M photo-realistic images with varied backgrounds, perspectives, and hand textures. To generate natural and diverse interacting poses, we propose a new pose optimization algorithm. Additionally, for better pose estimation accuracy, we introduce a transformer-based pose estimation network, TransHand, to leverage the correlation between interacting hands and verify the effectiveness of RenderIH in improving results. Our dataset is model-agnostic and can improve more accuracy of any hand pose estimation method in comparison to other real or synthetic datasets. Experiments have shown that pretraining on our synthetic data can significantly decrease the error from 6.76mm to 5.79mm, and our Transhand surpasses contemporary methods. Our dataset and code are available at https://github.com/adwardlee/RenderIH.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Li","given":"Lijun"},{"family":"Tian","given":"Linrui"},{"family":"Zhang","given":"Xindi"},{"family":"Wang","given":"Qi"},{"family":"Zhang","given":"Bang"},{"family":"Bo","given":"Liefeng"},{"family":"Liu","given":"Mengyuan"},{"family":"Chen","given":"Chen"}],"citation-key":"liRenderIHLargeScaleSynthetic2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"20395-20405","source":"openaccess.thecvf.com","title":"RenderIH: A Large-Scale Synthetic Dataset for 3D Interacting Hand Pose Estimation","title-short":"RenderIH","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Li_RenderIH_A_Large-Scale_Synthetic_Dataset_for_3D_Interacting_Hand_Pose_ICCV_2023_paper.html"},
  {"id":"liuCoarseFineImplicitRepresentation","abstract":"Recent research has explored implicit representations, such as signed distance function (SDF), for interacting hand-object reconstruction. SDF enables modeling hand-held objects with arbitrary topology and overcomes the resolution limitations of parametric models, allowing for finer-grained reconstruction. However, directly modeling detailed SDFs from visual clues presents challenges due to depth ambiguity and appearance similarity, especially in cluttered real-world scenes. In this paper, we propose a coarse-to-fine SDF framework for 3D hand-object reconstruction, which leverages the perceptual advantages of RGB-D modality in visual and geometric aspects, to progressively model the implicit field. Specifically, we model a coarse SDF for visual perception of overall scenes. Then, we propose a 3D Point-Aligned Implicit Function (3D PIFu) for fine-level SDF learning, which leverages both local geometric clues and the coarse-level visual priors to capture intricate details. Additionally, we propose a surface-aware efficient reconstruction strategy that sparsely performs SDF query based on the hand-object semantic priors. Experiments on two challenging hand-object datasets show that our method outperforms existing methods by a large margin.","author":[{"family":"Liu","given":"Xingyu"},{"family":"Ren","given":"Pengfei"},{"family":"Wang","given":"Jingyu"},{"family":"Qi","given":"Qi"},{"family":"Sun","given":"Haifeng"},{"family":"Liao","given":"Jianxin"}],"citation-key":"liuCoarseFineImplicitRepresentation","language":"en","source":"Zotero","title":"Coarse-to-Fine Implicit Representation Learning for 3D Hand-Object Reconstruction from a Single RGB-D Image","type":"article-journal"},
  {"id":"liuSingleDualViewAdaptationEgocentric2024","abstract":"The pursuit of accurate 3D hand pose estimation stands as a keystone for understanding human activity in the realm of egocentric vision. The majority of existing estimation methods still rely on single-view images as input, leading to potential limitations, e.g., limited field-of-view and ambiguity in depth. To address these problems, adding another camera to better capture the shape of hands is a practical direction. However, existing multi-view hand pose estimation methods suffer from two main drawbacks: 1) Requiring multi-view annotations for training, which are expensive. 2) During testing, the model becomes inapplicable if camera parameters/layout are not the same as those used in training. In this paper, we propose a novel Singleto-Dual-view adaptation (S2DHand) solution that adapts a pre-trained single-view estimator to dual views. Compared with existing multi-view training methods, 1) our adaptation process is unsupervised, eliminating the need for multiview annotation. 2) Moreover, our method can handle arbitrary dual-view pairs with unknown camera parameters, making the model applicable to diverse camera settings. Specifically, S2DHand is built on certain stereo constraints, including pair-wise cross-view consensus and invariance of transformation between both views. These two stereo constraints are used in a complementary manner to generate pseudo-labels, allowing reliable adaptation. Evaluation results reveal that S2DHand achieves significant improvements on arbitrary camera pairs under both in-dataset and cross-dataset settings, and outperforms existing adaptation methods with leading performance. Project page: https://github.com/ut-vision/S2DHand.","accessed":{"date-parts":[["2024",10,8]]},"author":[{"family":"Liu","given":"Ruicong"},{"family":"Ohkawa","given":"Takehiko"},{"family":"Zhang","given":"Mingfang"},{"family":"Sato","given":"Yoichi"}],"citation-key":"liuSingleDualViewAdaptationEgocentric2024","container-title":"2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR52733.2024.00071","event-place":"Seattle, WA, USA","event-title":"2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"9798350353006","issued":{"date-parts":[["2024",6,16]]},"language":"en","license":"https://doi.org/10.15223/policy-029","page":"677-686","publisher":"IEEE","publisher-place":"Seattle, WA, USA","source":"DOI.org (Crossref)","title":"Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10656245/"},
  {"id":"liuVMambaVisualState2024","abstract":"Designing computationally efficient network architectures persists as an ongoing necessity in computer vision. In this paper, we transplant Mamba, a state-space language model, into VMamba, a vision backbone that works in linear time complexity. At the core of VMamba lies a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D helps bridge the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the gathering of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments showcase VMamba's promising performance across diverse visual perception tasks, highlighting its advantages in input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba.","accessed":{"date-parts":[["2024",10,17]]},"author":[{"family":"Liu","given":"Yue"},{"family":"Tian","given":"Yunjie"},{"family":"Zhao","given":"Yuzhong"},{"family":"Yu","given":"Hongtian"},{"family":"Xie","given":"Lingxi"},{"family":"Wang","given":"Yaowei"},{"family":"Ye","given":"Qixiang"},{"family":"Liu","given":"Yunfan"}],"citation-key":"liuVMambaVisualState2024","issued":{"date-parts":[["2024",5,26]]},"number":"arXiv:2401.10166","publisher":"arXiv","source":"arXiv.org","title":"VMamba: Visual State Space Model","title-short":"VMamba","type":"article","URL":"http://arxiv.org/abs/2401.10166"},
  {"id":"loperSMPLSkinnedMultiperson2015","abstract":"We present a learned model of human body shape and pose-dependent shape variation that is more accurate than previous models and is compatible with existing graphics pipelines. Our Skinned Multi-Person Linear model (SMPL) is a skinned vertex-based model that accurately represents a wide variety of body shapes in natural human poses. The parameters of the model are learned from data including the rest pose template, blend weights, pose-dependent blend shapes, identity-dependent blend shapes, and a regressor from vertices to joint locations. Unlike previous models, the pose-dependent blend shapes are a linear function of the elements of the pose rotation matrices. This simple formulation enables training the entire model from a relatively large number of aligned 3D meshes of different people in different poses. We quantitatively evaluate variants of SMPL using linear or dual-quaternion blend skinning and show that both are more accurate than a Blend-SCAPE model trained on the same data. We also extend SMPL to realistically model dynamic soft-tissue deformations. Because it is based on blend skinning, SMPL is compatible with existing rendering engines and we make it available for research purposes.","accessed":{"date-parts":[["2024",11,12]]},"author":[{"family":"Loper","given":"Matthew"},{"family":"Mahmood","given":"Naureen"},{"family":"Romero","given":"Javier"},{"family":"Pons-Moll","given":"Gerard"},{"family":"Black","given":"Michael J."}],"citation-key":"loperSMPLSkinnedMultiperson2015","container-title":"ACM Trans. Graph.","DOI":"10.1145/2816795.2818013","ISSN":"0730-0301","issue":"6","issued":{"date-parts":[["2015",10,26]]},"page":"248:1‚Äì248:16","source":"ACM Digital Library","title":"SMPL: a skinned multi-person linear model","title-short":"SMPL","type":"article-journal","URL":"https://dl.acm.org/doi/10.1145/2816795.2818013","volume":"34"},
  {"id":"luanHighFidelity3D2023","abstract":"Despite the impressive performance obtained by recent single-image hand modeling techniques, they lack the capability to capture sufficient details of the 3D hand mesh. This deficiency greatly limits their applications when high fidelity hand modeling is required, e.g., personalized hand modeling. To address this problem, we design a frequency split network to generate 3D hand mesh using different frequency bands in a coarse-to-fine manner. To capture high-frequency personalized details, we transform the 3D mesh into the frequency domain, and propose a novel frequency decomposition loss to supervise each frequency component. By leveraging such a coarse-to-fine scheme, hand details that correspond to the higher frequency domain can be preserved. In addition, the proposed network is scalable, and can stop the inference at any resolution level to accommodate different hardwares with varying computational powers. To quantitatively evaluate the performance of our method in terms of recovering personalized shape details, we introduce a new evaluation metric named Mean Signal-to-Noise Ratio (MSNR) to measure the signal-to-noise ratio of each mesh frequency component. Extensive experiments demonstrate that our approach generates fine-grained details for high fidelity 3D hand reconstruction, and our evaluation metric is more effective for measuring mesh details compared with traditional metrics.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Luan","given":"Tianyu"},{"family":"Zhai","given":"Yuanhao"},{"family":"Meng","given":"Jingjing"},{"family":"Li","given":"Zhong"},{"family":"Chen","given":"Zhang"},{"family":"Xu","given":"Yi"},{"family":"Yuan","given":"Junsong"}],"citation-key":"luanHighFidelity3D2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"16795-16804","source":"openaccess.thecvf.com","title":"High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Luan_High_Fidelity_3D_Hand_Shape_Reconstruction_via_Scalable_Graph_Frequency_CVPR_2023_paper.html"},
  {"id":"luoPhysicsawareHandobjectInteraction2024","abstract":"The credibility and practicality of a reconstructed hand-object interaction sequence depend largely on its physical plausibility. However, due to high occlusions during hand-object interaction, physical plausibility remains a challenging criterion for purely vision-based tracking methods. To address this issue and enhance the results of existing hand trackers, this paper proposes a novel physically-aware hand motion de-noising method. Specifically, we introduce two learned loss terms that explicitly capture two crucial aspects of physical plausibility: grasp credibility and manipulation feasibility. These terms are used to train a physically-aware de-noising network. Qualitative and quantitative experiments demonstrate that our approach significantly improves both fine-grained physical plausibility and overall pose accuracy, surpassing current state-of-the-art de-noising methods.","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Luo","given":"Haowen"},{"family":"Liu","given":"Yunze"},{"family":"Yi","given":"Li"}],"citation-key":"luoPhysicsawareHandobjectInteraction2024","DOI":"10.48550/arXiv.2405.11481","issued":{"date-parts":[["2024",5,19]]},"number":"arXiv:2405.11481","publisher":"arXiv","source":"arXiv.org","title":"Physics-aware Hand-object Interaction Denoising","type":"article","URL":"http://arxiv.org/abs/2405.11481"},
  {"id":"ManipNetNeuralManipulation","abstract":"In this paper, we propose a hand-object spatial representation that can achieve generalization from limited data. Our representation combines the global object shape as voxel occupancies with local geometric details as samples of closest distances. This representation is used by a neural network to regress finger motions from input trajectories of wrists and objects.","accessed":{"date-parts":[["2024",10,9]]},"citation-key":"ManipNetNeuralManipulation","container-title":"Meta Research","language":"en","title":"ManipNet: Neural Manipulation Synthesis with a Hand-Object Spatial Representation - Meta Research","title-short":"ManipNet","type":"webpage","URL":"https://research.facebook.com/publications/manipnet-neural-manipulation-synthesis-with-a-hand-object-spatial-representation/"},
  {"id":"mildenhallNeRFRepresentingScenes2020","abstract":"We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.","accessed":{"date-parts":[["2024",10,4]]},"author":[{"family":"Mildenhall","given":"Ben"},{"family":"Srinivasan","given":"Pratul P."},{"family":"Tancik","given":"Matthew"},{"family":"Barron","given":"Jonathan T."},{"family":"Ramamoorthi","given":"Ravi"},{"family":"Ng","given":"Ren"}],"citation-key":"mildenhallNeRFRepresentingScenes2020","container-title":"arXiv.org","issued":{"date-parts":[["2020",3,19]]},"language":"en","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","title-short":"NeRF","type":"webpage","URL":"https://arxiv.org/abs/2003.08934v2"},
  {"id":"mokhtarHandMeThis2024","abstract":"\"What you see is what you get\" is a concept in eye gaze interaction of selecting only with your eye. Technologies such as Virtual and Augmented Reality (VR/AR) often exclude people with limited mobility due to their dependence on controllers or hand movement. Meanwhile, embodiment focuses on a one-to-one mapping of the body &amp; hands. However, previous work did not connect embodiment and accessibility with the aid of eye-gaze interaction. In this project, we explore how different gaze-controlled hand representations and animations affect users‚Äô reported presence and embodiment that current technology lacks and can be crucially important, in virtual environments. Our data showed significant differences in presence and self-location(embodiment), especially when comparing Dwell and Snap animations.","accessed":{"date-parts":[["2024",10,10]]},"author":[{"family":"Mokhtar","given":"Noha"},{"family":"Esteves","given":"Augusto"}],"citation-key":"mokhtarHandMeThis2024","collection-title":"ETRA '24","container-title":"Proceedings of the 2024 Symposium on Eye Tracking Research and Applications","DOI":"10.1145/3649902.3656362","event-place":"New York, NY, USA","ISBN":"9798400706073","issued":{"date-parts":[["2024",6,4]]},"page":"1‚Äì7","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Hand Me This: Exploring the Effects of Gaze-driven Animations and Hand Representations in Users‚Äô Sense of Presence and Embodiment","title-short":"Hand Me This","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3649902.3656362"},
  {"id":"moonDatasetRelighted3D2023a","abstract":"The two-hand interaction is one of the most challenging signals to analyze due to the self-similarity, complicated articulations, and occlusions of hands. Although several datasets have been proposed for the two-hand interaction analysis, all of them do not achieve 1) diverse and realistic image appearances and 2) diverse and large-scale groundtruth (GT) 3D poses at the same time. In this work, we propose Re:InterHand, a dataset of relighted 3D interacting hands that achieve the two goals. To this end, we employ a state-of-the-art hand relighting network with our accurately tracked two-hand 3D poses. We compare our Re:InterHand with existing 3D interacting hands datasets and show the benefit of it. Our Re:InterHand is available in https://mks0601.github.io/ReInterHand/.","accessed":{"date-parts":[["2024",10,30]]},"author":[{"family":"Moon","given":"Gyeongsik"},{"family":"Saito","given":"Shunsuke"},{"family":"Xu","given":"Weipeng"},{"family":"Joshi","given":"Rohan"},{"family":"Buffalini","given":"Julia"},{"family":"Bellan","given":"Harley"},{"family":"Rosen","given":"Nicholas"},{"family":"Richardson","given":"Jesse"},{"family":"Mize","given":"Mallorie"},{"family":"Bree","given":"Philippe","dropping-particle":"de"},{"family":"Simon","given":"Tomas"},{"family":"Peng","given":"Bo"},{"family":"Garg","given":"Shubham"},{"family":"McPhail","given":"Kevyn"},{"family":"Shiratori","given":"Takaaki"}],"citation-key":"moonDatasetRelighted3D2023a","DOI":"10.48550/arXiv.2310.17768","issued":{"date-parts":[["2023",10,26]]},"number":"arXiv:2310.17768","publisher":"arXiv","source":"arXiv.org","title":"A Dataset of Relighted 3D Interacting Hands","type":"article","URL":"http://arxiv.org/abs/2310.17768"},
  {"id":"moonDeepHandMeshWeaklysupervisedDeep2020","abstract":"Human hands play a central role in interacting with other people and objects. For realistic replication of such hand motions, high-fidelity hand meshes have to be reconstructed. In this study, we firstly propose DeepHandMesh, a weakly-supervised deep encoder-decoder framework for high-fidelity hand mesh modeling. We design our system to be trained in an end-to-end and weakly-supervised manner; therefore, it does not require groundtruth meshes. Instead, it relies on weaker supervisions such as 3D joint coordinates and multi-view depth maps, which are easier to get than groundtruth meshes and do not dependent on the mesh topology. Although the proposed DeepHandMesh is trained in a weakly-supervised way, it provides significantly more realistic hand mesh than previous fully-supervised hand models. Our newly introduced penetration avoidance loss further improves results by replicating physical interaction between hand parts. Finally, we demonstrate that our system can also be applied successfully to the 3D hand mesh estimation from general images. Our hand model, dataset, and codes are publicly available at https://mks0601.github.io/DeepHandMesh/.","accessed":{"date-parts":[["2024",11,5]]},"author":[{"family":"Moon","given":"Gyeongsik"},{"family":"Shiratori","given":"Takaaki"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"moonDeepHandMeshWeaklysupervisedDeep2020","DOI":"10.48550/arXiv.2008.08213","issued":{"date-parts":[["2020",8,19]]},"number":"arXiv:2008.08213","publisher":"arXiv","source":"arXiv.org","title":"DeepHandMesh: A Weakly-supervised Deep Encoder-Decoder Framework for High-fidelity Hand Mesh Modeling","title-short":"DeepHandMesh","type":"article","URL":"http://arxiv.org/abs/2008.08213"},
  {"id":"moonInterHand26MDatasetBaseline2020","abstract":"Analysis of hand-hand interactions is a crucial step towards better understanding human behavior. However, most researches in 3D hand pose estimation have focused on the isolated single hand case. Therefore, we firstly propose (1) a large-scale dataset, InterHand2.6M, and (2) a baseline network, InterNet, for 3D interacting hand pose estimation from a single RGB image. The proposed InterHand2.6M consists of \\textbf{2.6M labeled single and interacting hand frames} under various poses from multiple subjects. Our InterNet simultaneously performs 3D single and interacting hand pose estimation. In our experiments, we demonstrate big gains in 3D interacting hand pose estimation accuracy when leveraging the interacting hand data in InterHand2.6M. We also report the accuracy of InterNet on InterHand2.6M, which serves as a strong baseline for this new dataset. Finally, we show 3D interacting hand pose estimation results from general images. Our code and dataset are available at https://mks0601.github.io/InterHand2.6M/.","accessed":{"date-parts":[["2024",10,30]]},"author":[{"family":"Moon","given":"Gyeongsik"},{"family":"Yu","given":"Shoou-i"},{"family":"Wen","given":"He"},{"family":"Shiratori","given":"Takaaki"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"moonInterHand26MDatasetBaseline2020","DOI":"10.48550/arXiv.2008.09309","issued":{"date-parts":[["2020",8,21]]},"number":"arXiv:2008.09309","publisher":"arXiv","source":"arXiv.org","title":"InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image","title-short":"InterHand2.6M","type":"article","URL":"http://arxiv.org/abs/2008.09309"},
  {"id":"moWhere2ActPixelsActions2021","abstract":"One of the fundamental goals of visual perception is to allow agents to meaningfully interact with their environment. In this paper, we take a step towards that long-term goal -- we extract highly localized actionable information related to elementary actions such as pushing or pulling for articulated objects with movable parts. For example, given a drawer, our network predicts that applying a pulling force on the handle opens the drawer. We propose, discuss, and evaluate novel network architectures that given image and depth data, predict the set of actions possible at each pixel, and the regions over articulated parts that are likely to move under the force. We propose a learning-from-interaction framework with an online data sampling strategy that allows us to train the network in simulation (SAPIEN) and generalizes across categories. Check the website for code and data release: https://cs.stanford.edu/~kaichun/where2act/","accessed":{"date-parts":[["2024",11,22]]},"author":[{"family":"Mo","given":"Kaichun"},{"family":"Guibas","given":"Leonidas"},{"family":"Mukadam","given":"Mustafa"},{"family":"Gupta","given":"Abhinav"},{"family":"Tulsiani","given":"Shubham"}],"citation-key":"moWhere2ActPixelsActions2021","DOI":"10.48550/arXiv.2101.02692","issued":{"date-parts":[["2021",8,10]]},"number":"arXiv:2101.02692","publisher":"arXiv","source":"arXiv.org","title":"Where2Act: From Pixels to Actions for Articulated 3D Objects","title-short":"Where2Act","type":"article","URL":"http://arxiv.org/abs/2101.02692"},
  {"id":"mundraLiveHandRealtimePhotorealistic2023","abstract":"The human hand is the main medium through which we interact with our surroundings, making its digitization an important problem. While there are several works modeling the geometry of hands, little attention has been paid to capturing photo-realistic appearance. Moreover, for applications in extended reality and gaming, real-time rendering is critical. We present the first neural-implicit approach to photo-realistically render hands in real-time. This is a challenging problem as hands are textured and undergo strong articulations with pose-dependent effects. However, we show that this aim is achievable through our carefully designed method. This includes training on a low-resolution rendering of a neural radiance field, together with a 3D-consistent super-resolution module and mesh-guided sampling and space canonicalization. We demonstrate a novel application of perceptual loss on the image space, which is critical for learning details accurately. We also show a live demo where we photo-realistically render the human hand in real-time for the first time, while also modeling pose- and view-dependent appearance effects. We ablate all our design choices and show that they optimize for rendering speed and quality.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Mundra","given":"Akshay"},{"family":"R","given":"Mallikarjun B."},{"family":"Wang","given":"Jiayi"},{"family":"Habermann","given":"Marc"},{"family":"Theobalt","given":"Christian"},{"family":"Elgharib","given":"Mohamed"}],"citation-key":"mundraLiveHandRealtimePhotorealistic2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"18035-18045","source":"openaccess.thecvf.com","title":"LiveHand: Real-time and Photorealistic Neural Hand Rendering","title-short":"LiveHand","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Mundra_LiveHand_Real-time_and_Photorealistic_Neural_Hand_Rendering_ICCV_2023_paper.html"},
  {"id":"nagarajanEGOTOPOEnvironmentAffordances2020","abstract":"First-person video naturally brings the use of a physical environment to the forefront, since it shows the camera wearer interacting fluidly in a space based on his intentions. However, current methods largely separate the observed actions from the persistent space itself. We introduce a model for environment affordances that is learned directly from egocentric video. The main idea is to gain a human-centric model of a physical space (such as a kitchen) that captures (1) the primary spatial zones of interaction and (2) the likely activities they support. Our approach decomposes a space into a topological map derived from first-person activity, organizing an ego-video into a series of visits to the different zones. Further, we show how to link zones across multiple related environments (e.g., from videos of multiple kitchens) to obtain a consolidated representation of environment functionality. On EPIC-Kitchens and EGTEA+, we demonstrate our approach for learning scene affordances and anticipating future actions in long-form video.","accessed":{"date-parts":[["2024",11,22]]},"author":[{"family":"Nagarajan","given":"Tushar"},{"family":"Li","given":"Yanghao"},{"family":"Feichtenhofer","given":"Christoph"},{"family":"Grauman","given":"Kristen"}],"citation-key":"nagarajanEGOTOPOEnvironmentAffordances2020","DOI":"10.48550/arXiv.2001.04583","issued":{"date-parts":[["2020",3,27]]},"number":"arXiv:2001.04583","publisher":"arXiv","source":"arXiv.org","title":"EGO-TOPO: Environment Affordances from Egocentric Video","title-short":"EGO-TOPO","type":"article","URL":"http://arxiv.org/abs/2001.04583"},
  {"id":"nguyenS4NDModelingImages2022","abstract":"Visual data such as images and videos are typically modeled as discretizations of inherently continuous, multidimensional signals. Existing continuous-signal models attempt to exploit this fact by modeling the underlying signals of visual (e.g., image) data directly. However, these models have not yet been able to achieve competitive performance on practical vision tasks such as large-scale image and video classification. Building on a recent line of work on deep state space models (SSMs), we propose S4ND, a new multidimensional SSM layer that extends the continuous-signal modeling ability of SSMs to multidimensional data including images and videos. We show that S4ND can model large-scale visual data in $1$D, $2$D, and $3$D as continuous multidimensional signals and demonstrates strong performance by simply swapping Conv2D and self-attention layers with S4ND layers in existing state-of-the-art models. On ImageNet-1k, S4ND exceeds the performance of a Vision Transformer baseline by $1.5\\%$ when training with a $1$D sequence of patches, and matches ConvNeXt when modeling images in $2$D. For videos, S4ND improves on an inflated $3$D ConvNeXt in activity classification on HMDB-51 by $4\\%$. S4ND implicitly learns global, continuous convolutional kernels that are resolution invariant by construction, providing an inductive bias that enables generalization across multiple resolutions. By developing a simple bandlimiting modification to S4 to overcome aliasing, S4ND achieves strong zero-shot (unseen at training time) resolution performance, outperforming a baseline Conv2D by $40\\%$ on CIFAR-10 when trained on $8 \\times 8$ and tested on $32 \\times 32$ images. When trained with progressive resizing, S4ND comes within $\\sim 1\\%$ of a high-resolution model while training $22\\%$ faster.","accessed":{"date-parts":[["2024",10,23]]},"author":[{"family":"Nguyen","given":"Eric"},{"family":"Goel","given":"Karan"},{"family":"Gu","given":"Albert"},{"family":"Downs","given":"Gordon W."},{"family":"Shah","given":"Preey"},{"family":"Dao","given":"Tri"},{"family":"Baccus","given":"Stephen A."},{"family":"R√©","given":"Christopher"}],"citation-key":"nguyenS4NDModelingImages2022","DOI":"10.48550/arXiv.2210.06583","issued":{"date-parts":[["2022",10,14]]},"number":"arXiv:2210.06583","publisher":"arXiv","source":"arXiv.org","title":"S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces","title-short":"S4ND","type":"article","URL":"http://arxiv.org/abs/2210.06583"},
  {"id":"ohkawaAssemblyHandsEgocentricActivity2023","abstract":"We present AssemblyHands, a large-scale benchmark dataset with accurate 3D hand pose annotations, to facilitate the study of egocentric activities with challenging hand-object interactions. The dataset includes synchronized egocentric and exocentric images sampled from the recent Assembly101 dataset, in which participants assemble and disassemble take-apart toys. To obtain high-quality 3D hand pose annotations for the egocentric images, we develop an efficient pipeline, where we use an initial set of manual annotations to train a model to automatically annotate a much larger dataset. Our annotation model uses multi-view feature fusion and an iterative refinement scheme, and achieves an average keypoint error of 4.20 mm, which is 85 % lower than the error of the original annotations in Assembly101. AssemblyHands provides 3.0M annotated images, including 490K egocentric images, making it the largest existing benchmark dataset for egocentric 3D hand pose estimation. Using this data, we develop a strong single-view baseline of 3D hand pose estimation from egocentric images. Furthermore, we design a novel action classification task to evaluate predicted 3D hand poses. Our study shows that having higher-quality hand poses directly improves the ability to recognize actions.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Ohkawa","given":"Takehiko"},{"family":"He","given":"Kun"},{"family":"Sener","given":"Fadime"},{"family":"Hodan","given":"Tomas"},{"family":"Tran","given":"Luan"},{"family":"Keskin","given":"Cem"}],"citation-key":"ohkawaAssemblyHandsEgocentricActivity2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12999-13008","source":"openaccess.thecvf.com","title":"AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation","title-short":"AssemblyHands","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"ohRecovering3DHand2023","abstract":"Hands, one of the most dynamic parts of our body, suffer from blur due to their active movements. However, previous 3D hand mesh recovery methods have mainly focused on sharp hand images rather than considering blur due to the absence of datasets providing blurry hand images. We first present a novel dataset BlurHand, which contains blurry hand images with 3D groundtruths. The BlurHand is constructed by synthesizing motion blur from sequential sharp hand images, imitating realistic and natural motion blurs. In addition to the new dataset, we propose BlurHandNet, a baseline network for accurate 3D hand mesh recovery from a blurry hand image. Our BlurHandNet unfolds a blurry input image to a 3D hand mesh sequence to utilize temporal information in the blurry input image, while previous works output a static single hand mesh. We demonstrate the usefulness of BlurHand for the 3D hand mesh recovery from blurry images in our experiments. The proposed BlurHandNet produces much more robust results on blurry images while generalizing well to in-the-wild images. The training codes and BlurHand dataset are available at https://github.com/JaehaKim97/BlurHand_RELEASE.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Oh","given":"Yeonguk"},{"family":"Park","given":"JoonKyu"},{"family":"Kim","given":"Jaeha"},{"family":"Moon","given":"Gyeongsik"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"ohRecovering3DHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"554-563","source":"openaccess.thecvf.com","title":"Recovering 3D Hand Mesh Sequence From a Single Blurry Image: A New Dataset and Temporal Unfolding","title-short":"Recovering 3D Hand Mesh Sequence From a Single Blurry Image","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Oh_Recovering_3D_Hand_Mesh_Sequence_From_a_Single_Blurry_Image_CVPR_2023_paper.html"},
  {"id":"oikonomidisEfficientModelbased3D2011","abstract":"We present a novel solution to the problem of recovering and tracking the 3D position, orientation and full articulation of a human hand from markerless visual observations obtained by a Kinect sensor. We treat this as an optimization problem, seeking for the hand model parameters that minimize the discrepancy between the appearance and 3D structure of hypothesized instances of a hand model and actual hand observations. This optimization problem is effectively solved using a variant of Particle Swarm Optimization (PSO). The proposed method does not require special markers and/or a complex image acquisition setup. Being model based, it provides continuous solutions to the problem of tracking hand articulations. Extensive experiments with a prototype GPU-based implementation of the proposed method demonstrate that accurate and robust 3D tracking of hand articulations can be achieved in near real-time (15Hz).","accessed":{"date-parts":[["2024",11,5]]},"author":[{"family":"Oikonomidis","given":"Iason"},{"family":"Kyriazis","given":"Nikolaos"},{"family":"Argyros","given":"Antonis"}],"citation-key":"oikonomidisEfficientModelbased3D2011","container-title":"Procedings of the British Machine Vision Conference 2011","DOI":"10.5244/C.25.101","event-place":"Dundee","event-title":"British Machine Vision Conference 2011","ISBN":"978-1-901725-43-8","issued":{"date-parts":[["2011"]]},"language":"en","page":"101.1-101.11","publisher":"British Machine Vision Association","publisher-place":"Dundee","source":"DOI.org (Crossref)","title":"Efficient model-based 3D tracking of hand articulations using Kinect","type":"paper-conference","URL":"http://www.bmva.org/bmvc/2011/proceedings/paper101/index.html"},
  {"id":"pageBreakingLowSample2024","abstract":"As spatial computers move towards glasses, where constraints on size, weight and power are critical, a shift in how we use sensors is needed. Ultraleap has developed the first hand tracking pipeline using event cameras on an AR headset. Event cameras allow to break the frame-imaging paradigm of \"low power equates to lo sample rate\" and enhance user experience while maintaining low power budget.","accessed":{"date-parts":[["2024",10,10]]},"author":[{"family":"Page","given":"Ryan"},{"family":"Baesso","given":"Paolo"},{"family":"Clark","given":"Rory"},{"family":"Baker","given":"Greg"}],"citation-key":"pageBreakingLowSample2024","collection-title":"SIGGRAPH '24","container-title":"ACM SIGGRAPH 2024 Emerging Technologies","DOI":"10.1145/3641517.3664389","event-place":"New York, NY, USA","ISBN":"9798400705243","issued":{"date-parts":[["2024",7,13]]},"page":"1‚Äì2","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Breaking the low sample rate equals low power paradigm using an event based vision approach for hand tracking in AR","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3641517.3664389"},
  {"id":"park3DHandSequence","abstract":"Although hands frequently exhibit motion blur due to their dynamic nature, existing approaches for 3D hand recovery often disregard the impact of motion blur in hand images. Blurry hand images contain hands from multiple time steps, lack precise hand location at a specific time step, and introduce temporal ambiguity, leading to multiple possible hand trajectories. To address this issue and in the absence of datasets with real blur, we introduce the EBH dataset, which provides 1) hand images with real motion blur and 2) event data for authentic representation of fast hand movements. In conjunction with our new dataset, we present EBHNet, a novel network capable of recovering 3D hands from diverse input combinations, including blurry hand images, events, or both. Here, the event stream enhances motion understanding in blurry hands, addressing temporal ambiguity. Recognizing that blurry hand images include not only single 3D hands at a time step but also multiple hands along their motion trajectories, we design EBHNet to generate 3D hand sequences in motion. Moreover, to enable our EBHNet to predict 3D hands at novel, unsupervised time steps using a single shared module, we employ a Transformer-based module, temporal splitter, into EBHNet. Our experiments show the superior performance of EBH and EBHNet, especially in handling blurry hand images, making them valuable in real-world applications.","author":[{"family":"Park","given":"Joonkyu"},{"family":"Moon","given":"Gyeongsik"},{"family":"Xu","given":"Weipeng"},{"family":"Kaseman","given":"Evan"},{"family":"Shiratori","given":"Takaaki"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"park3DHandSequence","language":"en","source":"Zotero","title":"3D Hand Sequence Recovery from Real Blurry Images and Event Stream","type":"article-journal"},
  {"id":"parkAttentionHandTextdrivenControllable","abstract":"Recently, there has been a significant amount of research conducted on 3D hand reconstruction to use various forms of humancomputer interaction. However, 3D hand reconstruction in the wild is challenging due to extreme lack of in-the-wild 3D hand datasets. Especially, when hands are in complex pose such as interacting hands, the problems like appearance similarity, self-handed occclusion and depth ambiguity make it more difficult. To overcome these issues, we propose AttentionHand, a novel method for text-driven controllable hand image generation. Since AttentionHand can generate various and numerous inthe-wild hand images well-aligned with 3D hand label, we can acquire a new 3D hand dataset, and can relieve the domain gap between indoor and outdoor scenes. Our method needs easy-to-use four modalities (i.e, an RGB image, a hand mesh image from 3D label, a bounding box, and a text prompt). These modalities are embedded into the latent space by the encoding phase. Then, through the text attention stage, hand-related tokens from the given text prompt are attended to highlight hand-related regions of the latent embedding. After the highlighted embedding is fed to the visual attention stage, hand-related regions in the embedding are attended by conditioning global and local hand mesh images with the diffusion-based pipeline. In the decoding phase, the final feature is decoded to new hand images, which are well-aligned with the given hand mesh image and text prompt. As a result, AttentionHand achieved state-of-the-art among text-to-hand image generation models, and the performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.","author":[{"family":"Park","given":"Junho"},{"family":"Kong","given":"Kyeongbo"},{"family":"Kang","given":"Suk-Ju"}],"citation-key":"parkAttentionHandTextdrivenControllable","language":"en","source":"Zotero","title":"AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild","type":"article-journal"},
  {"id":"parkHandOccNetOcclusionRobust3D2022","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Park","given":"JoonKyu"},{"family":"Oh","given":"Yeonguk"},{"family":"Moon","given":"Gyeongsik"},{"family":"Choi","given":"Hongsuk"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"parkHandOccNetOcclusionRobust3D2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"1496-1505","source":"openaccess.thecvf.com","title":"HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network","title-short":"HandOccNet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Park_HandOccNet_Occlusion-Robust_3D_Hand_Mesh_Estimation_Network_CVPR_2022_paper.html"},
  {"id":"pavlakosReconstructingHands3D2024","abstract":"We present an approach that can reconstruct hands in 3D from monocular input. Our approach for Hand Mesh Recovery HaMeR follows a fully transformer-based architecture and can analyze hands with significantly increased accuracy and robustness compared to previous work. The key to HaMeR's success lies in scaling up both the data used for training and the capacity of the deep network for hand reconstruction. For training data we combine multiple datasets that contain 2D or 3D hand annotations. For the deep model we use a large scale Vision Transformer architecture. Our final model consistently outperforms the previous baselines on popular 3D hand pose benchmarks. To further evaluate the effect of our design in non-controlled settings we annotate existing in-the-wild datasets with 2D hand keypoint annotations. On this newly collected dataset of annotations HInt we demonstrate significant improvements over existing baselines. We will make our code data and models publicly available upon publication. We make our code data and models available on the project website: https://geopavlakos.github.io/hamer/.","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Pavlakos","given":"Georgios"},{"family":"Shan","given":"Dandan"},{"family":"Radosavovic","given":"Ilija"},{"family":"Kanazawa","given":"Angjoo"},{"family":"Fouhey","given":"David"},{"family":"Malik","given":"Jitendra"}],"citation-key":"pavlakosReconstructingHands3D2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"9826-9836","source":"openaccess.thecvf.com","title":"Reconstructing Hands in 3D with Transformers","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Pavlakos_Reconstructing_Hands_in_3D_with_Transformers_CVPR_2024_paper.html"},
  {"id":"pooleDreamFusionText3DUsing2022","abstract":"Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Poole","given":"Ben"},{"family":"Jain","given":"Ajay"},{"family":"Barron","given":"Jonathan T."},{"family":"Mildenhall","given":"Ben"}],"citation-key":"pooleDreamFusionText3DUsing2022","DOI":"10.48550/arXiv.2209.14988","issued":{"date-parts":[["2022",9,29]]},"number":"arXiv:2209.14988","publisher":"arXiv","source":"arXiv.org","title":"DreamFusion: Text-to-3D using 2D Diffusion","title-short":"DreamFusion","type":"article","URL":"http://arxiv.org/abs/2209.14988"},
  {"id":"potamiasHandyHighFidelity2023","abstract":"Over the last few years, with the advent of virtual and augmented reality, an enormous amount of research has been focused on modeling, tracking and reconstructing human hands. Given their power to express human behavior, hands have been a very important, but challenging component of the human body. Currently, most of the state-of-the-art reconstruction and pose estimation methods rely on the low polygon MANO model. Apart from its low polygon count, MANO model was trained with only 31 adult subjects, which not only limits its expressive power but also imposes unnecessary shape reconstruction constraints on pose estimation methods. Moreover, hand appearance remains almost unexplored and neglected from the majority of hand reconstruction methods. In this work, we propose \"Handy\", a large-scale model of the human hand, modeling both shape and appearance composed of over 1200 subjects which we make publicly available for the benefit of the research community. In contrast to current models, our proposed hand model was trained on a dataset with large diversity in age, gender, and ethnicity, which tackles the limitations of MANO and accurately reconstructs out-of-distribution samples. In order to create a high quality texture model, we trained a powerful GAN, which preserves high frequency details and is able to generate high resolution hand textures. To showcase the capabilities of the proposed model, we built a synthetic dataset of textured hands and trained a hand pose estimation network to reconstruct both the shape and appearance from single images. As it is demonstrated in an extensive series of quantitative as well as qualitative experiments, our model proves to be robust against the state-of-the-art and realistically captures the 3D hand shape and pose along with a high frequency detailed texture even in adverse \"in-the-wild\" conditions.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Potamias","given":"Rolandos Alexandros"},{"family":"Ploumpis","given":"Stylianos"},{"family":"Moschoglou","given":"Stylianos"},{"family":"Triantafyllou","given":"Vasileios"},{"family":"Zafeiriou","given":"Stefanos"}],"citation-key":"potamiasHandyHighFidelity2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"4670-4680","source":"openaccess.thecvf.com","title":"Handy: Towards a High Fidelity 3D Hand Shape and Appearance Model","title-short":"Handy","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Potamias_Handy_Towards_a_High_Fidelity_3D_Hand_Shape_and_Appearance_CVPR_2023_paper.html"},
  {"id":"prakash3DHandPose","abstract":"3D hand pose estimation in everyday egocentric images is challenging for several reasons: poor visual signal (occlusion from the object of interaction, low resolution & motion blur), large perspective distortion (hands are close to the camera), and lack of 3D annotations outside of controlled settings. While existing methods often use hand crops as input to focus on fine-grained visual information to deal with poor visual signal, the challenges arising from perspective distortion and lack of 3D annotations in the wild have not been systematically studied. We focus on this gap and explore the impact of different practices, i.e. crops as input, incorporating camera information, auxiliary supervision, scaling up datasets. We provide several insights that are applicable to both convolutional and transformer models, leading to better performance. Based on our findings, we also present WildHands, a system for 3D hand pose estimation in everyday egocentric images. Zero-shot evaluation on 4 diverse datasets (H2O, AssemblyHands, Epic-Kitchens, Ego-Exo4D) demonstrate the effectiveness of our approach across 2D and 3D metrics, where we beat past methods by 7.4% ‚Äì 66%. In system level comparisons, WildHands achieves the best 3D hand pose on ARCTIC egocentric split, outperforms FrankMocap across all metrics and HaMeR on 3 out of 6 metrics while being 10\n smaller and trained on 5\n less data.","author":[{"family":"Prakash","given":"Aditya"},{"family":"Tu","given":"Ruisen"},{"family":"Chang","given":"Matthew"},{"family":"Gupta","given":"Saurabh"}],"citation-key":"prakash3DHandPose","language":"en","source":"Zotero","title":"3D Hand Pose Estimation in Everyday Egocentric Images","type":"article-journal"},
  {"id":"prakash3DReconstructionObjects2024","abstract":"Prior works for reconstructing hand-held objects from a single image train models on images paired with 3D shapes. Such data is challenging to gather in the real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of a) in-the-wild raw video data showing hand-object interactions and b) synthetic 3D shape collections. In this paper, we propose modules to leverage 3D supervision from these sources to scale up the learning of models for reconstructing hand-held objects. Specifically, we extract multiview 2D mask supervision from videos and 3D shape priors from shape collections. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. Our experiments in the challenging object generalization setting on in-the-wild MOW dataset show 11.6% relative improvement over models trained with 3D supervision on existing datasets.","accessed":{"date-parts":[["2024",10,11]]},"author":[{"family":"Prakash","given":"Aditya"},{"family":"Chang","given":"Matthew"},{"family":"Jin","given":"Matthew"},{"family":"Tu","given":"Ruisen"},{"family":"Gupta","given":"Saurabh"}],"citation-key":"prakash3DReconstructionObjects2024","issued":{"date-parts":[["2024",9,23]]},"number":"arXiv:2305.03036","publisher":"arXiv","source":"arXiv.org","title":"3D Reconstruction of Objects in Hands without Real World 3D Supervision","type":"article","URL":"http://arxiv.org/abs/2305.03036","version":"2"},
  {"id":"prakashMitigatingPerspectiveDistortioninduced2024","abstract":"Objects undergo varying amounts of perspective distortion as they move across a camera's field of view. Models for predicting 3D from a single image often work with crops around the object of interest and ignore the location of the object in the camera's field of view. We note that ignoring this location information further exaggerates the inherent ambiguity in making 3D inferences from 2D images and can prevent models from even fitting to the training data. To mitigate this ambiguity, we propose Intrinsics-Aware Positional Encoding (KPE), which incorporates information about the location of crops in the image and camera intrinsics. Experiments on three popular 3D-from-a-single-image benchmarks: depth prediction on NYU, 3D object detection on KITTI & nuScenes, and predicting 3D shapes of articulated objects on ARCTIC, show the benefits of KPE.","accessed":{"date-parts":[["2024",10,4]]},"author":[{"family":"Prakash","given":"Aditya"},{"family":"Gupta","given":"Arjun"},{"family":"Gupta","given":"Saurabh"}],"citation-key":"prakashMitigatingPerspectiveDistortioninduced2024","DOI":"10.48550/arXiv.2312.06594","issued":{"date-parts":[["2024",9,23]]},"number":"arXiv:2312.06594","publisher":"arXiv","source":"arXiv.org","title":"Mitigating Perspective Distortion-induced Shape Ambiguity in Image Crops","type":"article","URL":"http://arxiv.org/abs/2312.06594"},
  {"id":"qianHTMLParametricHand2020","abstract":"Abstract. 3D hand reconstruction from images is a widely-studied problem in computer vision and graphics, and has a particularly high relevance for virtual and augmented reality. Although several 3D hand reconstruction approaches leverage hand models as a strong prior to resolve ambiguities and achieve more robust results, most existing models account only for the hand shape and poses and do not model the texture. To Ô¨Åll this gap, in this work we present HTML, the Ô¨Årst parametric texture model of human hands. Our model spans several dimensions of hand appearance variability (e.g., related to gender, ethnicity, or age) and only requires a commodity camera for data acquisition. Experimentally, we demonstrate that our appearance model can be used to tackle a range of challenging problems such as 3D hand reconstruction from a single monocular image. Furthermore, our appearance model can be used to deÔ¨Åne a neural rendering layer that enables training with a selfsupervised photometric loss. We make our model publicly available?.","accessed":{"date-parts":[["2024",11,18]]},"author":[{"family":"Qian","given":"Neng"},{"family":"Wang","given":"Jiayi"},{"family":"Mueller","given":"Franziska"},{"family":"Bernard","given":"Florian"},{"family":"Golyanik","given":"Vladislav"},{"family":"Theobalt","given":"Christian"}],"citation-key":"qianHTMLParametricHand2020","container-title":"Computer Vision ‚Äì ECCV 2020","DOI":"10.1007/978-3-030-58621-8_4","editor":[{"family":"Vedaldi","given":"Andrea"},{"family":"Bischof","given":"Horst"},{"family":"Brox","given":"Thomas"},{"family":"Frahm","given":"Jan-Michael"}],"event-place":"Cham","ISBN":"978-3-030-58620-1 978-3-030-58621-8","issued":{"date-parts":[["2020"]]},"language":"en","page":"54-71","publisher":"Springer International Publishing","publisher-place":"Cham","source":"DOI.org (Crossref)","title":"HTML: A Parametric Hand Texture Model for 3D Hand Reconstruction and Personalization","title-short":"HTML","type":"chapter","URL":"https://link.springer.com/10.1007/978-3-030-58621-8_4","volume":"12356"},
  {"id":"qiDiverse3DHand2023","abstract":"Predicting natural and diverse 3D hand gestures from the upper body dynamics is a practical yet challenging task in virtual avatar creation. Previous works usually overlook the asymmetric motions between two hands and generate two hands in a holistic manner, leading to unnatural results. In this work, we introduce a novel bilateral hand disentanglement based two-stage 3D hand generation method to achieve natural and diverse 3D hand prediction from body dynamics. In the first stage, we intend to generate natural hand gestures by two hand-disentanglement branches. Considering the asymmetric gestures and motions of two hands, we introduce a Spatial-Residual Memory (SRM) module to model spatial interaction between the body and each hand by residual learning. To enhance the coordination of two hand motions wrt. body dynamics holistically, we then present a Temporal-Motion Memory (TMM) module. TMM can effectively model the temporal association between body dynamics and two hand motions. The second stage is built upon the insight that 3D hand predictions should be non-deterministic given the sequential body postures. Thus, we further diversify our 3D hand predictions based on the initial output from the stage one. Concretely, we propose a Prototypical-Memory Sampling Strategy (PSS) to generate the non-deterministic hand gestures by gradient-based Markov Chain Monte Carlo (MCMC) sampling. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on the B2H dataset and our newly collected TED Hands dataset. The dataset and code are available at: https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Qi","given":"Xingqun"},{"family":"Liu","given":"Chen"},{"family":"Sun","given":"Muyi"},{"family":"Li","given":"Lincheng"},{"family":"Fan","given":"Changjie"},{"family":"Yu","given":"Xin"}],"citation-key":"qiDiverse3DHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"4616-4626","source":"openaccess.thecvf.com","title":"Diverse 3D Hand Gesture Prediction From Body Dynamics by Bilateral Hand Disentanglement","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Qi_Diverse_3D_Hand_Gesture_Prediction_From_Body_Dynamics_by_Bilateral_CVPR_2023_paper.html"},
  {"id":"qiHOISDFConstraining3D2024","abstract":"Human hands are highly articulated and versatile at handling objects. Jointly estimating the 3D poses of a hand and the object it manipulates from a monocular camera is challenging due to frequent occlusions. Thus, existing methods often rely on intermediate 3D shape representations to increase performance. These representations are typically explicit, such as 3D point clouds or meshes, and thus provide information in the direct surroundings of the intermediate hand pose estimate. To address this, we introduce HOISDF, a Signed Distance Field (SDF) guided hand-object pose estimation network, which jointly exploits hand and object SDFs to provide a global, implicit representation over the complete reconstruction volume. Specifically, the role of the SDFs is threefold: equip the visual encoder with implicit shape information, help to encode hand-object interactions, and guide the hand and object pose regression via SDF-based sampling and by augmenting the feature representations. We show that HOISDF achieves state-of-the-art results on hand-object pose estimation benchmarks (DexYCB and HO3Dv2). Code is available at https://github.com/amathislab/HOISDF","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Qi","given":"Haozhe"},{"family":"Zhao","given":"Chen"},{"family":"Salzmann","given":"Mathieu"},{"family":"Mathis","given":"Alexander"}],"citation-key":"qiHOISDFConstraining3D2024","DOI":"10.48550/arXiv.2402.17062","issued":{"date-parts":[["2024",2,26]]},"number":"arXiv:2402.17062","publisher":"arXiv","source":"arXiv.org","title":"HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields","title-short":"HOISDF","type":"article","URL":"http://arxiv.org/abs/2402.17062"},
  {"id":"qiHOISDFConstraining3D2024a","abstract":"Human hands are highly articulated and versatile at handling objects. Jointly estimating the 3D poses of a hand and the object it manipulates from a monocular camera is challenging due to frequent occlusions. Thus, existing methods often rely on intermediate 3D shape representations to increase performance. These representations are typically explicit, such as 3D point clouds or meshes, and thus provide information in the direct surroundings of the intermediate hand pose estimate. To address this, we introduce HOISDF, a Signed Distance Field (SDF) guided hand-object pose estimation network, which jointly exploits hand and object SDFs to provide a global, implicit representation over the complete reconstruction volume. Specifically, the role of the SDFs is threefold: equip the visual encoder with implicit shape information, help to encode hand-object interactions, and guide the hand and object pose regression via SDF-based sampling and by augmenting the feature representations. We show that HOISDF achieves state-of-the-art results on hand-object pose estimation benchmarks (DexYCB and HO3Dv2). Code is available at https://github.com/amathislab/HOISDF","accessed":{"date-parts":[["2024",7,23]]},"author":[{"family":"Qi","given":"Haozhe"},{"family":"Zhao","given":"Chen"},{"family":"Salzmann","given":"Mathieu"},{"family":"Mathis","given":"Alexander"}],"citation-key":"qiHOISDFConstraining3D2024a","DOI":"10.48550/arXiv.2402.17062","issued":{"date-parts":[["2024",2,26]]},"number":"arXiv:2402.17062","publisher":"arXiv","source":"arXiv.org","title":"HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields","title-short":"HOISDF","type":"article","URL":"http://arxiv.org/abs/2402.17062"},
  {"id":"quNovelViewSynthesisPose2023","abstract":"Hand-object interaction understanding and the barely addressed novel view synthesis are highly desired in the immersive communication, whereas it is challenging due to the high deformation of hand and heavy occlusions between hand and object. In this paper, we propose a neural rendering and pose estimation system for hand-object interaction from sparse views, which can also enable 3D hand-object interaction editing. We share the inspiration from recent scene understanding work that shows a scene specific model built beforehand can significantly improve and unblock vision tasks especially when inputs are sparse, and extend it to the dynamic hand-object interaction scenario and propose to solve the problem in two stages. We first learn the shape and appearance prior knowledge of hands and objects separately with the neural representation at the offline stage. During the online stage, we design a rendering-based joint model fitting framework to understand the dynamic hand-object interaction with the pre-built hand and object models as well as interaction priors, which thereby overcomes penetration and separation issues between hand and object and also enables novel view synthesis. In order to get stable contact during the hand-object interaction process in a sequence, we propose a stable contact loss to make the contact region to be consistent. Experiments demonstrate that our method outperforms the state-of-the-art methods. Code and dataset are available in project webpage https://iscas3dv.github.io/HO-NeRF.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Qu","given":"Wentian"},{"family":"Cui","given":"Zhaopeng"},{"family":"Zhang","given":"Yinda"},{"family":"Meng","given":"Chenyu"},{"family":"Ma","given":"Cuixia"},{"family":"Deng","given":"Xiaoming"},{"family":"Wang","given":"Hongan"}],"citation-key":"quNovelViewSynthesisPose2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"15100-15111","source":"openaccess.thecvf.com","title":"Novel-View Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Qu_Novel-View_Synthesis_and_Pose_Estimation_for_Hand-Object_Interaction_from_Sparse_ICCV_2023_paper.html"},
  {"id":"regmiCrossViewImageSynthesis2018","accessed":{"date-parts":[["2024",10,7]]},"author":[{"family":"Regmi","given":"Krishna"},{"family":"Borji","given":"Ali"}],"citation-key":"regmiCrossViewImageSynthesis2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"3501-3510","source":"openaccess.thecvf.com","title":"Cross-View Image Synthesis Using Conditional GANs","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Regmi_Cross-View_Image_Synthesis_CVPR_2018_paper.html"},
  {"id":"romeroEmbodiedHandsModeling2017","abstract":"Humans move their hands and bodies together to communicate and solve tasks. Capturing and replicating such coordinated activity is critical for virtual characters that behave realistically. Surprisingly, most methods treat the 3D modeling and tracking of bodies and hands separately. Here we formulate a model of hands and bodies interacting together and fit it to full-body 4D sequences. When scanning or capturing the full body in 3D, hands are small and often partially occluded, making their shape and pose hard to recover. To cope with low-resolution, occlusion, and noise, we develop a new model called MANO (hand Model with Articulated and Non-rigid defOrmations). MANO is learned from around 1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand poses. The model is realistic, low-dimensional, captures non-rigid shape changes with pose, is compatible with standard graphics packages, and can fit any human hand. MANO provides a compact mapping from hand poses to pose blend shape corrections and a linear manifold of pose synergies. We attach MANO to a standard parameterized 3D body shape model (SMPL), resulting in a fully articulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting complex, natural, activities of subjects captured with a 4D scanner. The fitting is fully automatic and results in full body models that move naturally with detailed hand motions and a realism not seen before in full body performance capture. The models and data are freely available for research purposes in our website (http://mano.is.tue.mpg.de).","accessed":{"date-parts":[["2024",8,17]]},"author":[{"family":"Romero","given":"Javier"},{"family":"Tzionas","given":"Dimitrios"},{"family":"Black","given":"Michael J."}],"citation-key":"romeroEmbodiedHandsModeling2017","container-title":"ACM Transactions on Graphics","container-title-short":"ACM Trans. Graph.","DOI":"10.1145/3130800.3130883","ISSN":"0730-0301, 1557-7368","issue":"6","issued":{"date-parts":[["2017",12,31]]},"page":"1-17","source":"arXiv.org","title":"Embodied Hands: Modeling and Capturing Hands and Bodies Together","title-short":"Embodied Hands","type":"article-journal","URL":"http://arxiv.org/abs/2201.02610","volume":"36"},
  {"id":"sartoriHowObjectsAre2011","abstract":"Background Substantial literature has demonstrated that how the hand approaches an object depends on the manipulative action that will follow object contact. Little is known about how the placement of individual fingers on objects is affected by the end-goal of the action. Methodology/Principal Findings Hand movement kinematics were measured during reaching for and grasping movements towards two objects (stimuli): a bottle with an ordinary cylindrical shape and a bottle with a concave constriction. The effects of the stimuli's weight (half full or completely full of water) and the end-goals (pouring, moving) of the action were also assessed. Analysis of key kinematic landmarks measured during reaching movements indicate that object affordance facilitates the end-goal of the action regardless of accuracy constraints. Furthermore, the placement of individual digits at contact is modulated by the shape of the object and the end-goal of the action. Conclusions/Significance These findings offer a substantial contribution to the current debate about the role played by affordances and end-goals in determining the structure of reach-to-grasp movements.","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Sartori","given":"Luisa"},{"family":"Straulino","given":"Elisa"},{"family":"Castiello","given":"Umberto"}],"citation-key":"sartoriHowObjectsAre2011","container-title":"PLOS ONE","container-title-short":"PLOS ONE","DOI":"10.1371/journal.pone.0025203","ISSN":"1932-6203","issue":"9","issued":{"date-parts":[["2011",9,28]]},"language":"en","page":"e25203","publisher":"Public Library of Science","source":"PLoS Journals","title":"How Objects Are Grasped: The Interplay between Affordances and End-Goals","title-short":"How Objects Are Grasped","type":"article-journal","URL":"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0025203","volume":"6"},
  {"id":"shamilUtility3DHand","abstract":"3D hand pose is an underexplored modality for action recognition. Poses are compact yet informative and can greatly benefit applications with limited compute budgets. However, poses alone offer an incomplete understanding of actions, as they cannot fully capture objects and environments with which humans interact. We propose HandFormer, a novel multimodal transformer, to efficiently model hand-object interactions. HandFormer combines 3D hand poses at a high temporal resolution for fine-grained motion modeling with sparsely sampled RGB frames for encoding scene semantics. Observing the unique characteristics of hand poses, we temporally factorize hand modeling and represent each joint by its short-term trajectories. This factorized pose representation combined with sparse RGB samples is remarkably efficient and highly accurate. Unimodal HandFormer with only hand poses outperforms existing skeleton-based methods at 5\n fewer FLOPs. With RGB, we achieve new state-of-the-art performance on Assembly101 and H2O with significant improvements in egocentric action recognition.","author":[{"family":"Shamil","given":"Salman"},{"family":"Chatterjee","given":"Dibyadip"},{"family":"Sener","given":"Fadime"},{"family":"Ma","given":"Shugao"},{"family":"Yao","given":"Angela"}],"citation-key":"shamilUtility3DHand","language":"en","source":"Zotero","title":"On the Utility of 3D Hand Poses for Action Recognition","type":"article-journal"},
  {"id":"shanUnderstandingHumanHands2020","accessed":{"date-parts":[["2024",11,22]]},"author":[{"family":"Shan","given":"Dandan"},{"family":"Geng","given":"Jiaqi"},{"family":"Shu","given":"Michelle"},{"family":"Fouhey","given":"David F."}],"citation-key":"shanUnderstandingHumanHands2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"9869-9878","source":"openaccess.thecvf.com","title":"Understanding Human Hands in Contact at Internet Scale","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Shan_Understanding_Human_Hands_in_Contact_at_Internet_Scale_CVPR_2020_paper.html"},
  {"id":"sheLearningCrosshandPolicies","abstract":"Reaching-and-grasping is a fundamental skill for robotic manipulation, but existing methods usually train models on a specific gripper and cannot be reused on another gripper. In this paper, we propose a novel method that can learn a unified policy model that can be easily transferred to different dexterous grippers. Our method consists of two stages: a gripper-agnostic policy model that predicts the displacements of pre-defined key points on the gripper, and a gripper-specific adaptation model that translates these displacements into adjustments for controlling the grippers‚Äô joints. The gripper state and interactions with objects are captured at the finger level using robust geometric representations, integrated with a transformer-based network to address variations in gripper morphology and geometry. In the experiments, we evaluate our method on several dexterous grippers and diverse objects, and the result shows that our method significantly outperforms the baseline methods. Pioneering the transfer of grasp policies across dexterous grippers, our method effectively demonstrates its potential for learning generalizable and transferable manipulation skills for various robotic hands.","author":[{"family":"She","given":"Qijin"},{"family":"Zhang","given":"Shishun"},{"family":"Ye","given":"Yunfan"},{"family":"Hu","given":"Ruizhen"},{"family":"Xu","given":"Kai"}],"citation-key":"sheLearningCrosshandPolicies","language":"en","source":"Zotero","title":"Learning Cross-hand Policies of High-DOF Reaching and Grasping","type":"article-journal"},
  {"id":"spurrWeaklySupervised3D2020","abstract":"Estimating 3D hand pose from 2D images is a difficult, inverse problem due to the inherent scale and depth ambiguities. Current state-of-the-art methods train fully supervised deep neural networks with 3D ground-truth data. However, acquiring 3D annotations is expensive, typically requiring calibrated multi-view setups or labor intensive manual annotations. While annotations of 2D keypoints are much easier to obtain, how to efficiently leverage such weakly-supervised data to improve the task of 3D hand pose prediction remains an important open question. The key difficulty stems from the fact that direct application of additional 2D supervision mostly benefits the 2D proxy objective but does little to alleviate the depth and scale ambiguities. Embracing this challenge we propose a set of novel losses. We show by extensive experiments that our proposed constraints significantly reduce the depth ambiguity and allow the network to more effectively leverage additional 2D annotated images. For example, on the challenging freiHAND dataset using additional 2D annotation without our proposed biomechanical constraints reduces the depth error by only $15\\%$, whereas the error is reduced significantly by $50\\%$ when the proposed biomechanical constraints are used.","accessed":{"date-parts":[["2024",10,15]]},"author":[{"family":"Spurr","given":"Adrian"},{"family":"Iqbal","given":"Umar"},{"family":"Molchanov","given":"Pavlo"},{"family":"Hilliges","given":"Otmar"},{"family":"Kautz","given":"Jan"}],"citation-key":"spurrWeaklySupervised3D2020","DOI":"10.48550/arXiv.2003.09282","issued":{"date-parts":[["2020",8,4]]},"number":"arXiv:2003.09282","publisher":"arXiv","source":"arXiv.org","title":"Weakly Supervised 3D Hand Pose Estimation via Biomechanical Constraints","type":"article","URL":"http://arxiv.org/abs/2003.09282"},
  {"id":"spurrWeaklySupervised3D2020a","abstract":"Estimating 3D hand pose from 2D images is a difficult, inverse problem due to the inherent scale and depth ambiguities. Current state-of-the-art methods train fully supervised deep neural networks with 3D ground-truth data. However, acquiring 3D annotations is expensive, typically requiring calibrated multi-view setups or labor intensive manual annotations. While annotations of 2D keypoints are much easier to obtain, how to efficiently leverage such weakly-supervised data to improve the task of 3D hand pose prediction remains an important open question. The key difficulty stems from the fact that direct application of additional 2D supervision mostly benefits the 2D proxy objective but does little to alleviate the depth and scale ambiguities. Embracing this challenge we propose a set of novel losses. We show by extensive experiments that our proposed constraints significantly reduce the depth ambiguity and allow the network to more effectively leverage additional 2D annotated images. For example, on the challenging freiHAND dataset using additional 2D annotation without our proposed biomechanical constraints reduces the depth error by only $15\\%$, whereas the error is reduced significantly by $50\\%$ when the proposed biomechanical constraints are used.","accessed":{"date-parts":[["2024",11,14]]},"author":[{"family":"Spurr","given":"Adrian"},{"family":"Iqbal","given":"Umar"},{"family":"Molchanov","given":"Pavlo"},{"family":"Hilliges","given":"Otmar"},{"family":"Kautz","given":"Jan"}],"citation-key":"spurrWeaklySupervised3D2020a","issued":{"date-parts":[["2020",8,4]]},"number":"arXiv:2003.09282","publisher":"arXiv","source":"arXiv.org","title":"Weakly Supervised 3D Hand Pose Estimation via Biomechanical Constraints","type":"article","URL":"http://arxiv.org/abs/2003.09282"},
  {"id":"sudhakarControllingWorldSleight","abstract":"Humans naturally build mental models of object interactions and dynamics, allowing them to imagine how their surroundings will change if they take a certain action. While generative models today have shown impressive results on generating/editing images unconditionally or conditioned on text, current methods do not provide the ability to perform object manipulation conditioned on actions, an important tool for world modeling and action planning. Therefore, we propose to learn an action-conditional generative models by learning from unlabeled videos of human hands interacting with objects. The vast quantity of such data on the internet allows for efficient scaling which can enable high-performing action-conditional models. Given an image, and the shape/location of a desired hand interaction, CoSHAND, synthesizes an image of a future after the interaction has occurred. Experiments show that the resulting model can predict the effects of hand-object interactions well, with strong generalization particularly to translation, stretching, and squeezing interactions of unseen objects in unseen environments. Further, CoSHAND can be sampled many times to predict multiple possible effects, modeling the uncertainty of forces in the interaction/environment. Finally, method generalizes to different embodiments, including non-human hands, i.e. robot hands, suggesting that generative video models can be powerful models for robotics.","author":[{"family":"Sudhakar","given":"Sruthi"},{"family":"Liu","given":"Ruoshi"},{"family":"Hoorick","given":"Basile Van"},{"family":"Vondrick","given":"Carl"}],"citation-key":"sudhakarControllingWorldSleight","language":"en","source":"Zotero","title":"Controlling the World by Sleight of Hand","type":"article-journal"},
  {"id":"sumnerEmbeddedDeformationShape2007","abstract":"We present an algorithm that generates natural and intuitive deformations via direct manipulation for a wide range of shape representations and editing scenarios. Our method builds a space deformation represented by a collection of affine transformations organized in a graph structure. One transformation is associated with each graph node and applies a deformation to the nearby space. Positional constraints are specified on the points of an embedded object. As the user manipulates the constraints, a nonlinear minimization problem is solved to find optimal values for the affine transformations. Feature preservation is encoded directly in the objective function by measuring the deviation of each transformation from a true rotation. This algorithm addresses the problem of \"embedded deformation\" since it deforms space through direct manipulation of objects embedded within it, while preserving the embedded objects' features. We demonstrate our method by editing meshes, polygon soups, mesh animations, and animated particle systems.","accessed":{"date-parts":[["2024",11,7]]},"author":[{"family":"Sumner","given":"Robert W."},{"family":"Schmid","given":"Johannes"},{"family":"Pauly","given":"Mark"}],"citation-key":"sumnerEmbeddedDeformationShape2007","collection-title":"SIGGRAPH '07","container-title":"ACM SIGGRAPH 2007 papers","DOI":"10.1145/1275808.1276478","event-place":"New York, NY, USA","ISBN":"978-1-4503-7836-9","issued":{"date-parts":[["2007",7,29]]},"page":"80‚Äìes","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Embedded deformation for shape manipulation","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/1275808.1276478"},
  {"id":"swamySHOWMeBenchmarkingObjectagnostic2023","abstract":"Recent hand-object interaction datasets show limited real object variability and rely on fitting the MANO parametric model to obtain groundtruth hand shapes. To go beyond these limitations and spur further research, we introduce the SHOWMe dataset which consists of 96 videos, annotated with real and detailed hand-object 3D textured meshes. Following recent work, we consider a rigid hand-object scenario, in which the pose of the hand with respect to the object remains constant during the whole video sequence. This assumption allows us to register sub-millimetre-precise groundtruth 3D scans to the image sequences in SHOWMe. Although simpler, this hypothesis makes sense in terms of applications where the required accuracy and level of detail is important eg., object hand-over in human-robot collaboration, object scanning, or manipulation and contact point analysis. Importantly, the rigidity of the hand-object systems allows to tackle video-based 3D reconstruction of unknown hand-held objects using a 2-stage pipeline consisting of a rigid registration step followed by a multi-view reconstruction (MVR) part. We carefully evaluate a set of non-trivial baselines for these two stages and show that it is possible to achieve promising object-agnostic 3D hand-object reconstructions employing an SfM toolbox or a hand pose estimator to recover the rigid transforms and off-the-shelf MVR algorithms. However, these methods remain sensitive to the initial camera pose estimates which might be imprecise due to lack of textures on the objects or heavy occlusions of the hands, leaving room for improvements in the reconstruction. Code and dataset are available at https://europe.naverlabs.com/research/showme","accessed":{"date-parts":[["2024",10,31]]},"author":[{"family":"Swamy","given":"Anilkumar"},{"family":"Leroy","given":"Vincent"},{"family":"Weinzaepfel","given":"Philippe"},{"family":"Baradel","given":"Fabien"},{"family":"Galaaoui","given":"Salma"},{"family":"Bregier","given":"Romain"},{"family":"Armando","given":"Matthieu"},{"family":"Franco","given":"Jean-Sebastien"},{"family":"Rogez","given":"Gregory"}],"citation-key":"swamySHOWMeBenchmarkingObjectagnostic2023","issued":{"date-parts":[["2023",9,19]]},"number":"arXiv:2309.10748","publisher":"arXiv","source":"arXiv.org","title":"SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction","title-short":"SHOWMe","type":"article","URL":"http://arxiv.org/abs/2309.10748"},
  {"id":"taheriGRABDatasetWholeBody2020","abstract":"Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While ‚Äúgrasping‚Äù is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of ‚Äúwhole-body grasps‚Äù. Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes. The dataset and code are available for research purposes at https://grab.is.tue.mpg.de.","author":[{"family":"Taheri","given":"Omid"},{"family":"Ghorbani","given":"Nima"},{"family":"Black","given":"Michael J."},{"family":"Tzionas","given":"Dimitrios"}],"citation-key":"taheriGRABDatasetWholeBody2020","container-title":"Computer Vision ‚Äì ECCV 2020","DOI":"10.1007/978-3-030-58548-8_34","editor":[{"family":"Vedaldi","given":"Andrea"},{"family":"Bischof","given":"Horst"},{"family":"Brox","given":"Thomas"},{"family":"Frahm","given":"Jan-Michael"}],"event-place":"Cham","ISBN":"978-3-030-58548-8","issued":{"date-parts":[["2020"]]},"language":"en","page":"581-600","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"GRAB: A Dataset of Whole-Body Human Grasping of Objects","title-short":"GRAB","type":"paper-conference"},
  {"id":"tangPromptingFutureDriven","abstract":"Hand motion prediction from both first- and third-person perspectives is vital for enhancing user experience in AR/VR and ensuring safe remote robotic arm control. Previous works typically focus on predicting hand motion trajectories or human body motion, with direct hand motion prediction remaining largely unexplored - despite the additional challenges posed by compact skeleton size. To address this, we propose a prompt-based Future Driven Diffusion Model (PromptFDDM) for predicting hand motion with guidance and prompts. Specifically, we develop a Spatial-Temporal Extractor Network (STEN) to predict hand motion with guidance, a Ground Truth Extractor Network (GTEN), and a Reference Data Generator Network (RDGN), which extract ground truth and substitute future data with generated reference data, respectively, to guide STEN. Additionally, interactive prompts generated from observed motions further enhance model performance. Experimental results on the FPHA and HO3D datasets demonstrate that the proposed PromptFDDM achieves state-of-the-art performance in both first- and third-person perspectives.","author":[{"family":"Tang","given":"Bowen"},{"family":"Zhang","given":"Kaihao"},{"family":"Luo","given":"Wenhan"},{"family":"Liu","given":"Wei"},{"family":"Li","given":"Hongdong"}],"citation-key":"tangPromptingFutureDriven","language":"en","source":"Zotero","title":"Prompting Future Driven Diffusion Model for Hand Motion Prediction","type":"article-journal"},
  {"id":"tevetHumanMotionDiffusion2022","abstract":"Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion. https://guytevet.github.io/mdm-page/ .","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Tevet","given":"Guy"},{"family":"Raab","given":"Sigal"},{"family":"Gordon","given":"Brian"},{"family":"Shafir","given":"Yonatan"},{"family":"Cohen-Or","given":"Daniel"},{"family":"Bermano","given":"Amit H."}],"citation-key":"tevetHumanMotionDiffusion2022","DOI":"10.48550/arXiv.2209.14916","issued":{"date-parts":[["2022",10,3]]},"number":"arXiv:2209.14916","publisher":"arXiv","source":"arXiv.org","title":"Human Motion Diffusion Model","type":"article","URL":"http://arxiv.org/abs/2209.14916"},
  {"id":"tseSpectralGraphormerSpectral2023","abstract":"We propose a novel transformer-based framework that reconstructs two high fidelity hands from multi-view RGB images. Unlike existing hand pose estimation methods, where one typically trains a deep network to regress hand model parameters from single RGB image, we consider a more challenging problem setting where we directly regress the absolute root poses of two-hands with extended forearm at high resolution from egocentric view. As existing datasets are either infeasible for egocentric viewpoints or lack background variations, we create a large-scale synthetic dataset with diverse scenarios and collect a real dataset from multi-calibrated camera setup to verify our proposed multi-view image feature fusion strategy. To make the reconstruction physically plausible, we propose two strategies: (i) a coarse-to-fine spectral graph convolution decoder to smoothen the meshes during upsampling and (ii) an optimisation-based refinement stage at inference to prevent self-penetrations. Through extensive quantitative and qualitative evaluations, we show that our framework is able to produce realistic two-hand reconstructions and demonstrate the generalisation of synthetic-trained models to real data, as well as real-time AR/VR applications.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Tse","given":"Tze Ho Elden"},{"family":"Mueller","given":"Franziska"},{"family":"Shen","given":"Zhengyang"},{"family":"Tang","given":"Danhang"},{"family":"Beeler","given":"Thabo"},{"family":"Dou","given":"Mingsong"},{"family":"Zhang","given":"Yinda"},{"family":"Petrovic","given":"Sasa"},{"family":"Chang","given":"Hyung Jin"},{"family":"Taylor","given":"Jonathan"},{"family":"Doosti","given":"Bardia"}],"citation-key":"tseSpectralGraphormerSpectral2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"14666-14677","source":"openaccess.thecvf.com","title":"Spectral Graphormer: Spectral Graph-Based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images","title-short":"Spectral Graphormer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Tse_Spectral_Graphormer_Spectral_Graph-Based_Transformer_for_Egocentric_Two-Hand_Reconstruction_using_ICCV_2023_paper.html"},
  {"id":"valassakisHandDGPCameraSpaceHand","abstract":"Predicting camera-space hand meshes from single RGB images is crucial for enabling realistic hand interactions in 3D virtual and augmented worlds. Previous work typically divided the task into two stages: given a cropped image of the hand, predict meshes in relative coordinates, followed by lifting these predictions into camera space in a separate and independent stage, often resulting in the loss of valuable contextual and scale information. To prevent the loss of these cues, we propose unifying these two stages into an end-to-end solution that addresses the 2D-3D correspondence problem. This solution enables backpropagation from camera space outputs to the rest of the network through a new differentiable global positioning module. We also introduce an image rectification step that harmonizes both the training dataset and the input image as if they were acquired with the same camera, helping to alleviate the inherent scale-depth ambiguity of the problem. We validate the effectiveness of our framework in evaluations against several baselines and state-of-the-art approaches across three public benchmarks.","author":[{"family":"Valassakis","given":"Eugene"},{"family":"Garcia-Hernando","given":"Guillermo"}],"citation-key":"valassakisHandDGPCameraSpaceHand","language":"en","source":"Zotero","title":"HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning","type":"article-journal"},
  {"id":"wangDeepSimHOStablePose2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Wang","given":"Rong"},{"family":"Mao","given":"Wei"},{"family":"Li","given":"Hongdong"}],"citation-key":"wangDeepSimHOStablePose2023","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2023",12,15]]},"language":"en","page":"79685-79697","source":"proceedings.neurips.cc","title":"DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation","title-short":"DeepSimHO","type":"article-journal","URL":"https://proceedings.neurips.cc/paper_files/paper/2023/hash/fbdaea4878318e214c0577dae4b8bc43-Abstract-Conference.html","volume":"36"},
  {"id":"wangMeMaHandExploitingMeshMano2023","abstract":"Existing methods proposed for hand reconstruction tasks usually parameterize a generic 3D hand model or predict hand mesh positions directly. The parametric representations consisting of hand shapes and rotational poses are more stable, while the non-parametric methods can predict more accurate mesh positions. In this paper, we propose to reconstruct meshes and estimate MANO parameters of two hands from a single RGB image simultaneously to utilize the merits of two kinds of hand representations. To fulfill this target, we propose novel Mesh-Mano interaction blocks (MMIBs), which take mesh vertices positions and MANO parameters as two kinds of query tokens. MMIB consists of one graph residual block to aggregate local information and two transformer encoders to model long-range dependencies. The transformer encoders are equipped with different asymmetric attention masks to model the intra-hand and inter-hand attention, respectively. Moreover, we introduce the mesh alignment refinement module to further enhance the mesh-image alignment. Extensive experiments on the InterHand2.6M benchmark demonstrate promising results over the state-of-the-art hand reconstruction methods.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Wang","given":"Congyi"},{"family":"Zhu","given":"Feida"},{"family":"Wen","given":"Shilei"}],"citation-key":"wangMeMaHandExploitingMeshMano2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"564-573","source":"openaccess.thecvf.com","title":"MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction","title-short":"MeMaHand","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Wang_MeMaHand_Exploiting_Mesh-Mano_Interaction_for_Single_Image_Two-Hand_Reconstruction_CVPR_2023_paper.html"},
  {"id":"wangSurvey3DHuman2024","abstract":"3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.","accessed":{"date-parts":[["2024",11,4]]},"author":[{"family":"Wang","given":"Ruihe"},{"family":"Cao","given":"Yukang"},{"family":"Han","given":"Kai"},{"family":"Wong","given":"Kwan-Yee K."}],"citation-key":"wangSurvey3DHuman2024","issued":{"date-parts":[["2024",6,6]]},"number":"arXiv:2406.04253","publisher":"arXiv","source":"arXiv.org","title":"A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation","type":"article","URL":"http://arxiv.org/abs/2406.04253"},
  {"id":"wangSurveyDeepLearningbased2022","abstract":"Hand pose estimation is one of the representative tasks in computer vision. Solving the hand pose estimation problem is essential for various fields such as virtual reality, augmented reality, mixed reality, and human-computer interaction. Due to the significant development of deep learning techniques, the hand pose estimation task has reached significant performance on many hand pose estimation datasets. However, the hand pose estimation task still faces many challenges due to the lack of large-scale labeled data, severe occlusion, low hand resolution, and background clutter. To better understand the hand pose estimation task, this paper presents a comprehensive survey of outstanding papers over the last five years. The paper first introduces 19 common hand pose estimation datasets, then extensively discusses some of the mainstream approaches in hand pose estimation, including fully supervised, semi-supervised, weakly supervised, and self-supervised learning methods. Finally, we extensively discuss the future evolution trends of hand pose estimation.","accessed":{"date-parts":[["2024",8,16]]},"author":[{"family":"Wang","given":"Shuaibing"},{"family":"Wang","given":"Shunli"},{"family":"Kuang","given":"HaoPeng"},{"family":"Li","given":"Fang"},{"family":"Qian","given":"Ziyun"},{"family":"Li","given":"Mingcheng"},{"family":"Zhang","given":"Lihua"}],"citation-key":"wangSurveyDeepLearningbased2022","container-title":"2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS)","DOI":"10.1109/CCIS57298.2022.10016310","event-title":"2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS)","issued":{"date-parts":[["2022",11]]},"page":"331-340","source":"IEEE Xplore","title":"A Survey of Deep Learning-based Hand Pose Estimation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10016310/?arnumber=10016310"},
  {"id":"wenHierarchicalTemporalTransformer2023","abstract":"Understanding dynamic hand motions and actions from egocentric RGB videos is a fundamental yet challenging task due to self-occlusion and ambiguity. To address occlusion and ambiguity, we develop a transformer-based framework to exploit temporal information for robust estimation. Noticing the different temporal granularity of and the semantic correlation between hand pose estimation and action recognition, we build a network hierarchy with two cascaded transformer encoders, where the first one exploits the short-term temporal cue for hand pose estimation, and the latter aggregates per-frame pose and object information over a longer time span to recognize the action. Our approach achieves competitive results on two first-person hand action benchmarks, namely FPHA and H2O. Extensive ablation studies verify our design choices.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Wen","given":"Yilin"},{"family":"Pan","given":"Hao"},{"family":"Yang","given":"Lei"},{"family":"Pan","given":"Jia"},{"family":"Komura","given":"Taku"},{"family":"Wang","given":"Wenping"}],"citation-key":"wenHierarchicalTemporalTransformer2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"21243-21253","source":"openaccess.thecvf.com","title":"Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition From Egocentric RGB Videos","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Wen_Hierarchical_Temporal_Transformer_for_3D_Hand_Pose_Estimation_and_Action_CVPR_2023_paper.html"},
  {"id":"wiederholdHOHMarkerlessMultimodal2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Wiederhold","given":"Noah"},{"family":"Megyeri","given":"Ava"},{"family":"Paris","given":"DiMaggio"},{"family":"Banerjee","given":"Sean"},{"family":"Banerjee","given":"Natasha"}],"citation-key":"wiederholdHOHMarkerlessMultimodal2023","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2023",12,15]]},"language":"en","page":"68736-68748","source":"proceedings.neurips.cc","title":"HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count","title-short":"HOH","type":"article-journal","URL":"https://proceedings.neurips.cc/paper_files/paper/2023/hash/d8c6a37c4c94e9a63e53d296f1f668ae-Abstract-Datasets_and_Benchmarks.html","volume":"36"},
  {"id":"wilesSynSinEndEndView2020","accessed":{"date-parts":[["2024",10,7]]},"author":[{"family":"Wiles","given":"Olivia"},{"family":"Gkioxari","given":"Georgia"},{"family":"Szeliski","given":"Richard"},{"family":"Johnson","given":"Justin"}],"citation-key":"wilesSynSinEndEndView2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"7467-7477","source":"openaccess.thecvf.com","title":"SynSin: End-to-End View Synthesis From a Single Image","title-short":"SynSin","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Wiles_SynSin_End-to-End_View_Synthesis_From_a_Single_Image_CVPR_2020_paper.html"},
  {"id":"xieHMDOMarkerlessMultiview2023","abstract":"We construct the first markerless deformable interaction dataset recording interactive motions of the hands and deformable objects, called HMDO (Hand Manipulation with Deformable Objects). With our built multi-view capture system, it captures the deformable interactions with multiple perspectives, various object shapes, and diverse interactive forms. Our motivation is the current lack of hand and deformable object interaction datasets, as 3D hand and deformable object reconstruction is challenging. Mainly due to mutual occlusion, the interaction area is difficult to observe, the visual features between the hand and the object are entangled, and the reconstruction of the interaction area deformation is difficult. To tackle this challenge, we propose a method to annotate our captured data. Our key idea is to collaborate with estimated hand features to guide the object global pose estimation, and then optimize the deformation process of the object by analyzing the relationship between the hand and the object. Through comprehensive evaluation, the proposed method can reconstruct interactive motions of hands and deformable objects with high quality. HMDO currently consists of 21600 frames over 12 sequences. In the future, this dataset could boost the research of learning-based reconstruction of deformable interaction scenes.","accessed":{"date-parts":[["2024",11,7]]},"author":[{"family":"Xie","given":"Wei"},{"family":"Yu","given":"Zhipeng"},{"family":"Zhao","given":"Zimeng"},{"family":"Zuo","given":"Binghui"},{"family":"Wang","given":"Yangang"}],"citation-key":"xieHMDOMarkerlessMultiview2023","container-title":"Graphical Models","container-title-short":"Graphical Models","DOI":"10.1016/j.gmod.2023.101178","ISSN":"1524-0703","issued":{"date-parts":[["2023",5,1]]},"page":"101178","source":"ScienceDirect","title":"HMDO : Markerless multi-view hand manipulation capture with deformable objects","title-short":"HMDO","type":"article-journal","URL":"https://www.sciencedirect.com/science/article/pii/S1524070323000085","volume":"127"},
  {"id":"xieMSMANOEnablingHand2024","abstract":"This work proposes a novel learning framework for visual hand dynamics analysis that takes into account the physiological aspects of hand motion. The existing models, which are simplified joint-actuated systems, often produce unnatural motions. To address this, we integrate a musculoskeletal system with a learnable parametric hand model, MANO, to create a new model, MS-MANO. This model emulates the dynamics of muscles and tendons to drive the skeletal system, imposing physiologically realistic constraints on the resulting torque trajectories. We further propose a simulation-in-the-loop pose refinement framework, BioPR, that refines the initial estimated pose through a multi-layer perceptron (MLP) network. Our evaluation of the accuracy of MS-MANO and the efficacy of the BioPR is conducted in two separate parts. The accuracy of MS-MANO is compared with MyoSuite, while the efficacy of BioPR is benchmarked against two large-scale public datasets and two recent state-of-the-art methods. The results demonstrate that our approach consistently improves the baseline methods both quantitatively and qualitatively.","accessed":{"date-parts":[["2024",10,10]]},"author":[{"family":"Xie","given":"Pengfei"},{"family":"Xu","given":"Wenqiang"},{"family":"Tang","given":"Tutian"},{"family":"Yu","given":"Zhenjun"},{"family":"Lu","given":"Cewu"}],"citation-key":"xieMSMANOEnablingHand2024","DOI":"10.48550/arXiv.2404.10227","issued":{"date-parts":[["2024",4,16]]},"number":"arXiv:2404.10227","publisher":"arXiv","source":"arXiv.org","title":"MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints","title-short":"MS-MANO","type":"article","URL":"http://arxiv.org/abs/2404.10227"},
  {"id":"xieNonrigidObjectContact2023","abstract":"Acquiring contact patterns between hands and nonrigid objects is a common concern in the vision and robotics community. However, existing learning-based methods focus more on contact with rigid ones from monocular images. When adopting them for nonrigid contact, a major problem is that the existing contact representation is restricted by the geometry of the object. Consequently, contact neighborhoods are stored in an unordered manner and contact features are difficult to align with image cues. At the core of our approach lies a novel hand-object contact representation called RUPs (Region Unwrapping Profiles), which unwrap the roughly estimated hand-object surfaces as multiple high-resolution 2D regional profiles. The region grouping strategy is consistent with the hand kinematic bone division because they are the primitive initiators for a composite contact pattern. Based on this representation, our Regional Unwrapping Transformer (RUFormer) learns the correlation priors across regions from monocular inputs and predicts corresponding contact and deformed transformations. Our experiments demonstrate that the proposed framework can robustly estimate the deformed degrees and deformed transformations, which make it suitable for both nonrigid and rigid contact.","accessed":{"date-parts":[["2024",9,27]]},"author":[{"family":"Xie","given":"Wei"},{"family":"Zhao","given":"Zimeng"},{"family":"Li","given":"Shiying"},{"family":"Zuo","given":"Binghui"},{"family":"Wang","given":"Yangang"}],"citation-key":"xieNonrigidObjectContact2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"9342-9351","source":"openaccess.thecvf.com","title":"Nonrigid Object Contact Estimation With Regional Unwrapping Transformer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Xie_Nonrigid_Object_Contact_Estimation_With_Regional_Unwrapping_Transformer_ICCV_2023_paper.html"},
  {"id":"xuEgoPCANewFramework2023","abstract":"With the surge in attention to Egocentric Hand-Object Interaction (Ego-HOI), large-scale datasets such as Ego4D and EPIC-KITCHENS have been proposed. However, most current research is built on resources derived from third-person video action recognition. This inherent domain gap between first- and third-person action videos, which have not been adequately addressed before, makes current Ego-HOI suboptimal. This paper rethinks and proposes a new framework as an infrastructure to advance Ego-HOI recognition by Probing, Curation and Adaption (EgoPCA). We contribute comprehensive pre-train sets, balanced test sets and a new baseline, which are complete with a training-finetuning strategy. With our new framework, we not only achieve state-of-the-art performance on Ego-HOI benchmarks but also build several new and effective mechanisms and settings to advance further research. We believe our data and the findings will pave a new way for Ego-HOI understanding. Code and data are available at https://mvig-rhos.com/ego_pca.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Xu","given":"Yue"},{"family":"Li","given":"Yong-Lu"},{"family":"Huang","given":"Zhemin"},{"family":"Liu","given":"Michael Xu"},{"family":"Lu","given":"Cewu"},{"family":"Tai","given":"Yu-Wing"},{"family":"Tang","given":"Chi-Keung"}],"citation-key":"xuEgoPCANewFramework2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"5273-5284","source":"openaccess.thecvf.com","title":"EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding","title-short":"EgoPCA","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Xu_EgoPCA_A_New_Framework_for_Egocentric_Hand-Object_Interaction_Understanding_ICCV_2023_paper.html"},
  {"id":"xuH2ONetHandOcclusionandOrientationAwareNetwork2023","abstract":"Real-time 3D hand mesh reconstruction is challenging, especially when the hand is holding some object. Beyond the previous methods, we design H2ONet to fully exploit non-occluded information from multiple frames to boost the reconstruction quality. First, we decouple hand mesh reconstruction into two branches, one to exploit finger-level non-occluded information and the other to exploit global hand orientation, with lightweight structures to promote real-time inference. Second, we propose finger-level occlusion-aware feature fusion, leveraging predicted finger-level occlusion information as guidance to fuse finger-level information across time frames. Further, we design hand-level occlusion-aware feature fusion to fetch non-occluded information from nearby time frames. We conduct experiments on the Dex-YCB and HO3D-v2 datasets with challenging hand-object occlusion cases, manifesting that H2ONet is able to run in real-time and achieves state-of-the-art performance on both the hand mesh and pose precision. The code will be released on GitHub.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Xu","given":"Hao"},{"family":"Wang","given":"Tianyu"},{"family":"Tang","given":"Xiao"},{"family":"Fu","given":"Chi-Wing"}],"citation-key":"xuH2ONetHandOcclusionandOrientationAwareNetwork2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"17048-17058","source":"openaccess.thecvf.com","title":"H2ONet: Hand-Occlusion-and-Orientation-Aware Network for Real-Time 3D Hand Mesh Reconstruction","title-short":"H2ONet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Xu_H2ONet_Hand-Occlusion-and-Orientation-Aware_Network_for_Real-Time_3D_Hand_Mesh_Reconstruction_CVPR_2023_paper.html"},
  {"id":"xuHandBoosterBoosting3D2024","abstract":"Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets. While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage. In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples. First, we construct versatile content-aware conditions to guide a diffusion model to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free. Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set. Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB benchmarks. Our code will be released on https://github.com/hxwork/HandBooster_Pytorch.","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Xu","given":"Hao"},{"family":"Li","given":"Haipeng"},{"family":"Wang","given":"Yinqiao"},{"family":"Liu","given":"Shuaicheng"},{"family":"Fu","given":"Chi-Wing"}],"citation-key":"xuHandBoosterBoosting3D2024","DOI":"10.48550/arXiv.2403.18575","issued":{"date-parts":[["2024",3,27]]},"number":"arXiv:2403.18575","publisher":"arXiv","source":"arXiv.org","title":"HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions","title-short":"HandBooster","type":"article","URL":"http://arxiv.org/abs/2403.18575"},
  {"id":"xuVisualTactileSensingInHand2023","abstract":"Tactile sensing is one of the modalities human rely on heavily to perceive the world. Working with vision, this modality refines local geometry structure, measures deformation at contact area, and indicates hand-object contact state. With the availability of open-source tactile sensors such as DIGIT, research on visual-tactile learning is becoming more accessible and reproducible. Leveraging this tactile sensor, we propose a novel visual-tactile in-hand object reconstruction framework VTacO, and extend it to VTacOH for hand-object reconstruction. Since our method can support both rigid and deformable object reconstruction, and no existing benchmark are proper for the goal. We propose a simulation environment, VT-Sim, which supports to generate hand-object interaction for both rigid and deformable objects. With VT-Sim, we generate a large-scale training dataset, and evaluate our method on it. Extensive experiments demonstrate that our proposed method can outperform the previous baseline methods qualitatively and quantitatively. Finally, we directly apply our model trained in simulation to various real-world test cases, which display qualitative results. Codes, models, simulation environment, datasets will be publicly available.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Xu","given":"Wenqiang"},{"family":"Yu","given":"Zhenjun"},{"family":"Xue","given":"Han"},{"family":"Ye","given":"Ruolin"},{"family":"Yao","given":"Siqiong"},{"family":"Lu","given":"Cewu"}],"citation-key":"xuVisualTactileSensingInHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8803-8812","source":"openaccess.thecvf.com","title":"Visual-Tactile Sensing for In-Hand Object Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.html"},
  {"id":"yangCPFLearningContact2021","accessed":{"date-parts":[["2024",9,27]]},"author":[{"family":"Yang","given":"Lixin"},{"family":"Zhan","given":"Xinyu"},{"family":"Li","given":"Kailin"},{"family":"Xu","given":"Wenqiang"},{"family":"Li","given":"Jiefeng"},{"family":"Lu","given":"Cewu"}],"citation-key":"yangCPFLearningContact2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11097-11106","source":"openaccess.thecvf.com","title":"CPF: Learning a Contact Potential Field To Model the Hand-Object Interaction","title-short":"CPF","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Yang_CPF_Learning_a_Contact_Potential_Field_To_Model_the_Hand-Object_ICCV_2021_paper.html"},
  {"id":"yangMLPHandRealTime","abstract":"Multi-view hand mesh reconstruction is a critical task for applications in virtual reality and human-computer interaction, but it remains a formidable challenge. Although existing multi-view hand reconstruction methods achieve remarkable accuracy, they typically come with an intensive computational burden that hinders real-time inference. To this end, we propose MLPHand, a novel method designed for real-time multi-view single hand reconstruction. MLP Hand consists of two primary modules: (1) a lightweight MLP-based Skeleton2Mesh model that efficiently recovers hand meshes from hand skeletons, and (2) a multi-view geometry feature fusion prediction module that enhances the Skeleton2Mesh model with detailed geometric information from multiple views. Experiments on three widely used datasets demonstrate that MLPHand can reduce computational complexity by 90% while achieving comparable reconstruction accuracy to existing state-of-the-art baselines.","author":[{"family":"Yang","given":"Jian"},{"family":"Li","given":"Jiakun"},{"family":"Li","given":"Guoming"},{"family":"Wu","given":"Huai-Yu"},{"family":"Shen","given":"Zhen"},{"family":"Fan","given":"Zhaoxin"}],"citation-key":"yangMLPHandRealTime","language":"en","source":"Zotero","title":"MLPHand: Real Time Multi-View 3D Hand Reconstruction via MLP Modeling","type":"article-journal"},
  {"id":"yangPOEMReconstructingHand2023","abstract":"Enable neural networks to capture 3D geometrical-aware features is essential in multi-view based vision tasks. Previous methods usually encode the 3D information of multi-view stereo into the 2D features. In contrast, we present a novel method, named POEM, that directly operates on the 3D POints Embedded in the Multi-view stereo for reconstructing hand mesh in it. Point is a natural form of 3D information and an ideal medium for fusing features across views, as it has different projections on different views. Our method is thus in light of a simple yet effective idea, that a complex 3D hand mesh can be represented by a set of 3D points that 1) are embedded in the multi-view stereo, 2) carry features from the multi-view images, and 3) encircle the hand. To leverage the power of points, we design two operations: point-based feature fusion and cross-set point attention mechanism. Evaluation on three challenging multi-view datasets shows that POEM outperforms the state-of-the-art in hand mesh reconstruction. Code and models are available for research at github.com/lixiny/POEM","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Yang","given":"Lixin"},{"family":"Xu","given":"Jian"},{"family":"Zhong","given":"Licheng"},{"family":"Zhan","given":"Xinyu"},{"family":"Wang","given":"Zhicheng"},{"family":"Wu","given":"Kejian"},{"family":"Lu","given":"Cewu"}],"citation-key":"yangPOEMReconstructingHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"21108-21117","source":"openaccess.thecvf.com","title":"POEM: Reconstructing Hand in a Point Embedded Multi-View Stereo","title-short":"POEM","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Yang_POEM_Reconstructing_Hand_in_a_Point_Embedded_Multi-View_Stereo_CVPR_2023_paper.html"},
  {"id":"yeAffordanceDiffusionSynthesizing2023","abstract":"Recent successes in image synthesis are powered by large-scale diffusion models. However, most methods are currently limited to either text- or image-conditioned generation for synthesizing an entire image, texture transfer or inserting objects into a user-specified region. In contrast, in this work we focus on synthesizing complex interactions (i.e., an articulated hand) with a given object. Given an RGB image of an object, we aim to hallucinate plausible images of a human hand interacting with it. We propose a two step generative approach that leverages a LayoutNet that samples an articulation-agnostic hand-object-interaction layout, and a ContentNet that synthesizes images of a hand grasping the object given the predicted layout. Both are built on top of a large-scale pretrained diffusion model to make use of its latent representation. Compared to baselines, the proposed method is shown to generalize better to novel objects and perform surprisingly well on out-of-distribution in-the-wild scenes. The resulting system allows us to predict descriptive affordance information, such as hand articulation and approaching orientation.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Ye","given":"Yufei"},{"family":"Li","given":"Xueting"},{"family":"Gupta","given":"Abhinav"},{"family":"De Mello","given":"Shalini"},{"family":"Birchfield","given":"Stan"},{"family":"Song","given":"Jiaming"},{"family":"Tulsiani","given":"Shubham"},{"family":"Liu","given":"Sifei"}],"citation-key":"yeAffordanceDiffusionSynthesizing2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"22479-22489","source":"openaccess.thecvf.com","title":"Affordance Diffusion: Synthesizing Hand-Object Interactions","title-short":"Affordance Diffusion","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Ye_Affordance_Diffusion_Synthesizing_Hand-Object_Interactions_CVPR_2023_paper.html"},
  {"id":"yeDiffusionGuidedReconstructionEveryday2023","abstract":"We tackle the task of reconstructing hand-object interactions from short video clips. Given an input video, our approach casts 3D inference as a per-video optimization and recovers a neural 3D representation of the object shape, as well as the time-varying motion and hand articulation. While the input video naturally provides some multi-view cues to guide 3D inference, these are insufficient on their own due to occlusions and limited viewpoint variations. To obtain accurate 3D, we augment the multi-view signals with generic data-driven priors to guide reconstruction. Specifically, we learn a diffusion network to model the conditional distribution of (geometric) renderings of objects conditioned on hand configuration and category label, and leverage it as a prior to guide the novel-view renderings of the reconstructed scene. We empirically evaluate our approach on egocentric videos across 6 object categories, and observe significant improvements over prior single-view and multi-view methods. Finally, we demonstrate our system's ability to reconstruct arbitrary clips from YouTube, showing both 1st and 3rd person interactions.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Ye","given":"Yufei"},{"family":"Hebbar","given":"Poorvi"},{"family":"Gupta","given":"Abhinav"},{"family":"Tulsiani","given":"Shubham"}],"citation-key":"yeDiffusionGuidedReconstructionEveryday2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"19717-19728","source":"openaccess.thecvf.com","title":"Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Diffusion-Guided_Reconstruction_of_Everyday_Hand-Object_Interaction_Clips_ICCV_2023_paper.html"},
  {"id":"yeGHOPGenerativeHandObject2024","abstract":"We propose G-HOP a denoising diffusion based generative prior for hand-object interactions that allows modeling both the 3D object and a human hand conditioned on the object category. To learn a 3D spatial diffusion model that can capture this joint distribution we represent the human hand via a skeletal distance field to obtain a representation aligned with the (latent) signed distance field for the object. We show that this hand-object prior can then serve as a generic guidance to facilitate other tasks like reconstruction from interaction clip and human grasp synthesis. We believe that our model trained by aggregating several diverse real-world interaction datasets spanning 155 categories represents a first approach that allows jointly generating both hand and object. Our empirical evaluations demonstrate the benefit of this joint prior in video-based reconstruction and human grasp synthesis outperforming current task-specific baselines.","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Ye","given":"Yufei"},{"family":"Gupta","given":"Abhinav"},{"family":"Kitani","given":"Kris"},{"family":"Tulsiani","given":"Shubham"}],"citation-key":"yeGHOPGenerativeHandObject2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"1911-1920","source":"openaccess.thecvf.com","title":"G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis","title-short":"G-HOP","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Ye_G-HOP_Generative_Hand-Object_Prior_for_Interaction_Reconstruction_and_Grasp_Synthesis_CVPR_2024_paper.html"},
  {"id":"yuACRAttentionCollaborationBased2023","abstract":"Reconstructing two hands from monocular RGB images is challenging due to frequent occlusion and mutual confusion. Existing methods mainly learn an entangled representation to encode two interacting hands, which are incredibly fragile to impaired interaction, such as truncated hands, separate hands, or external occlusion. This paper presents ACR (Attention Collaboration-based Regressor), which makes the first attempt to reconstruct hands in arbitrary scenarios. To achieve this, ACR explicitly mitigates interdependencies between hands and between parts by leveraging center and part-based attention for feature extraction. However, reducing interdependence helps release the input constraint while weakening the mutual reasoning about reconstructing the interacting hands. Thus, based on center attention, ACR also learns cross-hand prior that handle the interacting hands better. We evaluate our method on various types of hand reconstruction datasets. Our method significantly outperforms the best interacting-hand approaches on the InterHand2.6M dataset while yielding comparable performance with the state-of-the-art single-hand methods on the FreiHand dataset. More qualitative results on in-the-wild and hand-object interaction datasets and web images/videos further demonstrate the effectiveness of our approach for arbitrary hand reconstruction. Our code is available at https://github.com/ZhengdiYu/Arbitrary-Hands-3D-Reconstruction","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Yu","given":"Zhengdi"},{"family":"Huang","given":"Shaoli"},{"family":"Fang","given":"Chen"},{"family":"Breckon","given":"Toby P."},{"family":"Wang","given":"Jue"}],"citation-key":"yuACRAttentionCollaborationBased2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12955-12964","source":"openaccess.thecvf.com","title":"ACR: Attention Collaboration-Based Regressor for Arbitrary Two-Hand Reconstruction","title-short":"ACR","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Yu_ACR_Attention_Collaboration-Based_Regressor_for_Arbitrary_Two-Hand_Reconstruction_CVPR_2023_paper.html"},
  {"id":"yuOvercomingTradeOffAccuracy2023","abstract":"Direct mesh fitting for 3D hand shape reconstruction estimates highly accurate meshes. However, the resulting meshes are prone to artifacts and do not appear as plausible hand shapes. Conversely, parametric models like MANO ensure plausible hand shapes but are not as accurate as the non-parametric methods. In this work, we introduce a novel weakly-supervised hand shape estimation framework that integrates non-parametric mesh fitting with MANO models in an end-to-end fashion. Our joint model overcomes the tradeoff in accuracy and plausibility to yield well-aligned and high-quality 3D meshes, especially in challenging two-hand and hand-object interaction scenarios.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Yu","given":"Ziwei"},{"family":"Li","given":"Chen"},{"family":"Yang","given":"Linlin"},{"family":"Zheng","given":"Xiaoxu"},{"family":"Mi","given":"Michael Bi"},{"family":"Lee","given":"Gim Hee"},{"family":"Yao","given":"Angela"}],"citation-key":"yuOvercomingTradeOffAccuracy2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"544-553","source":"openaccess.thecvf.com","title":"Overcoming the Trade-Off Between Accuracy and Plausibility in 3D Hand Shape Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Overcoming_the_Trade-Off_Between_Accuracy_and_Plausibility_in_3D_Hand_CVPR_2023_paper.html"},
  {"id":"zhangBOTH2HandsInferring3D2024","abstract":"The recently emerging text-to-motion advances have spired numerous attempts for convenient and interactive human motion generation. Yet existing methods are largely limited to generating body motions only without considering the rich two-hand motions let alone handling various conditions like body dynamics or texts. To break the data bottleneck we propose BOTH57M a novel multi-modal dataset for two-hand motion generation. Our dataset includes accurate motion tracking for the human body and hands and provides pair-wised finger-level hand annotations and body descriptions. We further provide a strong baseline method BOTH2Hands for the novel task: generating vivid two-hand motions from both implicit body dynamics and explicit text prompts. We first warm up two parallel body-to-hand and text-to-hand diffusion models and then utilize the cross-attention transformer for motion blending. Extensive experiments and cross-validations demonstrate the effectiveness of our approach and dataset for generating convincing two-hand motions from the hybrid body-and-textual conditions. Our dataset and code will be disseminated to the community for future research which can be found at https://github.com/Godheritage/BOTH2Hands.","accessed":{"date-parts":[["2024",10,25]]},"author":[{"family":"Zhang","given":"Wenqian"},{"family":"Huang","given":"Molin"},{"family":"Zhou","given":"Yuxuan"},{"family":"Zhang","given":"Juze"},{"family":"Yu","given":"Jingyi"},{"family":"Wang","given":"Jingya"},{"family":"Xu","given":"Lan"}],"citation-key":"zhangBOTH2HandsInferring3D2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"2393-2404","source":"openaccess.thecvf.com","title":"BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics","title-short":"BOTH2Hands","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_BOTH2Hands_Inferring_3D_Hands_from_Both_Text_Prompts_and_Body_CVPR_2024_paper.html"},
  {"id":"zhangDDFHOHandHeldObject2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Zhang","given":"Chenyangguang"},{"family":"Di","given":"Yan"},{"family":"Zhang","given":"Ruida"},{"family":"Zhai","given":"Guangyao"},{"family":"Manhardt","given":"Fabian"},{"family":"Tombari","given":"Federico"},{"family":"Ji","given":"Xiangyang"}],"citation-key":"zhangDDFHOHandHeldObject2023","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2023",12,15]]},"language":"en","page":"56871-56884","source":"proceedings.neurips.cc","title":"DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field","title-short":"DDF-HO","type":"article-journal","URL":"https://proceedings.neurips.cc/paper_files/paper/2023/hash/b2876deb92cbd098219a10da25671577-Abstract-Conference.html","volume":"36"},
  {"id":"zhangHOIDiffusionGeneratingRealistic2024","abstract":"3D hand-object interaction data is scarce due to the hardware constraints in scaling up the data collection process. In this paper we propose HOIDiffusion for generating realistic and diverse 3D hand-object interaction data. Our model is a conditional diffusion model that takes both the 3D hand-object geometric structure and text description as inputs for image synthesis. This offers a more controllable and realistic synthesis as we can specify the structure and style inputs in a disentangled manner. HOIDiffusion is trained by leveraging a diffusion model pre-trained on large-scale natural images and a few 3D human demonstrations. Beyond controllable image synthesis we adopt the generated 3D data for learning 6D object pose estimation and show its effectiveness in improving perception systems. Project page: https://mq-zhang1.github.io/HOIDiffusion.","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Zhang","given":"Mengqi"},{"family":"Fu","given":"Yang"},{"family":"Ding","given":"Zheng"},{"family":"Liu","given":"Sifei"},{"family":"Tu","given":"Zhuowen"},{"family":"Wang","given":"Xiaolong"}],"citation-key":"zhangHOIDiffusionGeneratingRealistic2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"8521-8531","source":"openaccess.thecvf.com","title":"HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data","title-short":"HOIDiffusion","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_HOIDiffusion_Generating_Realistic_3D_Hand-Object_Interaction_Data_CVPR_2024_paper.html"},
  {"id":"zhangNL2ContactNaturalLanguage","abstract":"Modeling the physical contacts between the hand and object is standard for refining inaccurate hand poses and generating novel human grasp in 3D hand-object reconstruction. However, existing methods rely on geometric constraints that cannot be specified or controlled. This paper introduces a novel task of controllable 3D hand-object contact modeling with natural language descriptions. Challenges include i) the complexity of cross-modal modeling from language to contact, and ii) a lack of descriptive text for contact patterns. To address these issues, we propose NL2Contact, a model that generates controllable contacts by leveraging staged diffusion models. Given a language description of the hand and contact, NL2Contact generates realistic and faithful 3D hand-object contacts. To train the model, we build ContactDescribe, the first dataset with hand-centered contact descriptions. It contains multilevel and diverse descriptions generated by large language models based on carefully designed prompts (e.g., grasp action, grasp type, contact location, free finger status). We show applications of our model to grasp pose optimization and novel human grasp generation, both based on a textual contact description.","author":[{"family":"Zhang","given":"Zhongqun"},{"family":"Wang","given":"Hengfei"},{"family":"Yu","given":"Ziwei"},{"family":"Cheng","given":"Yihua"},{"family":"Yao","given":"Angela"},{"family":"Chang","given":"Hyung Jin"}],"citation-key":"zhangNL2ContactNaturalLanguage","language":"en","source":"Zotero","title":"NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model","type":"article-journal"},
  {"id":"zhangNL2ContactNaturalLanguage2024","abstract":"Modeling the physical contacts between the hand and object is standard for refining inaccurate hand poses and generating novel human grasp in 3D hand-object reconstruction. However, existing methods rely on geometric constraints that cannot be specified or controlled. This paper introduces a novel task of controllable 3D hand-object contact modeling with natural language descriptions. Challenges include i) the complexity of cross-modal modeling from language to contact, and ii) a lack of descriptive text for contact patterns. To address these issues, we propose NL2Contact, a model that generates controllable contacts by leveraging staged diffusion models. Given a language description of the hand and contact, NL2Contact generates realistic and faithful 3D hand-object contacts. To train the model, we build ContactDescribe, the first dataset with hand-centered contact descriptions. It contains multilevel and diverse descriptions generated by large language models based on carefully designed prompts (e.g., grasp action, grasp type, contact location, free finger status). We show applications of our model to grasp pose optimization and novel human grasp generation, both based on a textual contact description.","accessed":{"date-parts":[["2024",7,23]]},"author":[{"family":"Zhang","given":"Zhongqun"},{"family":"Wang","given":"Hengfei"},{"family":"Yu","given":"Ziwei"},{"family":"Cheng","given":"Yihua"},{"family":"Yao","given":"Angela"},{"family":"Chang","given":"Hyung Jin"}],"citation-key":"zhangNL2ContactNaturalLanguage2024","issued":{"date-parts":[["2024",7,17]]},"language":"en","number":"arXiv:2407.12727","publisher":"arXiv","source":"arXiv.org","title":"NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model","title-short":"NL2Contact","type":"article","URL":"http://arxiv.org/abs/2407.12727"},
  {"id":"zhangOCHIDFiOcclusionRobustHand2023","abstract":"Hand Pose Estimation (HPE) is crucial to many applications, but conventional cameras-based CM-HPE methods are completely subject to Line-of-Sight (LoS), as cameras cannot capture occluded objects. In this paper, we propose to exploit Radio-Frequency-Vision (RF-vision) capable of bypassing obstacles for achieving occluded HPE, and we introduce OCHID-Fi as the first RF-HPE method with 3D pose estimation capability. OCHID-Fi employs wideband RF sensors widely available on smart devices (e.g., iPhones) to probe 3D human hand pose and extract their skeletons behind obstacles. To overcome the challenge in labeling RF imaging given its human incomprehensible nature, OCHID-Fi employs a cross-modality and cross-domain training process. It uses a pre-trained CM-HPE network and a synchronized CM/RF dataset, to guide the training of its complex-valued RF-HPE network under LoS conditions. It further transfers knowledge learned from labeled LoS domain to unlabeled occluded domain via adversarial learning, enabling OCHID-Fi to generalize to unseen occluded scenarios. Experimental results demonstrate the superiority of OCHID-Fi: it achieves comparable accuracy to CM-HPE under normal conditions while maintaining such accuracy even in occluded scenarios, with empirical evidence for its generalizability to new domains.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Zhang","given":"Shujie"},{"family":"Zheng","given":"Tianyue"},{"family":"Chen","given":"Zhe"},{"family":"Hu","given":"Jingzhi"},{"family":"Khamis","given":"Abdelwahed"},{"family":"Liu","given":"Jiajun"},{"family":"Luo","given":"Jun"}],"citation-key":"zhangOCHIDFiOcclusionRobustHand2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"15112-15121","source":"openaccess.thecvf.com","title":"OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision","title-short":"OCHID-Fi","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_OCHID-Fi_Occlusion-Robust_Hand_Pose_Estimation_in_3D_via_RF-Vision_ICCV_2023_paper.html"},
  {"id":"zhangWeaklySupervised3DHand2024","abstract":"Fully-supervised monocular 3D hand reconstruction is often difficult because capturing the requisite 3D data entails deploying specialized equipment in a controlled environment. We introduce a weakly-supervised method that avoids such requirements by leveraging fundamental principles well-established in the understanding of the human hand's unique structure and functionality. Specifically, we systematically study hand knowledge from different sources, including biomechanics, functional anatomy, and physics. We effectively incorporate these valuable foundational insights into 3D hand reconstruction models through an appropriate set of differentiable training losses. This enables training solely with readily-obtainable 2D hand landmark annotations and eliminates the need for expensive 3D supervision. Moreover, we explicitly model the uncertainty that is inherent in image observations. We enhance the training process by exploiting a simple yet effective Negative Log Likelihood (NLL) loss that incorporates uncertainty into the loss function. Through extensive experiments, we demonstrate that our method significantly outperforms state-of-the-art weakly-supervised methods. For example, our method achieves nearly a 21\\% performance improvement on the widely adopted FreiHAND dataset.","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Zhang","given":"Yufei"},{"family":"Kephart","given":"Jeffrey O."},{"family":"Ji","given":"Qiang"}],"citation-key":"zhangWeaklySupervised3DHand2024","issued":{"date-parts":[["2024",7,17]]},"number":"arXiv:2407.12307","publisher":"arXiv","source":"arXiv.org","title":"Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance","type":"article","URL":"http://arxiv.org/abs/2407.12307"},
  {"id":"zhaoSemiSupervisedHandAppearance2023","abstract":"Enormous hand images with reliable annotations are collected through marker-based MoCap. Unfortunately, degradations caused by markers limit their application in hand appearance reconstruction. A clear appearance recovery insight is an image-to-image translation trained with unpaired data. However, most frameworks fail because there exists structure inconsistency from a degraded hand to a bare one. The core of our approach is to first disentangle the bare hand structure from those degraded images and then wrap the appearance to this structure with a dual adversarial discrimination (DAD) scheme. Both modules take full advantage of the semi-supervised learning paradigm: The structure disentanglement benefits from the modeling ability of ViT, and the translator is enhanced by the dual discrimination on both translation processes and translation results. Comprehensive evaluations have been conducted to prove that our framework can robustly recover photo-realistic hand appearance from diverse marker-contained and even object-occluded datasets. It provides a novel avenue to acquire bare hand appearance data for other downstream learning problems.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Zhao","given":"Zimeng"},{"family":"Zuo","given":"Binghui"},{"family":"Long","given":"Zhiyu"},{"family":"Wang","given":"Yangang"}],"citation-key":"zhaoSemiSupervisedHandAppearance2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12125-12136","source":"openaccess.thecvf.com","title":"Semi-Supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Semi-Supervised_Hand_Appearance_Recovery_via_Structure_Disentanglement_and_Dual_Adversarial_CVPR_2023_paper.html"},
  {"id":"zhaoStabilityDrivenContactReconstruction2022","accessed":{"date-parts":[["2024",11,7]]},"author":[{"family":"Zhao","given":"Zimeng"},{"family":"Zuo","given":"Binghui"},{"family":"Xie","given":"Wei"},{"family":"Wang","given":"Yangang"}],"citation-key":"zhaoStabilityDrivenContactReconstruction2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"1643-1653","source":"openaccess.thecvf.com","title":"Stability-Driven Contact Reconstruction From Monocular Color Images","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Zhao_Stability-Driven_Contact_Reconstruction_From_Monocular_Color_Images_CVPR_2022_paper.html"},
  {"id":"zhaoSUPPLEExtractingHand2021","abstract":"Embedding a unified skeleton into diverse hand meshes is a prominent task both for animation and pose estimation. Most existing methods extracted skeletons from humanoid characters under simple poses, e.g. T- pose or A- pose. Applying them directly to hand meshes may yield inaccurate or implausible results because hands have higher dexterity and similar endpoints. Furthermore, these methods did not attempt to extract skeleton directly from a scan model which may be not watertight and has much more vertices. Our key idea is to unwrap meshes with different topologies in the same image-based representation, named SUPPLE (Spherical UnwraPping ProfiLEs), and then train a convolutional encoder-decoder to extract skeleton under this representation. Experiments demonstrate that our framework produces reliable and accurate skeleton estimation results across a broad range of datasets, from raw scans to artist-designed models.","accessed":{"date-parts":[["2024",11,7]]},"author":[{"family":"Zhao","given":"Zimeng"},{"family":"Rao","given":"Ruting"},{"family":"Wang","given":"Yangang"}],"citation-key":"zhaoSUPPLEExtractingHand2021","container-title":"2021 International Conference on 3D Vision (3DV)","DOI":"10.1109/3DV53792.2021.00098","event-title":"2021 International Conference on 3D Vision (3DV)","ISSN":"2475-7888","issued":{"date-parts":[["2021",12]]},"page":"899-909","source":"IEEE Xplore","title":"SUPPLE: Extracting Hand Skeleton with Spherical Unwrapping Profiles","title-short":"SUPPLE","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9665909"},
  {"id":"zhengCAMSCAnonicalizedManipulation2023","abstract":"In this work, we focus on a novel task of category-level functional hand-object manipulation synthesis covering both rigid and articulated object categories. Given an object geometry, an initial human hand pose as well as a sparse control sequence of object poses, our goal is to generate a physically reasonable hand-object manipulation sequence that performs like human beings. To address such a challenge, we first design CAnonicalized Manipulation Spaces (CAMS), a two-level space hierarchy that canonicalizes the hand poses in an object-centric and contact-centric view. Benefiting from the representation capability of CAMS, we then present a two-stage framework for synthesizing human-like manipulation animations. Our framework achieves state-of-the-art performance for both rigid and articulated categories with impressive visual effects. Codes and video results can be found at our project homepage: https://cams-hoi.github.io/.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Zheng","given":"Juntian"},{"family":"Zheng","given":"Qingyuan"},{"family":"Fang","given":"Lixing"},{"family":"Liu","given":"Yun"},{"family":"Yi","given":"Li"}],"citation-key":"zhengCAMSCAnonicalizedManipulation2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"585-594","source":"openaccess.thecvf.com","title":"CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis","title-short":"CAMS","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_CAMS_CAnonicalized_Manipulation_Spaces_for_Category-Level_Functional_Hand-Object_Manipulation_Synthesis_CVPR_2023_paper.html"},
  {"id":"zhengHaMuCoHandPose2023","abstract":"Recent advancements in 3D hand pose estimation have shown promising results, but its effectiveness has primarily relied on the availability of large-scale annotated datasets, the creation of which is a laborious and costly process. To alleviate the label-hungry limitation, we propose a self-supervised learning framework, HaMuCo, that learns a single view hand pose estimator from multi-view pseudo 2D labels. However, one of the main challenges of self-supervised learning is the presence of noisy labels and the \"groupthink\" effect from multiple views. To overcome these issues, we introduce a cross-view interaction network that distills the single view estimator by utilizing the cross-view correlated features and enforcing multi-view consistency to achieve collaborative learning. Both the single view estimator and the cross-view interaction network are trained jointly in an end-to-end manner. Extensive experiments show that our method can achieve state-of-the-art performance on multi-view self-supervised hand pose estimation. Furthermore, the proposed cross-view interaction network can also be applied to hand pose estimation from multi-view input and outperforms previous methods under same settings.","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Zheng","given":"Xiaozheng"},{"family":"Wen","given":"Chao"},{"family":"Xue","given":"Zhou"},{"family":"Ren","given":"Pengfei"},{"family":"Wang","given":"Jingyu"}],"citation-key":"zhengHaMuCoHandPose2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"20763-20773","source":"openaccess.thecvf.com","title":"HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning","title-short":"HaMuCo","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_HaMuCo_Hand_Pose_Estimation_via_Multiview_Collaborative_Self-Supervised_Learning_ICCV_2023_paper.html"},
  {"id":"zhouGEARSLocalGeometryaware2024","abstract":"Generating realistic hand motion sequences in interaction with objects has gained increasing attention with the growing interest in digital humans. Prior work has illustrated the effectiveness of employing occupancy-based or distance-based virtual sensors to extract hand-object interaction features. Nonetheless these methods show limited generalizability across object categories shapes and sizes. We hypothesize that this is due to two reasons: 1) the limited expressiveness of employed virtual sensors and 2) scarcity of available training data. To tackle this challenge we introduce a novel joint-centered sensor designed to reason about local object geometry near potential interaction regions. The sensor queries for object surface points in the neighbourhood of each hand joint. As an important step towards mitigating the learning complexity we transform the points from global frame to hand template frame and use a shared module to process sensor features of each individual joint. This is followed by a spatio-temporal transformer network aimed at capturing correlation among the joints in different dimensions. Moreover we devise simple heuristic rules to augment the limited training sequences with vast static hand grasping samples. This leads to a broader spectrum of grasping types observed during training in turn enhancing our model's generalization capability. We evaluate on two public datasets GRAB and InterCap where our method shows superiority over baselines both quantitatively and perceptually.","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Zhou","given":"Keyang"},{"family":"Bhatnagar","given":"Bharat Lal"},{"family":"Lenssen","given":"Jan Eric"},{"family":"Pons-Moll","given":"Gerard"}],"citation-key":"zhouGEARSLocalGeometryaware2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"20634-20643","source":"openaccess.thecvf.com","title":"GEARS: Local Geometry-aware Hand-object Interaction Synthesis","title-short":"GEARS","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_GEARS_Local_Geometry-aware_Hand-object_Interaction_Synthesis_CVPR_2024_paper.html"},
  {"id":"zhouMonocularRealTimeHand2020","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Zhou","given":"Yuxiao"},{"family":"Habermann","given":"Marc"},{"family":"Xu","given":"Weipeng"},{"family":"Habibie","given":"Ikhsanul"},{"family":"Theobalt","given":"Christian"},{"family":"Xu","given":"Feng"}],"citation-key":"zhouMonocularRealTimeHand2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"5346-5355","source":"openaccess.thecvf.com","title":"Monocular Real-Time Hand Shape and Motion Capture Using Multi-Modal Data","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_Monocular_Real-Time_Hand_Shape_and_Motion_Capture_Using_Multi-Modal_Data_CVPR_2020_paper.html"},
  {"id":"zhouMonocularRealTimeHand2020a","abstract":"We present a novel method for monocular hand shape and pose estimation at unprecedented runtime performance of 100fps and at state-of-the-art accuracy. This is enabled by a new learning based architecture designed such that it can make use of all the sources of available hand training data: image data with either 2D or 3D annotations, as well as stand-alone 3D animations without corresponding image data. It features a 3D hand joint detection module and an inverse kinematics module which regresses not only 3D joint positions but also maps them to joint rotations in a single feed-forward pass. This output makes the method more directly usable for applications in computer vision and graphics compared to only regressing 3D joint positions. We demonstrate that our architectural design leads to a signiÔ¨Åcant quantitative and qualitative improvement over the state of the art on several challenging benchmarks. We will make our code publicly available for future research.","accessed":{"date-parts":[["2024",11,6]]},"author":[{"family":"Zhou","given":"Yuxiao"},{"family":"Habermann","given":"Marc"},{"family":"Xu","given":"Weipeng"},{"family":"Habibie","given":"Ikhsanul"},{"family":"Theobalt","given":"Christian"},{"family":"Xu","given":"Feng"}],"citation-key":"zhouMonocularRealTimeHand2020a","container-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR42600.2020.00539","event-place":"Seattle, WA, USA","event-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-72817-168-5","issued":{"date-parts":[["2020",6]]},"language":"en","license":"https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","page":"5345-5354","publisher":"IEEE","publisher-place":"Seattle, WA, USA","source":"DOI.org (Crossref)","title":"Monocular Real-Time Hand Shape and Motion Capture Using Multi-Modal Data","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9157654/"},
  {"id":"zhouSimpleBaselineEfficient2024","abstract":"Hand mesh reconstruction has attracted considerable attention in recent years with various approaches and techniques being proposed. Some of these methods incorporate complex components and designs which while effective may complicate the model and hinder efficiency. In this paper we decompose the mesh decoder into token generator and mesh regressor. Through extensive ablation experiments we found that the token generator should select discriminating and representative points while the mesh regressor needs to upsample sparse keypoints into dense meshes in multiple stages. Given these functionalities we can achieve high performance with minimal computational resources. Based on this observation we propose a simple yet effective baseline that outperforms state-of-the-art methods by a large margin while maintaining real-time efficiency. Our method outperforms existing solutions achieving state-of-the-art (SOTA) results across multiple datasets. On the FreiHAND dataset our approach produced a PA-MPJPE of 5.8mm and a PA-MPVPE of 6.1mm. Similarly on the DexYCB dataset we observed a PA-MPJPE of 5.5mm and a PA-MPVPE of 5.5mm. As for performance speed our method reached up to 33 frames per second (fps) when using HRNet and up to 70 fps when employing FastViT-MA36. Code will be made available.","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Zhou","given":"Zhishan"},{"family":"Zhou","given":"Shihao"},{"family":"Lv","given":"Zhi"},{"family":"Zou","given":"Minqiang"},{"family":"Tang","given":"Yao"},{"family":"Liang","given":"Jiajun"}],"citation-key":"zhouSimpleBaselineEfficient2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"1367-1376","source":"openaccess.thecvf.com","title":"A Simple Baseline for Efficient Hand Mesh Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_A_Simple_Baseline_for_Efficient_Hand_Mesh_Reconstruction_CVPR_2024_paper.html"},
  {"id":"zimmermannFreiHANDDatasetMarkerless2019","accessed":{"date-parts":[["2024",9,30]]},"author":[{"family":"Zimmermann","given":"Christian"},{"family":"Ceylan","given":"Duygu"},{"family":"Yang","given":"Jimei"},{"family":"Russell","given":"Bryan"},{"family":"Argus","given":"Max"},{"family":"Brox","given":"Thomas"}],"citation-key":"zimmermannFreiHANDDatasetMarkerless2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"813-822","source":"openaccess.thecvf.com","title":"FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape From Single RGB Images","title-short":"FreiHAND","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Zimmermann_FreiHAND_A_Dataset_for_Markerless_Capture_of_Hand_Pose_and_ICCV_2019_paper.html"},
  {"id":"zuoReconstructingInteractingHands2023","abstract":"Reconstructing interacting hands from monocular images is indispensable in AR/VR applications. Most existing solutions rely on the accurate localization of each skeleton joint. However, these methods tend to be unreliable due to the severe occlusion and confusing similarity among adjacent hand parts. This also defies human perception because humans can quickly imitate an interaction pattern without localizing all joints. Our key idea is to first construct a two-hand interaction prior and recast the interaction reconstruction task as the conditional sampling from the prior. To expand more interaction states, a large-scale multimodal dataset with physical plausibility is proposed. Then a VAE is trained to further condense these interaction patterns as latent codes in a prior distribution. When looking for image cues that contribute to interaction prior sampling, we propose the interaction adjacency heatmap (IAH). Compared with a joint-wise heatmap for localization, IAH assigns denser visible features to those invisible joints. Compared with an all-in-one visible heatmap, it provides more fine-grained local interaction information in each interaction region. Finally, the correlations between the extracted features and corresponding interaction codes are linked by the ViT module. Comprehensive evaluations on benchmark datasets have verified the effectiveness of this framework. The code and dataset are publicly available at https://github. com/binghui-z/InterPrior_pytorch.","accessed":{"date-parts":[["2024",11,19]]},"author":[{"family":"Zuo","given":"Binghui"},{"family":"Zhao","given":"Zimeng"},{"family":"Sun","given":"Wenqian"},{"family":"Xie","given":"Wei"},{"family":"Xue","given":"Zhou"},{"family":"Wang","given":"Yangang"}],"citation-key":"zuoReconstructingInteractingHands2023","container-title":"2023 IEEE/CVF International Conference on Computer Vision (ICCV)","DOI":"10.1109/ICCV51070.2023.00831","event-place":"Paris, France","event-title":"2023 IEEE/CVF International Conference on Computer Vision (ICCV)","ISBN":"9798350307184","issued":{"date-parts":[["2023",10,1]]},"language":"en","license":"https://doi.org/10.15223/policy-029","page":"9020-9030","publisher":"IEEE","publisher-place":"Paris, France","source":"DOI.org (Crossref)","title":"Reconstructing Interacting Hands with Interaction Prior from Monocular Images","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10378019/"}
]
