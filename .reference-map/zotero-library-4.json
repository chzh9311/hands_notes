[
  {"id":"aichDataFreeClassIncrementalHand2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Aich","given":"Shubhra"},{"family":"Ruiz-Santaquiteria","given":"Jesus"},{"family":"Lu","given":"Zhenyu"},{"family":"Garg","given":"Prachi"},{"family":"Joseph","given":"K. J."},{"family":"Garcia","given":"Alvaro Fernandez"},{"family":"Balasubramanian","given":"Vineeth N."},{"family":"Kin","given":"Kenrick"},{"family":"Wan","given":"Chengde"},{"family":"Camgoz","given":"Necati Cihan"},{"family":"Ma","given":"Shugao"},{"family":"De la Torre","given":"Fernando"}],"citation-key":"aichDataFreeClassIncrementalHand2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"20958-20967","source":"openaccess.thecvf.com","title":"Data-Free Class-Incremental Hand Gesture Recognition","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Aich_Data-Free_Class-Incremental_Hand_Gesture_Recognition_ICCV_2023_paper.html"},
  {"id":"ansuiniObjectActionSame2008","abstract":"Objects can be grasped in several ways due to their physical properties, the context surrounding the object, and the goal of the grasping agent. The aim of the present study was to investigate whether the prior-to-contact grasping kinematics of the same object vary as a result of different goals of the person grasping it. Subjects were requested to reach toward and grasp a bottle filled with water, and then complete one of the following tasks: (1) Grasp it without performing any subsequent action; (2) Lift and throw it; (3) Pour the water into a container; (4) Place it accurately on a target area; (5) Pass it to another person. We measured the angular excursions at both metacarpal-phalangeal (mcp) and proximal interphalangeal (pip) joints of all digits, and abduction angles of adjacent digit pairs by means of resistive sensors embedded in a glove. The results showed that the presence and the nature of the task to be performed following grasping affect the positioning of the fingers during the reaching phase. We contend that a one-to-one association between a sensory stimulus and a motor response does not capture all the aspects involved in grasping. The theoretical approach within which we frame our discussion considers internal models of anticipatory control which may provide a suitable explanation of our results.","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Ansuini","given":"Caterina"},{"family":"Giosa","given":"Livia"},{"family":"Turella","given":"Luca"},{"family":"Altoè","given":"Gianmarco"},{"family":"Castiello","given":"Umberto"}],"citation-key":"ansuiniObjectActionSame2008","container-title":"Experimental Brain Research","container-title-short":"Exp Brain Res","DOI":"10.1007/s00221-007-1136-4","ISSN":"1432-1106","issue":"1","issued":{"date-parts":[["2008",2,1]]},"language":"en","page":"111-119","source":"Springer Link","title":"An object for an action, the same object for other actions: effects on hand shaping","title-short":"An object for an action, the same object for other actions","type":"article-journal","URL":"https://doi.org/10.1007/s00221-007-1136-4","volume":"185"},
  {"id":"aziziOcclusionHandling3D","abstract":"Understanding human behavior fundamentally relies on accurate 3D human pose estimation. Graph Convolutional Networks (GCNs) have recently shown promising advancements, delivering state-of-the-art performance with rather lightweight architectures. In the context of graph-structured data, leveraging the eigenvectors of the graph Laplacian matrix for positional encoding is effective. Yet, the approach does not specify how to handle scenarios where edges in the input graph are missing. To this end, we propose a novel positional encoding technique, PerturbPE, that extracts consistent and regular components from the eigenbasis. Our method involves applying multiple perturbations and taking their average to extract the consistent and regular component from the eigenbasis. PerturbPE leverages the Rayleigh-Schrodinger Perturbation Theorem (RSPT) for calculating the perturbed eigenvectors. Employing this labeling technique enhances the robustness and generalizability of the model. Our results support our theoretical findings, e.g. our experimental analysis observed a performance enhancement of up to 12% on the Human3.6M dataset in instances where occlusion resulted in the absence of one edge. Furthermore, our novel approach significantly enhances performance in scenarios where two edges are missing, setting a new benchmark for state-of-the-art.","author":[{"family":"Azizi","given":"Niloofar"},{"family":"Fayyaz","given":"Mohsen"},{"family":"Bischof","given":"Horst"}],"citation-key":"aziziOcclusionHandling3D","language":"en","source":"Zotero","title":"Occlusion Handling in 3D Human Pose Estimation with Perturbed Positional Encoding","type":"article-journal"},
  {"id":"baoUncertaintyawareStateSpace2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Bao","given":"Wentao"},{"family":"Chen","given":"Lele"},{"family":"Zeng","given":"Libing"},{"family":"Li","given":"Zhong"},{"family":"Xu","given":"Yi"},{"family":"Yuan","given":"Junsong"},{"family":"Kong","given":"Yu"}],"citation-key":"baoUncertaintyawareStateSpace2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"13702-13711","source":"openaccess.thecvf.com","title":"Uncertainty-aware State Space Transformer for Egocentric 3D Hand Trajectory Forecasting","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Bao_Uncertainty-aware_State_Space_Transformer_for_Egocentric_3D_Hand_Trajectory_Forecasting_ICCV_2023_paper.html"},
  {"id":"brahmbhattContactDBAnalyzingPredicting2019","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Brahmbhatt","given":"Samarth"},{"family":"Ham","given":"Cusuh"},{"family":"Kemp","given":"Charles C."},{"family":"Hays","given":"James"}],"citation-key":"brahmbhattContactDBAnalyzingPredicting2019","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2019"]]},"page":"8709-8719","source":"openaccess.thecvf.com","title":"ContactDB: Analyzing and Predicting Grasp Contact via Thermal Imaging","title-short":"ContactDB","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2019/html/Brahmbhatt_ContactDB_Analyzing_and_Predicting_Grasp_Contact_via_Thermal_Imaging_CVPR_2019_paper.html"},
  {"id":"brahmbhattContactPoseDatasetGrasps2020","abstract":"Grasping is natural for humans. However, it involves complex hand configurations and soft tissue deformation that can result in complicated regions of contact between the hand and the object. Understanding and modeling this contact can potentially improve hand models, AR/VR experiences, and robotic grasping. Yet, we currently lack datasets of hand-object contact paired with other data modalities, which is crucial for developing and evaluating contact modeling techniques. We introduce ContactPose, the first dataset of hand-object contact paired with hand pose, object pose, and RGB-D images. ContactPose has 2306 unique grasps of 25 household objects grasped with 2 functional intents by 50 participants, and more than 2.9 M RGB-D grasp images. Analysis of ContactPose data reveals interesting relationships between hand pose and contact. We use this data to rigorously evaluate various data representations, heuristics from the literature, and learning methods for contact modeling. Data, code, and trained models are available at https://contactpose.cc.gatech.edu.","author":[{"family":"Brahmbhatt","given":"Samarth"},{"family":"Tang","given":"Chengcheng"},{"family":"Twigg","given":"Christopher D."},{"family":"Kemp","given":"Charles C."},{"family":"Hays","given":"James"}],"citation-key":"brahmbhattContactPoseDatasetGrasps2020","container-title":"Computer Vision – ECCV 2020","DOI":"10.1007/978-3-030-58601-0_22","editor":[{"family":"Vedaldi","given":"Andrea"},{"family":"Bischof","given":"Horst"},{"family":"Brox","given":"Thomas"},{"family":"Frahm","given":"Jan-Michael"}],"event-place":"Cham","ISBN":"978-3-030-58601-0","issued":{"date-parts":[["2020"]]},"language":"en","page":"361-378","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"ContactPose: A Dataset of Grasps with Object Contact and Hand Pose","title-short":"ContactPose","type":"paper-conference"},
  {"id":"castielloNeuroscienceGrasping2005","abstract":"Considerable advances in our knowledge of human and non-human primate grasping control have been made during the past decade, using a combination of behavioural, neuroimaging and electrophysiological approaches. As a result, the neural circuitry and mechanisms of grasping are being elucidated. However, few attempts have been made to reconcile findings across species.Various experiments using kinematic techniques indicate that the mechanics of grasping in humans vary depending on object attributes such as fragility, size, shape, texture and weight. By contrast, kinematic studies in monkeys have been limited and confined to the testing of an object's size and shape.Single-unit physiology studies indicate that grasping might be represented by neurons in a network of brain areas, including the motor, premotor and parietal cortices. Because these areas contain representations of hand actions and the structures of objects, it has been suggested that their integrity might be crucial for successful grasping.Findings from patients with brain damage who have difficulty in grasping objects are difficult to reconcile with neurophysiological findings, as the patients' lesions are confined to regions that, in monkeys, do not seem to be involved in grasping-related visuomotor transformations.Recent positron emission tomography and functional MRI studies in humans have indicated possible human homologues of the brain regions that seem to be involved in grasping in monkeys. However, because of the difficulty of studying real grasping in the neuroimaging environment, many laboratories have taken to studying rather unnatural tasks. So, there are inconsistencies in the experimental protocols that make these results difficult to compare and interpret.One consistent region that has been identified as being involved in grasping tasks is the human homologue of the monkey anterior intraparietal area. However, in most of the studies that show such activation, participants were constrained to a single type of grasp (a precision grip), which raises the question of whether different grasping patterns are represented in this area.Contextual information that is involved in the unfolding of the grasping action (for example, using the same object for different purposes) has been largely neglected. A fundamental step is to uncover whether and to what extent contextual factors affect movement organization in humans and monkeys.To make progress in this field, it would be useful to implement a coordinated series of experiments in which similar protocols are applied to monkeys and humans, using various techniques. This multi-pronged approach should ideally combine functional imaging with MRI-compatible electrophysiological and kinematic recordings.","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Castiello","given":"Umberto"}],"citation-key":"castielloNeuroscienceGrasping2005","container-title":"Nature Reviews Neuroscience","container-title-short":"Nat Rev Neurosci","DOI":"10.1038/nrn1744","ISSN":"1471-0048","issue":"9","issued":{"date-parts":[["2005",9]]},"language":"en","license":"2005 Springer Nature Limited","page":"726-736","publisher":"Nature Publishing Group","source":"www.nature.com","title":"The neuroscience of grasping","type":"article-journal","URL":"https://www.nature.com/articles/nrn1744","volume":"6"},
  {"id":"chaoDexYCBBenchmarkCapturing2021","accessed":{"date-parts":[["2024",9,30]]},"author":[{"family":"Chao","given":"Yu-Wei"},{"family":"Yang","given":"Wei"},{"family":"Xiang","given":"Yu"},{"family":"Molchanov","given":"Pavlo"},{"family":"Handa","given":"Ankur"},{"family":"Tremblay","given":"Jonathan"},{"family":"Narang","given":"Yashraj S."},{"family":"Van Wyk","given":"Karl"},{"family":"Iqbal","given":"Umar"},{"family":"Birchfield","given":"Stan"},{"family":"Kautz","given":"Jan"},{"family":"Fox","given":"Dieter"}],"citation-key":"chaoDexYCBBenchmarkCapturing2021","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2021"]]},"language":"en","page":"9044-9053","source":"openaccess.thecvf.com","title":"DexYCB: A Benchmark for Capturing Hand Grasping of Objects","title-short":"DexYCB","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2021/html/Chao_DexYCB_A_Benchmark_for_Capturing_Hand_Grasping_of_Objects_CVPR_2021_paper.html"},
  {"id":"chaText2HOITextguided3D2024","accessed":{"date-parts":[["2024",10,22]]},"author":[{"family":"Cha","given":"Junuk"},{"family":"Kim","given":"Jihyeon"},{"family":"Yoon","given":"Jae Shin"},{"family":"Baek","given":"Seungryul"}],"citation-key":"chaText2HOITextguided3D2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"1577-1585","source":"openaccess.thecvf.com","title":"Text2HOI: Text-guided 3D Motion Generation for Hand-Object Interaction","title-short":"Text2HOI","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Cha_Text2HOI_Text-guided_3D_Motion_Generation_for_Hand-Object_Interaction_CVPR_2024_paper.html"},
  {"id":"cheng4Diff3DAwareDiffusion","abstract":"We present 4Diff, a 3D-aware diffusion model addressing the exo-to-ego viewpoint translation task — generating first-person (egocentric) view images from the corresponding third-person (exocentric) images. Building on the diffusion model’s ability to generate photorealistic images, we propose a transformer-based diffusion model that incorporates geometry priors through two mechanisms: (i) egocentric point cloud rasterization and (ii) 3D-aware rotary cross-attention. Egocentric point cloud rasterization converts the input exocentric image into an egocentric layout, which is subsequently used by a diffusion image transformer. As a component of the diffusion transformer’s denoiser block, the 3D-aware rotary cross-attention further incorporates 3D information and semantic features from the source exocentric view. Our 4Diff achieves stateof-the-art results on the challenging and diverse Ego-Exo4D multiview dataset and exhibits robust generalization to novel environments not encountered during training. Our code, processed data, and pretrained models are publicly available at https://klauscc.github.io/4diff.","author":[{"family":"Cheng","given":"Feng"},{"family":"Luo","given":"Mi"},{"family":"Wang","given":"Huiyu"},{"family":"Dimakis","given":"Alex"},{"family":"Torresani","given":"Lorenzo"},{"family":"Bertasius","given":"Gedas"},{"family":"Grauman","given":"Kristen"}],"citation-key":"cheng4Diff3DAwareDiffusion","language":"en","source":"Zotero","title":"4Diff: 3D-Aware Diffusion Model for Third-to-First Viewpoint Translation","type":"article-journal"},
  {"id":"chengHandDAGTDenoisingAdaptive2024","abstract":"The extraction of keypoint positions from input hand frames, known as 3D hand pose estimation, is crucial for various human-computer interaction applications. However, current approaches often struggle with the dynamic nature of self-occlusion of hands and intra-occlusion with interacting objects. To address this challenge, this paper proposes the Denoising Adaptive Graph Transformer, HandDAGT, for hand pose estimation. The proposed HandDAGT leverages a transformer structure to thoroughly explore effective geometric features from input patches. Additionally, it incorporates a novel attention mechanism to adaptively weigh the contribution of kinematic correspondence and local geometric features for the estimation of specific keypoints. This attribute enables the model to adaptively employ kinematic and local information based on the occlusion situation, enhancing its robustness and accuracy. Furthermore, we introduce a novel denoising training strategy aimed at improving the model's robust performance in the face of occlusion challenges. Experimental results show that the proposed model significantly outperforms the existing methods on four challenging hand pose benchmark datasets. Codes and pre-trained models are publicly available at https://github.com/cwc1260/HandDAGT.","accessed":{"date-parts":[["2024",10,22]]},"author":[{"family":"Cheng","given":"Wencan"},{"family":"Kim","given":"Eunji"},{"family":"Ko","given":"Jong Hwan"}],"citation-key":"chengHandDAGTDenoisingAdaptive2024","DOI":"10.48550/arXiv.2407.20542","issued":{"date-parts":[["2024",7,30]]},"number":"arXiv:2407.20542","publisher":"arXiv","source":"arXiv.org","title":"HandDAGT: A Denoising Adaptive Graph Transformer for 3D Hand Pose Estimation","title-short":"HandDAGT","type":"article","URL":"http://arxiv.org/abs/2407.20542"},
  {"id":"chengHandR2N2Iterative3D2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Cheng","given":"Wencan"},{"family":"Ko","given":"Jong Hwan"}],"citation-key":"chengHandR2N2Iterative3D2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"20904-20913","source":"openaccess.thecvf.com","title":"HandR2N2: Iterative 3D Hand Pose Estimation Using a Residual Recurrent Neural Network","title-short":"HandR2N2","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Cheng_HandR2N2_Iterative_3D_Hand_Pose_Estimation_Using_a_Residual_Recurrent_ICCV_2023_paper.html"},
  {"id":"chengRicher2DUnderstanding2023","accessed":{"date-parts":[["2024",10,4]]},"author":[{"family":"Cheng","given":"Tianyi"},{"family":"Shan","given":"Dandan"},{"family":"Hassen","given":"Ayda"},{"family":"Higgins","given":"Richard"},{"family":"Fouhey","given":"David"}],"citation-key":"chengRicher2DUnderstanding2023","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2023",12,15]]},"language":"en","page":"30453-30465","source":"proceedings.neurips.cc","title":"Towards A Richer 2D Understanding of Hands at Scale","type":"article-journal","URL":"https://proceedings.neurips.cc/paper_files/paper/2023/hash/612a7948f3294a02a63d970566ca8536-Abstract-Conference.html","volume":"36"},
  {"id":"chenGSDFGeometryDrivenSigned2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Chen","given":"Zerui"},{"family":"Chen","given":"Shizhe"},{"family":"Schmid","given":"Cordelia"},{"family":"Laptev","given":"Ivan"}],"citation-key":"chenGSDFGeometryDrivenSigned2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12890-12900","source":"openaccess.thecvf.com","title":"gSDF: Geometry-Driven Signed Distance Functions for 3D Hand-Object Reconstruction","title-short":"gSDF","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Chen_gSDF_Geometry-Driven_Signed_Distance_Functions_for_3D_Hand-Object_Reconstruction_CVPR_2023_paper.html"},
  {"id":"chenHandAvatarFreePose","author":[{"family":"Chen","given":"Xingyu"},{"family":"Wang","given":"Baoyuan"},{"family":"Shum","given":"Heung-Yeung"},{"family":"Ai","given":"Xiaobing"}],"citation-key":"chenHandAvatarFreePose","language":"en","source":"Zotero","title":"Hand Avatar: Free-Pose Hand Animation and Rendering from Monocular Video – Supplementary Material","type":"article-journal"},
  {"id":"chenHandAvatarFreePose2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Chen","given":"Xingyu"},{"family":"Wang","given":"Baoyuan"},{"family":"Shum","given":"Heung-Yeung"}],"citation-key":"chenHandAvatarFreePose2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8683-8693","source":"openaccess.thecvf.com","title":"Hand Avatar: Free-Pose Hand Animation and Rendering From Monocular Video","title-short":"Hand Avatar","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Chen_Hand_Avatar_Free-Pose_Hand_Animation_and_Rendering_From_Monocular_Video_CVPR_2023_paper.html"},
  {"id":"chenMobReconMobileFriendlyHand2022","accessed":{"date-parts":[["2024",9,30]]},"author":[{"family":"Chen","given":"Xingyu"},{"family":"Liu","given":"Yufeng"},{"family":"Dong","given":"Yajiao"},{"family":"Zhang","given":"Xiong"},{"family":"Ma","given":"Chongyang"},{"family":"Xiong","given":"Yanmin"},{"family":"Zhang","given":"Yuan"},{"family":"Guo","given":"Xiaoyan"}],"citation-key":"chenMobReconMobileFriendlyHand2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"20544-20554","source":"openaccess.thecvf.com","title":"MobRecon: Mobile-Friendly Hand Mesh Reconstruction From Monocular Image","title-short":"MobRecon","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Chen_MobRecon_Mobile-Friendly_Hand_Mesh_Reconstruction_From_Monocular_Image_CVPR_2022_paper.html"},
  {"id":"chenTrackingReconstructingHand2023","abstract":"In this work, we tackle the challenging task of jointly tracking hand object poses and reconstructing their shapes from depth point cloud sequences in the wild, given the initial poses at frame 0. We for the first time propose a point cloud-based hand joint tracking network, HandTrackNet, to estimate the inter-frame hand joint motion. Our HandTrackNet proposes a novel hand pose canonicalization module to ease the tracking task, yielding accurate and robust hand joint tracking. Our pipeline then reconstructs the full hand via converting the predicted hand joints into a MANO hand. For object tracking, we devise a simple yet effective module that estimates the object SDF from the first frame and performs optimization-based tracking. Finally, a joint optimization step is adopted to perform joint hand and object reasoning, which alleviates the occlusion-induced ambiguity and further refines the hand pose. During training, the whole pipeline only sees purely synthetic data, which are synthesized with sufficient variations and by depth simulation for the ease of generalization. The whole pipeline is pertinent to the generalization gaps and thus directly transferable to real in-the-wild data. We evaluate our method on two real hand object interaction datasets, e.g. HO3D and DexYCB, without any fine-tuning. Our experiments demonstrate that the proposed method significantly outperforms the previous state-of-the-art depth-based hand and object pose estimation and tracking methods, running at a frame rate of 9 FPS. We have released our code on https://github.com/PKU-EPIC/HOTrack.","accessed":{"date-parts":[["2024",10,9]]},"author":[{"family":"Chen","given":"Jiayi"},{"family":"Yan","given":"Mi"},{"family":"Zhang","given":"Jiazhao"},{"family":"Xu","given":"Yinzhen"},{"family":"Li","given":"Xiaolong"},{"family":"Weng","given":"Yijia"},{"family":"Yi","given":"Li"},{"family":"Song","given":"Shuran"},{"family":"Wang","given":"He"}],"citation-key":"chenTrackingReconstructingHand2023","container-title":"Proceedings of the AAAI Conference on Artificial Intelligence","DOI":"10.1609/aaai.v37i1.25103","ISSN":"2374-3468","issue":"1","issued":{"date-parts":[["2023",6,26]]},"language":"en","license":"Copyright (c) 2023 Association for the Advancement of Artificial Intelligence","number":"1","page":"304-312","source":"ojs.aaai.org","title":"Tracking and Reconstructing Hand Object Interactions from Point Cloud Sequences in the Wild","type":"article-journal","URL":"https://ojs.aaai.org/index.php/AAAI/article/view/25103","volume":"37"},
  {"id":"choCrossAttentionDisentangledModalities2022","abstract":"Transformer encoder architectures have recently achieved state-of-the-art results on monocular 3D human mesh reconstruction, but they require a substantial number of parameters and expensive computations. Due to the large memory overhead and slow inference speed, it is difficult to deploy such models for practical use. In this paper, we propose a novel transformer encoder-decoder architecture for 3D human mesh reconstruction from a single image, called FastMETRO. We identify the performance bottleneck in the encoder-based transformers is caused by the token design which introduces high complexity interactions among input tokens. We disentangle the interactions via an encoder-decoder architecture, which allows our model to demand much fewer parameters and shorter inference time. In addition, we impose the prior knowledge of human body’s morphological relationship via attention masking and mesh upsampling operations, which leads to faster convergence with higher accuracy. Our FastMETRO improves the Pareto-front of accuracy and efficiency, and clearly outperforms image-based methods on Human3.6M and 3DPW. Furthermore, we validate its generalizability on FreiHAND.","author":[{"family":"Cho","given":"Junhyeong"},{"family":"Youwang","given":"Kim"},{"family":"Oh","given":"Tae-Hyun"}],"citation-key":"choCrossAttentionDisentangledModalities2022","container-title":"Computer Vision – ECCV 2022","DOI":"10.1007/978-3-031-19769-7_20","editor":[{"family":"Avidan","given":"Shai"},{"family":"Brostow","given":"Gabriel"},{"family":"Cissé","given":"Moustapha"},{"family":"Farinella","given":"Giovanni Maria"},{"family":"Hassner","given":"Tal"}],"event-place":"Cham","ISBN":"978-3-031-19769-7","issued":{"date-parts":[["2022"]]},"language":"en","page":"342-359","publisher":"Springer Nature Switzerland","publisher-place":"Cham","source":"Springer Link","title":"Cross-Attention of Disentangled Modalities for 3D Human Mesh Recovery with Transformers","type":"paper-conference"},
  {"id":"choDenseHandObjectHOGraspNet","abstract":"Existing datasets for 3D hand-object interaction are limited either in the data cardinality, data variations in interaction scenarios, or the quality of annotations. In this work, we present a comprehensive new training dataset for hand-object interaction called HOGraspNet. It is the only real dataset that captures full grasp taxonomies, providing grasp annotation and wide intraclass variations. Using grasp taxonomies as atomic actions, their space and time combinatorial can represent complex hand activities around objects. We select 22 rigid objects from the YCB dataset and 8 other compound objects using shape and size taxonomies, ensuring coverage of all hand grasp configurations. The dataset includes diverse hand shapes from 99 participants aged 10 to 74, continuous video frames, and a 1.5M RGB-Depth of sparse frames with annotations. It offers labels for 3D hand and object meshes, 3D keypoints, contact maps, and grasp labels. Accurate hand and object 3D meshes are obtained by fitting the hand parametric model (MANO) and the hand implicit function (HALO) to multi-view RGBD frames, with the MoCap system only for objects. Note that HALO fitting does not require any parameter tuning, enabling scalability to the dataset’s size with comparable accuracy to MANO. We evaluate HOGraspNet on relevant tasks: grasp classification and 3D hand pose estimation. The result shows performance variations based on grasp type and object class, indicating the potential importance of the interaction space captured by our dataset. The provided data aims at learning universal shape priors or foundation models for 3D hand-object interaction. Our dataset and code are available at https://hograspnet2024.github.io/.","author":[{"family":"Cho","given":"Woojin"},{"family":"Lee","given":"Jihyun"},{"family":"Yi","given":"Minjae"},{"family":"Kim","given":"Minje"},{"family":"Woo","given":"Taeyun"},{"family":"Kim","given":"Donghwan"},{"family":"Ha","given":"Taewook"},{"family":"Lee","given":"Hyokeun"},{"family":"Ryu","given":"Je-Hwan"},{"family":"Woo","given":"Woontack"},{"family":"Kim","given":"Tae-Kyun"}],"citation-key":"choDenseHandObjectHOGraspNet","language":"en","source":"Zotero","title":"Dense Hand-Object(HO) GraspNet with Full Grasping Taxonomy and Dynamics","type":"article-journal"},
  {"id":"coronaGanHandPredictingHuman2020","abstract":"The rise of deep learning has brought remarkable progress in estimating hand geometry from images where the hands are part of the scene. This paper focuses on a new problem not explored so far, consisting in predicting how a human would grasp one or several objects, given a single RGB image of these objects. This is a problem with enormous potential in e.g. augmented reality, robotics or prosthetic design. In order to predict feasible grasps, we need to understand the semantic content of the image, its geometric structure and all potential interactions with a hand physical model. To this end, we introduce a generative model that jointly reasons in all these levels and 1) regresses the 3D shape and pose of the objects in the scene; 2) estimates the grasp types; and 3) reﬁnes the 51-DoF of a 3D hand model that minimize a graspability loss. To train this model we build the YCB-Affordance dataset, that contains more than 133k images of 21 objects in the YCB-Video dataset [69]. We have annotated these images with more than 28M plausible 3D human grasps according to a 33-class taxonomy. A thorough evaluation in synthetic and real images shows that our model can robustly predict realistic grasps, even in cluttered scenes with multiple objects in close contact.","accessed":{"date-parts":[["2024",10,29]]},"author":[{"family":"Corona","given":"Enric"},{"family":"Pumarola","given":"Albert"},{"family":"Alenya","given":"Guillem"},{"family":"Moreno-Noguer","given":"Francesc"},{"family":"Rogez","given":"Gregory"}],"citation-key":"coronaGanHandPredictingHuman2020","container-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR42600.2020.00508","event-place":"Seattle, WA, USA","event-title":"2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-72817-168-5","issued":{"date-parts":[["2020",6]]},"language":"en","license":"https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","page":"5030-5040","publisher":"IEEE","publisher-place":"Seattle, WA, USA","source":"DOI.org (Crossref)","title":"GanHand: Predicting Human Grasp Affordances in Multi-Object Scenes","title-short":"GanHand","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9156512/"},
  {"id":"coronaLISALearningImplicit2022a","abstract":"This paper proposes a do-it-all neural model of human hands, named LISA. The model can capture accurate hand shape and appearance, generalize to arbitrary hand subjects, provide dense surface correspondences, be reconstructed from images in the wild and easily animated. We train LISA by minimizing the shape and appearance losses on a large set of multi-view RGB image sequences annotated with coarse 3D poses of the hand skeleton. For a 3D point in the hand local coordinate, our model predicts the color and the signed distance with respect to each hand bone independently, and then combines the per-bone predictions using predicted skinning weights. The shape, color and pose representations are disentangled by design, allowing to estimate or animate only selected parameters. We experimentally demonstrate that LISA can accurately reconstruct a dynamic hand from monocular or multi-view sequences, achieving a noticeably higher quality of reconstructed hand shapes compared to baseline approaches. Project page: https://www.iri.upc.edu/people/ecorona/lisa/.","accessed":{"date-parts":[["2024",11,4]]},"author":[{"family":"Corona","given":"Enric"},{"family":"Hodan","given":"Tomas"},{"family":"Vo","given":"Minh"},{"family":"Moreno-Noguer","given":"Francesc"},{"family":"Sweeney","given":"Chris"},{"family":"Newcombe","given":"Richard"},{"family":"Ma","given":"Lingni"}],"citation-key":"coronaLISALearningImplicit2022a","issued":{"date-parts":[["2022",4,4]]},"number":"arXiv:2204.01695","publisher":"arXiv","source":"arXiv.org","title":"LISA: Learning Implicit Shape and Appearance of Hands","title-short":"LISA","type":"article","URL":"http://arxiv.org/abs/2204.01695"},
  {"id":"daoTransformersAreSSMs2024","abstract":"While Transformers have been the main architecture behind deep learning's success in language modeling, state-space models (SSMs) such as Mamba have recently been shown to match or outperform Transformers at small to medium scale. We show that these families of models are actually quite closely related, and develop a rich framework of theoretical connections between SSMs and variants of attention, connected through various decompositions of a well-studied class of structured semiseparable matrices. Our state space duality (SSD) framework allows us to design a new architecture (Mamba-2) whose core layer is an a refinement of Mamba's selective SSM that is 2-8X faster, while continuing to be competitive with Transformers on language modeling.","accessed":{"date-parts":[["2024",10,23]]},"author":[{"family":"Dao","given":"Tri"},{"family":"Gu","given":"Albert"}],"citation-key":"daoTransformersAreSSMs2024","DOI":"10.48550/arXiv.2405.21060","issued":{"date-parts":[["2024",5,31]]},"number":"arXiv:2405.21060","publisher":"arXiv","source":"arXiv.org","title":"Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality","title-short":"Transformers are SSMs","type":"article","URL":"http://arxiv.org/abs/2405.21060"},
  {"id":"dongHambaSingleview3D2024","abstract":"3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, but they fail to achieve robust and accurate performance due to insufficient modeling of joint spatial relations. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to learn the joint relations and spatial sequences for enhancing the reconstruction performance. Specifically, we design a novel Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space modeling features and jointly considers global and local features to improve performance. Extensive experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. Hamba is currently Rank 1 in two challenging competition leaderboards on 3D hand reconstruction. The code will be available upon acceptance. [Website](https://humansensinglab.github.io/Hamba/).","accessed":{"date-parts":[["2024",7,23]]},"author":[{"family":"Dong","given":"Haoye"},{"family":"Chharia","given":"Aviral"},{"family":"Gou","given":"Wenbo"},{"family":"Carrasco","given":"Francisco Vicente"},{"family":"De la Torre","given":"Fernando"}],"citation-key":"dongHambaSingleview3D2024","issued":{"date-parts":[["2024",7,12]]},"language":"en","number":"arXiv:2407.09646","publisher":"arXiv","source":"arXiv.org","title":"Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba","title-short":"Hamba","type":"article","URL":"http://arxiv.org/abs/2407.09646"},
  {"id":"dongHambaSingleview3D2024a","abstract":"3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, but they fail to achieve robust and accurate performance due to insufficient modeling of joint spatial relations. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to learn the joint relations and spatial sequences for enhancing the reconstruction performance. Specifically, we design a novel Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space modeling features and jointly considers global and local features to improve performance. Extensive experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. Hamba is currently Rank 1 in two challenging competition leaderboards on 3D hand reconstruction. The code will be available upon acceptance. [Website](https://humansensinglab.github.io/Hamba/).","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Dong","given":"Haoye"},{"family":"Chharia","given":"Aviral"},{"family":"Gou","given":"Wenbo"},{"family":"Carrasco","given":"Francisco Vicente"},{"family":"Torre","given":"Fernando De","dropping-particle":"la"}],"citation-key":"dongHambaSingleview3D2024a","DOI":"10.48550/arXiv.2407.09646","issued":{"date-parts":[["2024",7,12]]},"number":"arXiv:2407.09646","publisher":"arXiv","source":"arXiv.org","title":"Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba","title-short":"Hamba","type":"article","URL":"http://arxiv.org/abs/2407.09646"},
  {"id":"dongHambaSingleview3D2024b","abstract":"3D Hand reconstruction from a single RGB image is challenging due to the articulated motion, self-occlusion, and interaction with objects. Existing SOTA methods employ attention-based transformers to learn the 3D hand pose and shape, but they fail to achieve robust and accurate performance due to insufficient modeling of joint spatial relations. To address this problem, we propose a novel graph-guided Mamba framework, named Hamba, which bridges graph learning and state space modeling. Our core idea is to reformulate Mamba's scanning into graph-guided bidirectional scanning for 3D reconstruction using a few effective tokens. This enables us to learn the joint relations and spatial sequences for enhancing the reconstruction performance. Specifically, we design a novel Graph-guided State Space (GSS) block that learns the graph-structured relations and spatial sequences of joints and uses 88.5% fewer tokens than attention-based methods. Additionally, we integrate the state space features and the global features using a fusion module. By utilizing the GSS block and the fusion module, Hamba effectively leverages the graph-guided state space modeling features and jointly considers global and local features to improve performance. Extensive experiments on several benchmarks and in-the-wild tests demonstrate that Hamba significantly outperforms existing SOTAs, achieving the PA-MPVPE of 5.3mm and F@15mm of 0.992 on FreiHAND. Hamba is currently Rank 1 in two challenging competition leaderboards on 3D hand reconstruction. The code will be available upon acceptance. [Website](https://humansensinglab.github.io/Hamba/).","accessed":{"date-parts":[["2024",10,17]]},"author":[{"family":"Dong","given":"Haoye"},{"family":"Chharia","given":"Aviral"},{"family":"Gou","given":"Wenbo"},{"family":"Carrasco","given":"Francisco Vicente"},{"family":"Torre","given":"Fernando De","dropping-particle":"la"}],"citation-key":"dongHambaSingleview3D2024b","DOI":"10.48550/arXiv.2407.09646","issued":{"date-parts":[["2024",7,12]]},"number":"arXiv:2407.09646","publisher":"arXiv","source":"arXiv.org","title":"Hamba: Single-view 3D Hand Reconstruction with Graph-guided Bi-Scanning Mamba","title-short":"Hamba","type":"article","URL":"http://arxiv.org/abs/2407.09646"},
  {"id":"fanARCTICDatasetDexterous2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Fan","given":"Zicong"},{"family":"Taheri","given":"Omid"},{"family":"Tzionas","given":"Dimitrios"},{"family":"Kocabas","given":"Muhammed"},{"family":"Kaufmann","given":"Manuel"},{"family":"Black","given":"Michael J."},{"family":"Hilliges","given":"Otmar"}],"citation-key":"fanARCTICDatasetDexterous2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12943-12954","source":"openaccess.thecvf.com","title":"ARCTIC: A Dataset for Dexterous Bimanual Hand-Object Manipulation","title-short":"ARCTIC","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Fan_ARCTIC_A_Dataset_for_Dexterous_Bimanual_Hand-Object_Manipulation_CVPR_2023_paper.html"},
  {"id":"fanBenchmarksChallengesPose","abstract":"We interact with the world with our hands and see it through our own (egocentric) perspective. A holistic 3D understanding of such interactions from egocentric views is important for tasks in robotics, AR/VR, action recognition and motion generation. Accurately reconstructing such interactions in 3D is challenging due to heavy occlusion, viewpoint bias, camera distortion, and motion blur from the head movement. To this end, we designed the HANDS23 challenge based on the AssemblyHands and ARCTIC datasets with carefully designed training and testing splits. Based on the results of the top submitted methods and more recent baselines on the leaderboards, we perform a thorough analysis on 3D hand(-object) reconstruction tasks. Our analysis demonstrates the effectiveness of addressing distortion specific to egocentric cameras, adopting high-capacity transformers to learn complex hand-object interactions, and fusing predictions from different views. Our study further reveals challenging scenarios intractable with state-of-the-art methods, such as fast hand motion, object reconstruction from narrow egocentric views, and close contact between two hands and objects. Our efforts will enrich the community’s knowledge foundation and facilitate future hand studies on egocentric hand-object interactions.","author":[{"family":"Fan","given":"Zicong"},{"family":"Ohkawa","given":"Takehiko"},{"family":"Yang","given":"Linlin"},{"family":"Lin","given":"Nie"},{"family":"Zhou","given":"Zhishan"},{"family":"Zhou","given":"Shihao"},{"family":"Liang","given":"Jiajun"},{"family":"Gao","given":"Zhong"},{"family":"Zhang","given":"Xuanyang"},{"family":"Zhang","given":"Xue"},{"family":"Li","given":"Fei"},{"family":"Zheng","given":"Liu"},{"family":"Lu","given":"Feng"},{"family":"Zeid","given":"Karim Abou"},{"family":"Leibe","given":"Bastian"},{"family":"On","given":"Jeongwan"},{"family":"Baek","given":"Seungryul"},{"family":"Prakash","given":"Aditya"},{"family":"Gupta","given":"Saurabh"},{"family":"He","given":"Kun"},{"family":"Sato","given":"Yoichi"},{"family":"Hilliges","given":"Otmar"},{"family":"Chang","given":"Hyung Jin"},{"family":"Yao","given":"Angela"}],"citation-key":"fanBenchmarksChallengesPose","language":"en","source":"Zotero","title":"Benchmarks and Challenges in Pose Estimation for Egocentric Hand Interactions with Objects","type":"article-journal"},
  {"id":"fanHOLDCategoryagnostic3D2023","abstract":"Since humans interact with diverse objects every day, the holistic 3D capture of these interactions is important to understand and model human behaviour. However, most existing methods for hand-object reconstruction from RGB either assume pre-scanned object templates or heavily rely on limited 3D hand-object data, restricting their ability to scale and generalize to more unconstrained interaction settings. To this end, we introduce HOLD -- the first category-agnostic method that reconstructs an articulated hand and object jointly from a monocular interaction video. We develop a compositional articulated implicit model that can reconstruct disentangled 3D hand and object from 2D images. We also further incorporate hand-object constraints to improve hand-object poses and consequently the reconstruction quality. Our method does not rely on 3D hand-object annotations while outperforming fully-supervised baselines in both in-the-lab and challenging in-the-wild settings. Moreover, we qualitatively show its robustness in reconstructing from in-the-wild videos. Code: https://github.com/zc-alexfan/hold","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Fan","given":"Zicong"},{"family":"Parelli","given":"Maria"},{"family":"Kadoglou","given":"Maria Eleni"},{"family":"Kocabas","given":"Muhammed"},{"family":"Chen","given":"Xu"},{"family":"Black","given":"Michael J."},{"family":"Hilliges","given":"Otmar"}],"citation-key":"fanHOLDCategoryagnostic3D2023","DOI":"10.48550/arXiv.2311.18448","issued":{"date-parts":[["2023",11,30]]},"number":"arXiv:2311.18448","publisher":"arXiv","source":"arXiv.org","title":"HOLD: Category-agnostic 3D Reconstruction of Interacting Hands and Objects from Video","title-short":"HOLD","type":"article","URL":"http://arxiv.org/abs/2311.18448"},
  {"id":"fanLearningDisambiguateStrongly2021","abstract":"In natural conversation and interaction, our hands often overlap or are in contact with each other. Due to the homogeneous appearance of hands, this makes estimating the 3D pose of interacting hands from images difficult. In this paper we demonstrate that self-similarity, and the resulting ambiguities in assigning pixel observations to the respective hands and their parts, is a major cause of the final 3D pose error. Motivated by this insight, we propose DIGIT, a novel method for estimating the 3D poses of two interacting hands from a single monocular image. The method consists of two interwoven branches that process the input imagery into a per-pixel semantic part segmentation mask and a visual feature volume. In contrast to prior work, we do not decouple the segmentation from the pose estimation stage, but rather leverage the per-pixel probabilities directly in the downstream pose estimation task. To do so, the part probabilities are merged with the visual features and processed via fully-convolutional layers. We experimentally show that the proposed approach achieves new state-of-the-art performance on the InterHand2.6M [33] dataset. We provide detailed ablation studies to demonstrate the efficacy of our method and to provide insights into how the modelling of pixel ownership affects 3D hand pose estimation.","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Fan","given":"Zicong"},{"family":"Spurr","given":"Adrian"},{"family":"Kocabas","given":"Muhammed"},{"family":"Tang","given":"Siyu"},{"family":"Black","given":"Michael J."},{"family":"Hilliges","given":"Otmar"}],"citation-key":"fanLearningDisambiguateStrongly2021","container-title":"2021 International Conference on 3D Vision (3DV)","DOI":"10.1109/3DV53792.2021.00011","event-title":"2021 International Conference on 3D Vision (3DV)","ISSN":"2475-7888","issued":{"date-parts":[["2021",12]]},"page":"1-10","source":"IEEE Xplore","title":"Learning To Disambiguate Strongly Interacting Hands via Probabilistic Per-Pixel Part Segmentation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/9665878"},
  {"id":"fuDeformerDynamicFusion2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Fu","given":"Qichen"},{"family":"Liu","given":"Xingyu"},{"family":"Xu","given":"Ran"},{"family":"Niebles","given":"Juan Carlos"},{"family":"Kitani","given":"Kris M."}],"citation-key":"fuDeformerDynamicFusion2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"23600-23611","source":"openaccess.thecvf.com","title":"Deformer: Dynamic Fusion Transformer for Robust Hand Pose Estimation","title-short":"Deformer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Fu_Deformer_Dynamic_Fusion_Transformer_for_Robust_Hand_Pose_Estimation_ICCV_2023_paper.html"},
  {"id":"fuDSCoDualStreamConditional","abstract":"Reconstructing hand-held objects from a single RGB image is a challenging task in computer vision. In contrast to prior works that utilize deterministic modeling paradigms, we employ a point cloud denoising diffusion model to account for the probabilistic nature of this problem. In the core, we introduce centroid-fixed Dual-Stream Conditional diffusion for monocular hand-held object reconstruction (D-SCo), tackling two predominant challenges. First, to avoid the object centroid from deviating, we utilize a novel hand-constrained centroid fixing paradigm, enhancing the stability of diffusion and reverse processes and the precision of feature projection. Second, we introduce a dual-stream denoiser to semantically and geometrically model hand-object interactions with a novel unified hand-object semantic embedding, enhancing the reconstruction performance of the hand-occluded region of the object. Experiments on the synthetic ObMan dataset and three real-world datasets HO3D, MOW and DexYCB demonstrate that our approach can surpass all other state-of-the-art methods.","author":[{"family":"Fu","given":"Bowen"},{"family":"Wang","given":"Gu"},{"family":"Zhang","given":"Chenyangguang"},{"family":"Di","given":"Yan"},{"family":"Huang","given":"Ziqin"},{"family":"Leng","given":"Zhiying"},{"family":"Manhardt","given":"Fabian"},{"family":"Ji","given":"Xiangyang"},{"family":"Tombari","given":"Federico"}],"citation-key":"fuDSCoDualStreamConditional","language":"en","source":"Zotero","title":"D-SCo: Dual-Stream Conditional Diffusion for Monocular Hand-Held Object Reconstruction","type":"article-journal"},
  {"id":"garcia-hernandoFirstPersonHandAction2018","abstract":"In this work we study the use of 3D hand poses to recognize first-person dynamic hand actions interacting with 3D objects. Towards this goal, we collected RGB-D video sequences comprised of more than 100K frames of 45 daily hand action categories, involving 26 different objects in several hand configurations. To obtain hand pose annotations, we used our own mo-cap system that automatically infers the 3D location of each of the 21 joints of a hand model via 6 magnetic sensors and inverse kinematics. Additionally, we recorded the 6D object poses and provide 3D object models for a subset of hand-object interaction sequences. To the best of our knowledge, this is the first benchmark that enables the study of first-person hand actions with the use of 3D hand poses. We present an extensive experimental evaluation of RGB-D and pose-based action recognition by 18 baselines/state-of-the-art approaches. The impact of using appearance features, poses, and their combinations are measured, and the different training/testing protocols are evaluated. Finally, we assess how ready the 3D hand pose estimation field is when hands are severely occluded by objects in egocentric views and its influence on action recognition. From the results, we see clear benefits of using hand pose as a cue for action recognition compared to other data modalities. Our dataset and experiments can be of interest to communities of 3D hand pose estimation, 6D object pose, and robotics as well as action recognition.","accessed":{"date-parts":[["2024",10,29]]},"author":[{"family":"Garcia-Hernando","given":"Guillermo"},{"family":"Yuan","given":"Shanxin"},{"family":"Baek","given":"Seungryul"},{"family":"Kim","given":"Tae-Kyun"}],"citation-key":"garcia-hernandoFirstPersonHandAction2018","DOI":"10.48550/arXiv.1704.02463","issued":{"date-parts":[["2018",4,10]]},"number":"arXiv:1704.02463","publisher":"arXiv","source":"arXiv.org","title":"First-Person Hand Action Benchmark with RGB-D Videos and 3D Hand Pose Annotations","type":"article","URL":"http://arxiv.org/abs/1704.02463"},
  {"id":"gradyContactOptOptimizingContact2021a","abstract":"Physical contact between hands and objects plays a critical role in human grasps. We show that optimizing the pose of a hand to achieve expected contact with an object can improve hand poses inferred via image-based methods. Given a hand mesh and an object mesh, a deep model trained on ground truth contact data infers desirable contact across the surfaces of the meshes. Then, ContactOpt efﬁciently optimizes the pose of the hand to achieve desirable contact using a differentiable contact model. Notably, our contact model encourages mesh interpenetration to approximate deformable soft tissue in the hand. In our evaluations, our methods resulted in grasps that better matched ground truth contact, had lower kinematic error, and were signiﬁcantly preferred by human participants. Code for this work will be publicly released.","accessed":{"date-parts":[["2024",10,15]]},"author":[{"family":"Grady","given":"Patrick"},{"family":"Tang","given":"Chengcheng"},{"family":"Twigg","given":"Christopher D."},{"family":"Vo","given":"Minh"},{"family":"Brahmbhatt","given":"Samarth"},{"family":"Kemp","given":"Charles C."}],"citation-key":"gradyContactOptOptimizingContact2021a","container-title":"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR46437.2021.00152","event-place":"Nashville, TN, USA","event-title":"2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"978-1-66544-509-2","issued":{"date-parts":[["2021",6]]},"language":"en","license":"https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html","page":"1471-1481","publisher":"IEEE","publisher-place":"Nashville, TN, USA","source":"DOI.org (Crossref)","title":"ContactOpt: Optimizing Contact to Improve Grasps","title-short":"ContactOpt","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/9578455/"},
  {"id":"graumanEgo4DWorld3000","author":[{"family":"Grauman","given":"Kristen"},{"family":"Westbury","given":"Andrew"},{"family":"Byrne","given":"Eugene"},{"family":"Chavis","given":"Zachary"},{"family":"Furnari","given":"Antonino"},{"family":"Girdhar","given":"Rohit"},{"family":"Hamburger","given":"Jackson"},{"family":"Jiang","given":"Hao"},{"family":"Liu","given":"Miao"},{"family":"Liu","given":"Xingyu"},{"family":"Martin","given":"Miguel"},{"family":"Nagarajan","given":"Tushar"},{"family":"Radosavovic","given":"Ilija"},{"family":"Ramakrishnan","given":"Santhosh Kumar"},{"family":"Ryan","given":"Fiona"},{"family":"Sharma","given":"Jayant"},{"family":"Wray","given":"Michael"},{"family":"Xu","given":"Mengmeng"},{"family":"Xu","given":"Eric Zhongcong"},{"family":"Zhao","given":"Chen"},{"family":"Bansal","given":"Siddhant"},{"family":"Batra","given":"Dhruv"},{"family":"Cartillier","given":"Vincent"},{"family":"Crane","given":"Sean"},{"family":"Do","given":"Tien"},{"family":"Doulaty","given":"Morrie"},{"family":"Erapalli","given":"Akshay"},{"family":"Feichtenhofer","given":"Christoph"},{"family":"Fragomeni","given":"Adriano"},{"family":"Fu","given":"Qichen"},{"family":"Gebreselasie","given":"Abrham"},{"family":"Gonzalez","given":"Cristina"},{"family":"Hillis","given":"James"},{"family":"Huang","given":"Xuhua"},{"family":"Huang","given":"Yifei"},{"family":"Jia","given":"Wenqi"},{"family":"Khoo","given":"Weslie"},{"family":"Kolar","given":"Jachym"},{"family":"Kottur","given":"Satwik"},{"family":"Kumar","given":"Anurag"},{"family":"Landini","given":"Federico"},{"family":"Li","given":"Chao"},{"family":"Li","given":"Yanghao"},{"family":"Li","given":"Zhenqiang"},{"family":"Mangalam","given":"Karttikeya"},{"family":"Modhugu","given":"Raghava"},{"family":"Munro","given":"Jonathan"},{"family":"Murrell","given":"Tullie"},{"family":"Nishiyasu","given":"Takumi"},{"family":"Price","given":"Will"},{"family":"Ruiz","given":"Paola"},{"family":"Ramazanova","given":"Merey"},{"family":"Sari","given":"Leda"},{"family":"Somasundaram","given":"Kiran"},{"family":"Southerland","given":"Audrey"},{"family":"Sugano","given":"Yusuke"},{"family":"Tao","given":"Ruijie"},{"family":"Vo","given":"Minh"},{"family":"Wang","given":"Yuchen"},{"family":"Wu","given":"Xindi"},{"family":"Yagi","given":"Takuma"},{"family":"Zhao","given":"Ziwei"},{"family":"Zhu","given":"Yunyi"},{"family":"Arbelaez","given":"Pablo"},{"family":"Crandall","given":"David"},{"family":"Damen","given":"Dima"},{"family":"Farinella","given":"Giovanni Maria"},{"family":"Fuegen","given":"Christian"},{"family":"Ghanem","given":"Bernard"},{"family":"Ithapu","given":"Vamsi Krishna"},{"family":"Jawahar","given":"C V"},{"family":"Joo","given":"Hanbyul"},{"family":"Kitani","given":"Kris"},{"family":"Li","given":"Haizhou"},{"family":"Newcombe","given":"Richard"},{"family":"Oliva","given":"Aude"},{"family":"Park","given":"Hyun Soo"},{"family":"Rehg","given":"James M"},{"family":"Sato","given":"Yoichi"},{"family":"Shi","given":"Jianbo"},{"family":"Shou","given":"Mike Zheng"},{"family":"Torralba","given":"Antonio"},{"family":"Torresani","given":"Lorenzo"},{"family":"Yan","given":"Mingfei"},{"family":"Malik","given":"Jitendra"}],"citation-key":"graumanEgo4DWorld3000","language":"en","source":"Zotero","title":"Ego4D: Around the World in 3,000 Hours of Egocentric Video","type":"article-journal"},
  {"id":"guMambaLinearTimeSequence2024","abstract":"Foundation models, now powering most of the exciting applications in deep learning, are almost universally based on the Transformer architecture and its core attention module. Many subquadratic-time architectures such as linear attention, gated convolution and recurrent models, and structured state space models (SSMs) have been developed to address Transformers' computational inefficiency on long sequences, but they have not performed as well as attention on important modalities such as language. We identify that a key weakness of such models is their inability to perform content-based reasoning, and make several improvements. First, simply letting the SSM parameters be functions of the input addresses their weakness with discrete modalities, allowing the model to selectively propagate or forget information along the sequence length dimension depending on the current token. Second, even though this change prevents the use of efficient convolutions, we design a hardware-aware parallel algorithm in recurrent mode. We integrate these selective SSMs into a simplified end-to-end neural network architecture without attention or even MLP blocks (Mamba). Mamba enjoys fast inference (5$\\times$ higher throughput than Transformers) and linear scaling in sequence length, and its performance improves on real data up to million-length sequences. As a general sequence model backbone, Mamba achieves state-of-the-art performance across several modalities such as language, audio, and genomics. On language modeling, our Mamba-3B model outperforms Transformers of the same size and matches Transformers twice its size, both in pretraining and downstream evaluation.","accessed":{"date-parts":[["2024",10,17]]},"author":[{"family":"Gu","given":"Albert"},{"family":"Dao","given":"Tri"}],"citation-key":"guMambaLinearTimeSequence2024","DOI":"10.48550/arXiv.2312.00752","issued":{"date-parts":[["2024",5,31]]},"number":"arXiv:2312.00752","publisher":"arXiv","source":"arXiv.org","title":"Mamba: Linear-Time Sequence Modeling with Selective State Spaces","title-short":"Mamba","type":"article","URL":"http://arxiv.org/abs/2312.00752"},
  {"id":"hampaliHand3DObject2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Hampali","given":"Shreyas"},{"family":"Hodan","given":"Tomas"},{"family":"Tran","given":"Luan"},{"family":"Ma","given":"Lingni"},{"family":"Keskin","given":"Cem"},{"family":"Lepetit","given":"Vincent"}],"citation-key":"hampaliHand3DObject2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"17079-17088","source":"openaccess.thecvf.com","title":"In-Hand 3D Object Scanning From an RGB Sequence","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Hampali_In-Hand_3D_Object_Scanning_From_an_RGB_Sequence_CVPR_2023_paper.html"},
  {"id":"hassonLeveragingPhotometricConsistency2020","accessed":{"date-parts":[["2024",10,3]]},"author":[{"family":"Hasson","given":"Yana"},{"family":"Tekin","given":"Bugra"},{"family":"Bogo","given":"Federica"},{"family":"Laptev","given":"Ivan"},{"family":"Pollefeys","given":"Marc"},{"family":"Schmid","given":"Cordelia"}],"citation-key":"hassonLeveragingPhotometricConsistency2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"571-580","source":"openaccess.thecvf.com","title":"Leveraging Photometric Consistency Over Time for Sparsely Supervised Hand-Object Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Hasson_Leveraging_Photometric_Consistency_Over_Time_for_Sparsely_Supervised_Hand-Object_Reconstruction_CVPR_2020_paper.html"},
  {"id":"hassonUnconstrainedJointHandObject2021","abstract":"Our work aims to obtain 3D reconstruction of hands and manipulated objects from monocular videos. Reconstructing hand-object manipulations holds a great potential for robotics and learning from human demonstrations. The supervised learning approach to this problem, however, requires 3D supervision and remains limited to constrained laboratory settings and simulators for which 3D ground truth is available. In this paper we first propose a learning-free fitting approach for hand-object reconstruction which can seamlessly handle two-hand object interactions. Our method relies on cues obtained with common methods for object detection, hand pose estimation and instance segmentation. We quantitatively evaluate our approach and show that it can be applied to datasets with varying levels of difficulty for which training data is unavailable.","accessed":{"date-parts":[["2024",10,3]]},"author":[{"family":"Hasson","given":"Yana"},{"family":"Varol","given":"Gül"},{"family":"Schmid","given":"Cordelia"},{"family":"Laptev","given":"Ivan"}],"citation-key":"hassonUnconstrainedJointHandObject2021","container-title":"2021 International Conference on 3D Vision (3DV)","DOI":"10.1109/3DV53792.2021.00075","event-title":"2021 International Conference on 3D Vision (3DV)","ISSN":"2475-7888","issued":{"date-parts":[["2021",12]]},"page":"659-668","source":"IEEE Xplore","title":"Towards Unconstrained Joint Hand-Object Reconstruction From RGB Videos","type":"paper-conference","URL":"https://ieeexplore.ieee.org/abstract/document/9665955"},
  {"id":"huangNeuralVotingField2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Huang","given":"Lin"},{"family":"Lin","given":"Chung-Ching"},{"family":"Lin","given":"Kevin"},{"family":"Liang","given":"Lin"},{"family":"Wang","given":"Lijuan"},{"family":"Yuan","given":"Junsong"},{"family":"Liu","given":"Zicheng"}],"citation-key":"huangNeuralVotingField2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8969-8978","source":"openaccess.thecvf.com","title":"Neural Voting Field for Camera-Space 3D Hand Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Huang_Neural_Voting_Field_for_Camera-Space_3D_Hand_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"huangPHRITParametricHand2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Huang","given":"Zhisheng"},{"family":"Chen","given":"Yujin"},{"family":"Kang","given":"Di"},{"family":"Zhang","given":"Jinlu"},{"family":"Tu","given":"Zhigang"}],"citation-key":"huangPHRITParametricHand2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"14974-14984","source":"openaccess.thecvf.com","title":"PHRIT: Parametric Hand Representation with Implicit Template","title-short":"PHRIT","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Huang_PHRIT_Parametric_Hand_Representation_with_Implicit_Template_ICCV_2023_paper.html"},
  {"id":"huHandObjectInteractionController2024","abstract":"Hand manipulating objects is an important interaction motion in our daily activities. We faithfully reconstruct this motion with a single RGBD camera by a novel deep reinforcement learning method to leverage physics. Firstly, we propose object compensation control which establishes direct object control to make the network training more stable. Meanwhile, by leveraging the compensation force and torque, we seamlessly upgrade the simple point contact model to a more physical-plausible surface contact model, further improving the reconstruction accuracy and physical correctness. Experiments indicate that without involving any heuristic physical rules, this work still successfully involves physics in the reconstruction of hand-object interactions which are complex motions hard to imitate with deep reinforcement learning. Our code and data are available at https://github.com/hu-hy17/HOIC.","accessed":{"date-parts":[["2024",10,10]]},"author":[{"family":"Hu","given":"Haoyu"},{"family":"Yi","given":"Xinyu"},{"family":"Cao","given":"Zhe"},{"family":"Yong","given":"Jun-Hai"},{"family":"Xu","given":"Feng"}],"citation-key":"huHandObjectInteractionController2024","DOI":"10.48550/arXiv.2405.02676","issued":{"date-parts":[["2024",5,4]]},"number":"arXiv:2405.02676","publisher":"arXiv","source":"arXiv.org","title":"Hand-Object Interaction Controller (HOIC): Deep Reinforcement Learning for Reconstructing Interactions with Physics","title-short":"Hand-Object Interaction Controller (HOIC)","type":"article","URL":"http://arxiv.org/abs/2405.02676"},
  {"id":"iqbalHandPoseEstimation2018","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Iqbal","given":"Umar"},{"family":"Molchanov","given":"Pavlo"},{"family":"Gall","given":"Thomas Breuel Juergen"},{"family":"Kautz","given":"Jan"}],"citation-key":"iqbalHandPoseEstimation2018","event-title":"Proceedings of the European Conference on Computer Vision (ECCV)","issued":{"date-parts":[["2018"]]},"page":"118-134","source":"openaccess.thecvf.com","title":"Hand Pose Estimation via Latent 2.5D Heatmap Regression","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ECCV_2018/html/Umar_Iqbal_Hand_Pose_Estimation_ECCV_2018_paper.html"},
  {"id":"iwaseRelightableHandsEfficientNeural2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Iwase","given":"Shun"},{"family":"Saito","given":"Shunsuke"},{"family":"Simon","given":"Tomas"},{"family":"Lombardi","given":"Stephen"},{"family":"Bagautdinov","given":"Timur"},{"family":"Joshi","given":"Rohan"},{"family":"Prada","given":"Fabian"},{"family":"Shiratori","given":"Takaaki"},{"family":"Sheikh","given":"Yaser"},{"family":"Saragih","given":"Jason"}],"citation-key":"iwaseRelightableHandsEfficientNeural2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"16663-16673","source":"openaccess.thecvf.com","title":"RelightableHands: Efficient Neural Relighting of Articulated Hand Models","title-short":"RelightableHands","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Iwase_RelightableHands_Efficient_Neural_Relighting_of_Articulated_Hand_Models_CVPR_2023_paper.html"},
  {"id":"jianAffordPoseLargeScaleDataset2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Jian","given":"Juntao"},{"family":"Liu","given":"Xiuping"},{"family":"Li","given":"Manyi"},{"family":"Hu","given":"Ruizhen"},{"family":"Liu","given":"Jian"}],"citation-key":"jianAffordPoseLargeScaleDataset2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"14713-14724","source":"openaccess.thecvf.com","title":"AffordPose: A Large-Scale Dataset of Hand-Object Interactions with Affordance-Driven Hand Pose","title-short":"AffordPose","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Jian_AffordPose_A_Large-Scale_Dataset_of_Hand-Object_Interactions_with_Affordance-Driven_Hand_ICCV_2023_paper.html"},
  {"id":"jiangA2JTransformerAnchorJointTransformer2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Jiang","given":"Changlong"},{"family":"Xiao","given":"Yang"},{"family":"Wu","given":"Cunlin"},{"family":"Zhang","given":"Mingyang"},{"family":"Zheng","given":"Jinghong"},{"family":"Cao","given":"Zhiguo"},{"family":"Zhou","given":"Joey Tianyi"}],"citation-key":"jiangA2JTransformerAnchorJointTransformer2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8846-8855","source":"openaccess.thecvf.com","title":"A2J-Transformer: Anchor-to-Joint Transformer Network for 3D Interacting Hand Pose Estimation From a Single RGB Image","title-short":"A2J-Transformer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_A2J-Transformer_Anchor-to-Joint_Transformer_Network_for_3D_Interacting_Hand_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"jiangComplementingEventStreams2024","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Jiang","given":"Jianping"},{"family":"Zhou","given":"Xinyu"},{"family":"Wang","given":"Bingxuan"},{"family":"Deng","given":"Xiaoming"},{"family":"Xu","given":"Chao"},{"family":"Shi","given":"Boxin"}],"citation-key":"jiangComplementingEventStreams2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"24944-24954","source":"openaccess.thecvf.com","title":"Complementing Event Streams and RGB Frames for Hand Mesh Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_Complementing_Event_Streams_and_RGB_Frames_for_Hand_Mesh_Reconstruction_CVPR_2024_paper.html"},
  {"id":"jiangProbabilisticAttentionModel2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Jiang","given":"Zheheng"},{"family":"Rahmani","given":"Hossein"},{"family":"Black","given":"Sue"},{"family":"Williams","given":"Bryan M."}],"citation-key":"jiangProbabilisticAttentionModel2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"758-767","source":"openaccess.thecvf.com","title":"A Probabilistic Attention Model With Occlusion-Aware Texture Regression for 3D Hand Reconstruction From a Single RGB Image","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Jiang_A_Probabilistic_Attention_Model_With_Occlusion-Aware_Texture_Regression_for_3D_CVPR_2023_paper.html"},
  {"id":"karunratanakulGuidedMotionDiffusion2023","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Karunratanakul","given":"Korrawe"},{"family":"Preechakul","given":"Konpat"},{"family":"Suwajanakorn","given":"Supasorn"},{"family":"Tang","given":"Siyu"}],"citation-key":"karunratanakulGuidedMotionDiffusion2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"2151-2162","source":"openaccess.thecvf.com","title":"Guided Motion Diffusion for Controllable Human Motion Synthesis","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Karunratanakul_Guided_Motion_Diffusion_for_Controllable_Human_Motion_Synthesis_ICCV_2023_paper.html"},
  {"id":"karunratanakulHARPPersonalizedHand2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Karunratanakul","given":"Korrawe"},{"family":"Prokudin","given":"Sergey"},{"family":"Hilliges","given":"Otmar"},{"family":"Tang","given":"Siyu"}],"citation-key":"karunratanakulHARPPersonalizedHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12802-12813","source":"openaccess.thecvf.com","title":"HARP: Personalized Hand Reconstruction From a Monocular RGB Video","title-short":"HARP","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Karunratanakul_HARP_Personalized_Hand_Reconstruction_From_a_Monocular_RGB_Video_CVPR_2023_paper.html"},
  {"id":"karunratanakulSkeletonDrivenNeuralOccupancy2021","abstract":"We present Hand ArticuLated Occupancy (HALO), a novel representation of articulated hands that bridges the advantages of 3D keypoints and neural implicit surfaces and can be used in end-to-end trainable architectures. Unlike existing statistical parametric hand models (e.g.~MANO), HALO directly leverages 3D joint skeleton as input and produces a neural occupancy volume representing the posed hand surface. The key benefits of HALO are (1) it is driven by 3D key points, which have benefits in terms of accuracy and are easier to learn for neural networks than the latent hand-model parameters; (2) it provides a differentiable volumetric occupancy representation of the posed hand; (3) it can be trained end-to-end, allowing the formulation of losses on the hand surface that benefit the learning of 3D keypoints. We demonstrate the applicability of HALO to the task of conditional generation of hands that grasp 3D objects. The differentiable nature of HALO is shown to improve the quality of the synthesized hands both in terms of physical plausibility and user preference.","accessed":{"date-parts":[["2024",10,25]]},"author":[{"family":"Karunratanakul","given":"Korrawe"},{"family":"Spurr","given":"Adrian"},{"family":"Fan","given":"Zicong"},{"family":"Hilliges","given":"Otmar"},{"family":"Tang","given":"Siyu"}],"citation-key":"karunratanakulSkeletonDrivenNeuralOccupancy2021","issued":{"date-parts":[["2021",9,23]]},"number":"arXiv:2109.11399","publisher":"arXiv","source":"arXiv.org","title":"A Skeleton-Driven Neural Occupancy Representation for Articulated Hands","type":"article","URL":"http://arxiv.org/abs/2109.11399"},
  {"id":"khirodkarSapiensFoundationHuman2024","abstract":"We present Sapiens, a family of models for four fundamental human-centric vision tasks -- 2D pose estimation, body-part segmentation, depth estimation, and surface normal prediction. Our models natively support 1K high-resolution inference and are extremely easy to adapt for individual tasks by simply fine-tuning models pretrained on over 300 million in-the-wild human images. We observe that, given the same computational budget, self-supervised pretraining on a curated dataset of human images significantly boosts the performance for a diverse set of human-centric tasks. The resulting models exhibit remarkable generalization to in-the-wild data, even when labeled data is scarce or entirely synthetic. Our simple model design also brings scalability -- model performance across tasks improves as we scale the number of parameters from 0.3 to 2 billion. Sapiens consistently surpasses existing baselines across various human-centric benchmarks. We achieve significant improvements over the prior state-of-the-art on Humans-5K (pose) by 7.6 mAP, Humans-2K (part-seg) by 17.1 mIoU, Hi4D (depth) by 22.4% relative RMSE, and THuman2 (normal) by 53.5% relative angular error. Project page: https://about.meta.com/realitylabs/codecavatars/sapiens.","accessed":{"date-parts":[["2024",10,8]]},"author":[{"family":"Khirodkar","given":"Rawal"},{"family":"Bagautdinov","given":"Timur"},{"family":"Martinez","given":"Julieta"},{"family":"Zhaoen","given":"Su"},{"family":"James","given":"Austin"},{"family":"Selednik","given":"Peter"},{"family":"Anderson","given":"Stuart"},{"family":"Saito","given":"Shunsuke"}],"citation-key":"khirodkarSapiensFoundationHuman2024","DOI":"10.48550/arXiv.2408.12569","issued":{"date-parts":[["2024",8,26]]},"number":"arXiv:2408.12569","publisher":"arXiv","source":"arXiv.org","title":"Sapiens: Foundation for Human Vision Models","title-short":"Sapiens","type":"article","URL":"http://arxiv.org/abs/2408.12569"},
  {"id":"kulonWeaklySupervisedMeshConvolutionalHand2020","accessed":{"date-parts":[["2024",10,15]]},"author":[{"family":"Kulon","given":"Dominik"},{"family":"Guler","given":"Riza Alp"},{"family":"Kokkinos","given":"Iasonas"},{"family":"Bronstein","given":"Michael M."},{"family":"Zafeiriou","given":"Stefanos"}],"citation-key":"kulonWeaklySupervisedMeshConvolutionalHand2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"4990-5000","source":"openaccess.thecvf.com","title":"Weakly-Supervised Mesh-Convolutional Hand Reconstruction in the Wild","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Kulon_Weakly-Supervised_Mesh-Convolutional_Hand_Reconstruction_in_the_Wild_CVPR_2020_paper.html"},
  {"id":"leeIm2HandsLearningAttentive2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Lee","given":"Jihyun"},{"family":"Sung","given":"Minhyuk"},{"family":"Choi","given":"Honggyu"},{"family":"Kim","given":"Tae-Kyun"}],"citation-key":"leeIm2HandsLearningAttentive2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"21169-21178","source":"openaccess.thecvf.com","title":"Im2Hands: Learning Attentive Implicit Representation of Interacting Two-Hand Shapes","title-short":"Im2Hands","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Lee_Im2Hands_Learning_Attentive_Implicit_Representation_of_Interacting_Two-Hand_Shapes_CVPR_2023_paper.html"},
  {"id":"leeInterHandGenTwoHandInteraction2024","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Lee","given":"Jihyun"},{"family":"Saito","given":"Shunsuke"},{"family":"Nam","given":"Giljoo"},{"family":"Sung","given":"Minhyuk"},{"family":"Kim","given":"Tae-Kyun"}],"citation-key":"leeInterHandGenTwoHandInteraction2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"527-537","source":"openaccess.thecvf.com","title":"InterHandGen: Two-Hand Interaction Generation via Cascaded Reverse Diffusion","title-short":"InterHandGen","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Lee_InterHandGen_Two-Hand_Interaction_Generation_via_Cascaded_Reverse_Diffusion_CVPR_2024_paper.html"},
  {"id":"lengDynamicHyperbolicAttention2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Leng","given":"Zhiying"},{"family":"Wu","given":"Shun-Cheng"},{"family":"Saleh","given":"Mahdi"},{"family":"Montanaro","given":"Antonio"},{"family":"Yu","given":"Hao"},{"family":"Wang","given":"Yin"},{"family":"Navab","given":"Nassir"},{"family":"Liang","given":"Xiaohui"},{"family":"Tombari","given":"Federico"}],"citation-key":"lengDynamicHyperbolicAttention2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"14894-14904","source":"openaccess.thecvf.com","title":"Dynamic Hyperbolic Attention Network for Fine Hand-object Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Leng_Dynamic_Hyperbolic_Attention_Network_for_Fine_Hand-object_Reconstruction_ICCV_2023_paper.html"},
  {"id":"leonardiAreSyntheticData","abstract":"In this study, we investigate the effectiveness of synthetic data in enhancing egocentric hand-object interaction detection. Via extensive experiments and comparative analyses on three egocentric datasets, VISOR, EgoHOS, and ENIGMA-51, our findings reveal how to exploit synthetic data for the HOI detection task when real labeled data are scarce or unavailable. Specifically, by leveraging only 10% of real labeled data, we achieve improvements in Overall AP compared to baselines trained exclusively on real data of: +5.67% on EPIC-KITCHENS VISOR, +8.24% on EgoHOS, and +11.69% on ENIGMA-51. Our analysis is supported by a novel data generation pipeline and the newly introduced HOI-Synth benchmark which augments existing datasets with synthetic images of hand-object interactions automatically labeled with hand-object contact states, bounding boxes, and pixel-wise segmentation masks. Data, code, and data generation tools to support future research are released at: https://fpv-iplab.github.io/HOI-Synth/.","author":[{"family":"Leonardi","given":"Rosario"},{"family":"Furnari","given":"Antonino"},{"family":"Ragusa","given":"Francesco"},{"family":"Farinella","given":"Giovanni Maria"}],"citation-key":"leonardiAreSyntheticData","language":"en","source":"Zotero","title":"Are Synthetic Data Useful for Egocentric Hand-Object Interaction Detection?","type":"article-journal"},
  {"id":"liCHORDCategorylevelHandheld2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Li","given":"Kailin"},{"family":"Yang","given":"Lixin"},{"family":"Zhen","given":"Haoyu"},{"family":"Lin","given":"Zenan"},{"family":"Zhan","given":"Xinyu"},{"family":"Zhong","given":"Licheng"},{"family":"Xu","given":"Jian"},{"family":"Wu","given":"Kejian"},{"family":"Lu","given":"Cewu"}],"citation-key":"liCHORDCategorylevelHandheld2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"9444-9454","source":"openaccess.thecvf.com","title":"CHORD: Category-level Hand-held Object Reconstruction via Shape Deformation","title-short":"CHORD","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Li_CHORD_Category-level_Hand-held_Object_Reconstruction_via_Shape_Deformation_ICCV_2023_paper.html"},
  {"id":"liHHMRHolisticHand2024","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Li","given":"Mengcheng"},{"family":"Zhang","given":"Hongwen"},{"family":"Zhang","given":"Yuxiang"},{"family":"Shao","given":"Ruizhi"},{"family":"Yu","given":"Tao"},{"family":"Liu","given":"Yebin"}],"citation-key":"liHHMRHolisticHand2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"645-654","source":"openaccess.thecvf.com","title":"HHMR: Holistic Hand Mesh Recovery by Enhancing the Multimodal Controllability of Graph Diffusion Models","title-short":"HHMR","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Li_HHMR_Holistic_Hand_Mesh_Recovery_by_Enhancing_the_Multimodal_Controllability_CVPR_2024_paper.html"},
  {"id":"linCrossDomain3DHand2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Lin","given":"Qiuxia"},{"family":"Yang","given":"Linlin"},{"family":"Yao","given":"Angela"}],"citation-key":"linCrossDomain3DHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"17184-17193","source":"openaccess.thecvf.com","title":"Cross-Domain 3D Hand Pose Estimation With Dual Modalities","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Cross-Domain_3D_Hand_Pose_Estimation_With_Dual_Modalities_CVPR_2023_paper.html"},
  {"id":"linHarmoniousFeatureLearning2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Lin","given":"Zhifeng"},{"family":"Ding","given":"Changxing"},{"family":"Yao","given":"Huan"},{"family":"Kuang","given":"Zengsheng"},{"family":"Huang","given":"Shaoli"}],"citation-key":"linHarmoniousFeatureLearning2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12989-12998","source":"openaccess.thecvf.com","title":"Harmonious Feature Learning for Interactive Hand-Object Pose Estimation","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Lin_Harmonious_Feature_Learning_for_Interactive_Hand-Object_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"linMeshGraphormer2021","accessed":{"date-parts":[["2024",9,30]]},"author":[{"family":"Lin","given":"Kevin"},{"family":"Wang","given":"Lijuan"},{"family":"Liu","given":"Zicheng"}],"citation-key":"linMeshGraphormer2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"12939-12948","source":"openaccess.thecvf.com","title":"Mesh Graphormer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Lin_Mesh_Graphormer_ICCV_2021_paper.html"},
  {"id":"liRenderIHLargeScaleSynthetic2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Li","given":"Lijun"},{"family":"Tian","given":"Linrui"},{"family":"Zhang","given":"Xindi"},{"family":"Wang","given":"Qi"},{"family":"Zhang","given":"Bang"},{"family":"Bo","given":"Liefeng"},{"family":"Liu","given":"Mengyuan"},{"family":"Chen","given":"Chen"}],"citation-key":"liRenderIHLargeScaleSynthetic2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"20395-20405","source":"openaccess.thecvf.com","title":"RenderIH: A Large-Scale Synthetic Dataset for 3D Interacting Hand Pose Estimation","title-short":"RenderIH","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Li_RenderIH_A_Large-Scale_Synthetic_Dataset_for_3D_Interacting_Hand_Pose_ICCV_2023_paper.html"},
  {"id":"liuCoarseFineImplicitRepresentation","abstract":"Recent research has explored implicit representations, such as signed distance function (SDF), for interacting hand-object reconstruction. SDF enables modeling hand-held objects with arbitrary topology and overcomes the resolution limitations of parametric models, allowing for finer-grained reconstruction. However, directly modeling detailed SDFs from visual clues presents challenges due to depth ambiguity and appearance similarity, especially in cluttered real-world scenes. In this paper, we propose a coarse-to-fine SDF framework for 3D hand-object reconstruction, which leverages the perceptual advantages of RGB-D modality in visual and geometric aspects, to progressively model the implicit field. Specifically, we model a coarse SDF for visual perception of overall scenes. Then, we propose a 3D Point-Aligned Implicit Function (3D PIFu) for fine-level SDF learning, which leverages both local geometric clues and the coarse-level visual priors to capture intricate details. Additionally, we propose a surface-aware efficient reconstruction strategy that sparsely performs SDF query based on the hand-object semantic priors. Experiments on two challenging hand-object datasets show that our method outperforms existing methods by a large margin.","author":[{"family":"Liu","given":"Xingyu"},{"family":"Ren","given":"Pengfei"},{"family":"Wang","given":"Jingyu"},{"family":"Qi","given":"Qi"},{"family":"Sun","given":"Haifeng"},{"family":"Liao","given":"Jianxin"}],"citation-key":"liuCoarseFineImplicitRepresentation","language":"en","source":"Zotero","title":"Coarse-to-Fine Implicit Representation Learning for 3D Hand-Object Reconstruction from a Single RGB-D Image","type":"article-journal"},
  {"id":"liuSingleDualViewAdaptationEgocentric2024","abstract":"The pursuit of accurate 3D hand pose estimation stands as a keystone for understanding human activity in the realm of egocentric vision. The majority of existing estimation methods still rely on single-view images as input, leading to potential limitations, e.g., limited field-of-view and ambiguity in depth. To address these problems, adding another camera to better capture the shape of hands is a practical direction. However, existing multi-view hand pose estimation methods suffer from two main drawbacks: 1) Requiring multi-view annotations for training, which are expensive. 2) During testing, the model becomes inapplicable if camera parameters/layout are not the same as those used in training. In this paper, we propose a novel Singleto-Dual-view adaptation (S2DHand) solution that adapts a pre-trained single-view estimator to dual views. Compared with existing multi-view training methods, 1) our adaptation process is unsupervised, eliminating the need for multiview annotation. 2) Moreover, our method can handle arbitrary dual-view pairs with unknown camera parameters, making the model applicable to diverse camera settings. Specifically, S2DHand is built on certain stereo constraints, including pair-wise cross-view consensus and invariance of transformation between both views. These two stereo constraints are used in a complementary manner to generate pseudo-labels, allowing reliable adaptation. Evaluation results reveal that S2DHand achieves significant improvements on arbitrary camera pairs under both in-dataset and cross-dataset settings, and outperforms existing adaptation methods with leading performance. Project page: https://github.com/ut-vision/S2DHand.","accessed":{"date-parts":[["2024",10,8]]},"author":[{"family":"Liu","given":"Ruicong"},{"family":"Ohkawa","given":"Takehiko"},{"family":"Zhang","given":"Mingfang"},{"family":"Sato","given":"Yoichi"}],"citation-key":"liuSingleDualViewAdaptationEgocentric2024","container-title":"2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","DOI":"10.1109/CVPR52733.2024.00071","event-place":"Seattle, WA, USA","event-title":"2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)","ISBN":"9798350353006","issued":{"date-parts":[["2024",6,16]]},"language":"en","license":"https://doi.org/10.15223/policy-029","page":"677-686","publisher":"IEEE","publisher-place":"Seattle, WA, USA","source":"DOI.org (Crossref)","title":"Single-to-Dual-View Adaptation for Egocentric 3D Hand Pose Estimation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10656245/"},
  {"id":"liuVMambaVisualState2024","abstract":"Designing computationally efficient network architectures persists as an ongoing necessity in computer vision. In this paper, we transplant Mamba, a state-space language model, into VMamba, a vision backbone that works in linear time complexity. At the core of VMamba lies a stack of Visual State-Space (VSS) blocks with the 2D Selective Scan (SS2D) module. By traversing along four scanning routes, SS2D helps bridge the gap between the ordered nature of 1D selective scan and the non-sequential structure of 2D vision data, which facilitates the gathering of contextual information from various sources and perspectives. Based on the VSS blocks, we develop a family of VMamba architectures and accelerate them through a succession of architectural and implementation enhancements. Extensive experiments showcase VMamba's promising performance across diverse visual perception tasks, highlighting its advantages in input scaling efficiency compared to existing benchmark models. Source code is available at https://github.com/MzeroMiko/VMamba.","accessed":{"date-parts":[["2024",10,17]]},"author":[{"family":"Liu","given":"Yue"},{"family":"Tian","given":"Yunjie"},{"family":"Zhao","given":"Yuzhong"},{"family":"Yu","given":"Hongtian"},{"family":"Xie","given":"Lingxi"},{"family":"Wang","given":"Yaowei"},{"family":"Ye","given":"Qixiang"},{"family":"Liu","given":"Yunfan"}],"citation-key":"liuVMambaVisualState2024","issued":{"date-parts":[["2024",5,26]]},"number":"arXiv:2401.10166","publisher":"arXiv","source":"arXiv.org","title":"VMamba: Visual State Space Model","title-short":"VMamba","type":"article","URL":"http://arxiv.org/abs/2401.10166"},
  {"id":"luanHighFidelity3D2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Luan","given":"Tianyu"},{"family":"Zhai","given":"Yuanhao"},{"family":"Meng","given":"Jingjing"},{"family":"Li","given":"Zhong"},{"family":"Chen","given":"Zhang"},{"family":"Xu","given":"Yi"},{"family":"Yuan","given":"Junsong"}],"citation-key":"luanHighFidelity3D2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"16795-16804","source":"openaccess.thecvf.com","title":"High Fidelity 3D Hand Shape Reconstruction via Scalable Graph Frequency Decomposition","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Luan_High_Fidelity_3D_Hand_Shape_Reconstruction_via_Scalable_Graph_Frequency_CVPR_2023_paper.html"},
  {"id":"luoPhysicsawareHandobjectInteraction2024","abstract":"The credibility and practicality of a reconstructed hand-object interaction sequence depend largely on its physical plausibility. However, due to high occlusions during hand-object interaction, physical plausibility remains a challenging criterion for purely vision-based tracking methods. To address this issue and enhance the results of existing hand trackers, this paper proposes a novel physically-aware hand motion de-noising method. Specifically, we introduce two learned loss terms that explicitly capture two crucial aspects of physical plausibility: grasp credibility and manipulation feasibility. These terms are used to train a physically-aware de-noising network. Qualitative and quantitative experiments demonstrate that our approach significantly improves both fine-grained physical plausibility and overall pose accuracy, surpassing current state-of-the-art de-noising methods.","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Luo","given":"Haowen"},{"family":"Liu","given":"Yunze"},{"family":"Yi","given":"Li"}],"citation-key":"luoPhysicsawareHandobjectInteraction2024","DOI":"10.48550/arXiv.2405.11481","issued":{"date-parts":[["2024",5,19]]},"number":"arXiv:2405.11481","publisher":"arXiv","source":"arXiv.org","title":"Physics-aware Hand-object Interaction Denoising","type":"article","URL":"http://arxiv.org/abs/2405.11481"},
  {"id":"ManipNetNeuralManipulation","abstract":"In this paper, we propose a hand-object spatial representation that can achieve generalization from limited data. Our representation combines the global object shape as voxel occupancies with local geometric details as samples of closest distances. This representation is used by a neural network to regress finger motions from input trajectories of wrists and objects.","accessed":{"date-parts":[["2024",10,9]]},"citation-key":"ManipNetNeuralManipulation","container-title":"Meta Research","language":"en","title":"ManipNet: Neural Manipulation Synthesis with a Hand-Object Spatial Representation - Meta Research","title-short":"ManipNet","type":"webpage","URL":"https://research.facebook.com/publications/manipnet-neural-manipulation-synthesis-with-a-hand-object-spatial-representation/"},
  {"id":"mildenhallNeRFRepresentingScenes2020","abstract":"We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location $(x,y,z)$ and viewing direction $(\\theta, \\phi)$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.","accessed":{"date-parts":[["2024",10,4]]},"author":[{"family":"Mildenhall","given":"Ben"},{"family":"Srinivasan","given":"Pratul P."},{"family":"Tancik","given":"Matthew"},{"family":"Barron","given":"Jonathan T."},{"family":"Ramamoorthi","given":"Ravi"},{"family":"Ng","given":"Ren"}],"citation-key":"mildenhallNeRFRepresentingScenes2020","container-title":"arXiv.org","issued":{"date-parts":[["2020",3,19]]},"language":"en","title":"NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis","title-short":"NeRF","type":"webpage","URL":"https://arxiv.org/abs/2003.08934v2"},
  {"id":"mokhtarHandMeThis2024","abstract":"\"What you see is what you get\" is a concept in eye gaze interaction of selecting only with your eye. Technologies such as Virtual and Augmented Reality (VR/AR) often exclude people with limited mobility due to their dependence on controllers or hand movement. Meanwhile, embodiment focuses on a one-to-one mapping of the body &amp; hands. However, previous work did not connect embodiment and accessibility with the aid of eye-gaze interaction. In this project, we explore how different gaze-controlled hand representations and animations affect users’ reported presence and embodiment that current technology lacks and can be crucially important, in virtual environments. Our data showed significant differences in presence and self-location(embodiment), especially when comparing Dwell and Snap animations.","accessed":{"date-parts":[["2024",10,10]]},"author":[{"family":"Mokhtar","given":"Noha"},{"family":"Esteves","given":"Augusto"}],"citation-key":"mokhtarHandMeThis2024","collection-title":"ETRA '24","container-title":"Proceedings of the 2024 Symposium on Eye Tracking Research and Applications","DOI":"10.1145/3649902.3656362","event-place":"New York, NY, USA","ISBN":"9798400706073","issued":{"date-parts":[["2024",6,4]]},"page":"1–7","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Hand Me This: Exploring the Effects of Gaze-driven Animations and Hand Representations in Users’ Sense of Presence and Embodiment","title-short":"Hand Me This","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3649902.3656362"},
  {"id":"moonDatasetRelighted3D2023a","abstract":"The two-hand interaction is one of the most challenging signals to analyze due to the self-similarity, complicated articulations, and occlusions of hands. Although several datasets have been proposed for the two-hand interaction analysis, all of them do not achieve 1) diverse and realistic image appearances and 2) diverse and large-scale groundtruth (GT) 3D poses at the same time. In this work, we propose Re:InterHand, a dataset of relighted 3D interacting hands that achieve the two goals. To this end, we employ a state-of-the-art hand relighting network with our accurately tracked two-hand 3D poses. We compare our Re:InterHand with existing 3D interacting hands datasets and show the benefit of it. Our Re:InterHand is available in https://mks0601.github.io/ReInterHand/.","accessed":{"date-parts":[["2024",10,30]]},"author":[{"family":"Moon","given":"Gyeongsik"},{"family":"Saito","given":"Shunsuke"},{"family":"Xu","given":"Weipeng"},{"family":"Joshi","given":"Rohan"},{"family":"Buffalini","given":"Julia"},{"family":"Bellan","given":"Harley"},{"family":"Rosen","given":"Nicholas"},{"family":"Richardson","given":"Jesse"},{"family":"Mize","given":"Mallorie"},{"family":"Bree","given":"Philippe","dropping-particle":"de"},{"family":"Simon","given":"Tomas"},{"family":"Peng","given":"Bo"},{"family":"Garg","given":"Shubham"},{"family":"McPhail","given":"Kevyn"},{"family":"Shiratori","given":"Takaaki"}],"citation-key":"moonDatasetRelighted3D2023a","DOI":"10.48550/arXiv.2310.17768","issued":{"date-parts":[["2023",10,26]]},"number":"arXiv:2310.17768","publisher":"arXiv","source":"arXiv.org","title":"A Dataset of Relighted 3D Interacting Hands","type":"article","URL":"http://arxiv.org/abs/2310.17768"},
  {"id":"moonInterHand26MDatasetBaseline2020","abstract":"Analysis of hand-hand interactions is a crucial step towards better understanding human behavior. However, most researches in 3D hand pose estimation have focused on the isolated single hand case. Therefore, we firstly propose (1) a large-scale dataset, InterHand2.6M, and (2) a baseline network, InterNet, for 3D interacting hand pose estimation from a single RGB image. The proposed InterHand2.6M consists of \\textbf{2.6M labeled single and interacting hand frames} under various poses from multiple subjects. Our InterNet simultaneously performs 3D single and interacting hand pose estimation. In our experiments, we demonstrate big gains in 3D interacting hand pose estimation accuracy when leveraging the interacting hand data in InterHand2.6M. We also report the accuracy of InterNet on InterHand2.6M, which serves as a strong baseline for this new dataset. Finally, we show 3D interacting hand pose estimation results from general images. Our code and dataset are available at https://mks0601.github.io/InterHand2.6M/.","accessed":{"date-parts":[["2024",10,30]]},"author":[{"family":"Moon","given":"Gyeongsik"},{"family":"Yu","given":"Shoou-i"},{"family":"Wen","given":"He"},{"family":"Shiratori","given":"Takaaki"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"moonInterHand26MDatasetBaseline2020","DOI":"10.48550/arXiv.2008.09309","issued":{"date-parts":[["2020",8,21]]},"number":"arXiv:2008.09309","publisher":"arXiv","source":"arXiv.org","title":"InterHand2.6M: A Dataset and Baseline for 3D Interacting Hand Pose Estimation from a Single RGB Image","title-short":"InterHand2.6M","type":"article","URL":"http://arxiv.org/abs/2008.09309"},
  {"id":"mundraLiveHandRealtimePhotorealistic2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Mundra","given":"Akshay"},{"family":"R","given":"Mallikarjun B."},{"family":"Wang","given":"Jiayi"},{"family":"Habermann","given":"Marc"},{"family":"Theobalt","given":"Christian"},{"family":"Elgharib","given":"Mohamed"}],"citation-key":"mundraLiveHandRealtimePhotorealistic2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"18035-18045","source":"openaccess.thecvf.com","title":"LiveHand: Real-time and Photorealistic Neural Hand Rendering","title-short":"LiveHand","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Mundra_LiveHand_Real-time_and_Photorealistic_Neural_Hand_Rendering_ICCV_2023_paper.html"},
  {"id":"nguyenS4NDModelingImages2022","abstract":"Visual data such as images and videos are typically modeled as discretizations of inherently continuous, multidimensional signals. Existing continuous-signal models attempt to exploit this fact by modeling the underlying signals of visual (e.g., image) data directly. However, these models have not yet been able to achieve competitive performance on practical vision tasks such as large-scale image and video classification. Building on a recent line of work on deep state space models (SSMs), we propose S4ND, a new multidimensional SSM layer that extends the continuous-signal modeling ability of SSMs to multidimensional data including images and videos. We show that S4ND can model large-scale visual data in $1$D, $2$D, and $3$D as continuous multidimensional signals and demonstrates strong performance by simply swapping Conv2D and self-attention layers with S4ND layers in existing state-of-the-art models. On ImageNet-1k, S4ND exceeds the performance of a Vision Transformer baseline by $1.5\\%$ when training with a $1$D sequence of patches, and matches ConvNeXt when modeling images in $2$D. For videos, S4ND improves on an inflated $3$D ConvNeXt in activity classification on HMDB-51 by $4\\%$. S4ND implicitly learns global, continuous convolutional kernels that are resolution invariant by construction, providing an inductive bias that enables generalization across multiple resolutions. By developing a simple bandlimiting modification to S4 to overcome aliasing, S4ND achieves strong zero-shot (unseen at training time) resolution performance, outperforming a baseline Conv2D by $40\\%$ on CIFAR-10 when trained on $8 \\times 8$ and tested on $32 \\times 32$ images. When trained with progressive resizing, S4ND comes within $\\sim 1\\%$ of a high-resolution model while training $22\\%$ faster.","accessed":{"date-parts":[["2024",10,23]]},"author":[{"family":"Nguyen","given":"Eric"},{"family":"Goel","given":"Karan"},{"family":"Gu","given":"Albert"},{"family":"Downs","given":"Gordon W."},{"family":"Shah","given":"Preey"},{"family":"Dao","given":"Tri"},{"family":"Baccus","given":"Stephen A."},{"family":"Ré","given":"Christopher"}],"citation-key":"nguyenS4NDModelingImages2022","DOI":"10.48550/arXiv.2210.06583","issued":{"date-parts":[["2022",10,14]]},"number":"arXiv:2210.06583","publisher":"arXiv","source":"arXiv.org","title":"S4ND: Modeling Images and Videos as Multidimensional Signals Using State Spaces","title-short":"S4ND","type":"article","URL":"http://arxiv.org/abs/2210.06583"},
  {"id":"ohkawaAssemblyHandsEgocentricActivity2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Ohkawa","given":"Takehiko"},{"family":"He","given":"Kun"},{"family":"Sener","given":"Fadime"},{"family":"Hodan","given":"Tomas"},{"family":"Tran","given":"Luan"},{"family":"Keskin","given":"Cem"}],"citation-key":"ohkawaAssemblyHandsEgocentricActivity2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12999-13008","source":"openaccess.thecvf.com","title":"AssemblyHands: Towards Egocentric Activity Understanding via 3D Hand Pose Estimation","title-short":"AssemblyHands","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Ohkawa_AssemblyHands_Towards_Egocentric_Activity_Understanding_via_3D_Hand_Pose_Estimation_CVPR_2023_paper.html"},
  {"id":"ohRecovering3DHand2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Oh","given":"Yeonguk"},{"family":"Park","given":"JoonKyu"},{"family":"Kim","given":"Jaeha"},{"family":"Moon","given":"Gyeongsik"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"ohRecovering3DHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"554-563","source":"openaccess.thecvf.com","title":"Recovering 3D Hand Mesh Sequence From a Single Blurry Image: A New Dataset and Temporal Unfolding","title-short":"Recovering 3D Hand Mesh Sequence From a Single Blurry Image","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Oh_Recovering_3D_Hand_Mesh_Sequence_From_a_Single_Blurry_Image_CVPR_2023_paper.html"},
  {"id":"pageBreakingLowSample2024","abstract":"As spatial computers move towards glasses, where constraints on size, weight and power are critical, a shift in how we use sensors is needed. Ultraleap has developed the first hand tracking pipeline using event cameras on an AR headset. Event cameras allow to break the frame-imaging paradigm of \"low power equates to lo sample rate\" and enhance user experience while maintaining low power budget.","accessed":{"date-parts":[["2024",10,10]]},"author":[{"family":"Page","given":"Ryan"},{"family":"Baesso","given":"Paolo"},{"family":"Clark","given":"Rory"},{"family":"Baker","given":"Greg"}],"citation-key":"pageBreakingLowSample2024","collection-title":"SIGGRAPH '24","container-title":"ACM SIGGRAPH 2024 Emerging Technologies","DOI":"10.1145/3641517.3664389","event-place":"New York, NY, USA","ISBN":"9798400705243","issued":{"date-parts":[["2024",7,13]]},"page":"1–2","publisher":"Association for Computing Machinery","publisher-place":"New York, NY, USA","source":"ACM Digital Library","title":"Breaking the low sample rate equals low power paradigm using an event based vision approach for hand tracking in AR","type":"paper-conference","URL":"https://dl.acm.org/doi/10.1145/3641517.3664389"},
  {"id":"park3DHandSequence","abstract":"Although hands frequently exhibit motion blur due to their dynamic nature, existing approaches for 3D hand recovery often disregard the impact of motion blur in hand images. Blurry hand images contain hands from multiple time steps, lack precise hand location at a specific time step, and introduce temporal ambiguity, leading to multiple possible hand trajectories. To address this issue and in the absence of datasets with real blur, we introduce the EBH dataset, which provides 1) hand images with real motion blur and 2) event data for authentic representation of fast hand movements. In conjunction with our new dataset, we present EBHNet, a novel network capable of recovering 3D hands from diverse input combinations, including blurry hand images, events, or both. Here, the event stream enhances motion understanding in blurry hands, addressing temporal ambiguity. Recognizing that blurry hand images include not only single 3D hands at a time step but also multiple hands along their motion trajectories, we design EBHNet to generate 3D hand sequences in motion. Moreover, to enable our EBHNet to predict 3D hands at novel, unsupervised time steps using a single shared module, we employ a Transformer-based module, temporal splitter, into EBHNet. Our experiments show the superior performance of EBH and EBHNet, especially in handling blurry hand images, making them valuable in real-world applications.","author":[{"family":"Park","given":"Joonkyu"},{"family":"Moon","given":"Gyeongsik"},{"family":"Xu","given":"Weipeng"},{"family":"Kaseman","given":"Evan"},{"family":"Shiratori","given":"Takaaki"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"park3DHandSequence","language":"en","source":"Zotero","title":"3D Hand Sequence Recovery from Real Blurry Images and Event Stream","type":"article-journal"},
  {"id":"parkAttentionHandTextdrivenControllable","abstract":"Recently, there has been a significant amount of research conducted on 3D hand reconstruction to use various forms of humancomputer interaction. However, 3D hand reconstruction in the wild is challenging due to extreme lack of in-the-wild 3D hand datasets. Especially, when hands are in complex pose such as interacting hands, the problems like appearance similarity, self-handed occclusion and depth ambiguity make it more difficult. To overcome these issues, we propose AttentionHand, a novel method for text-driven controllable hand image generation. Since AttentionHand can generate various and numerous inthe-wild hand images well-aligned with 3D hand label, we can acquire a new 3D hand dataset, and can relieve the domain gap between indoor and outdoor scenes. Our method needs easy-to-use four modalities (i.e, an RGB image, a hand mesh image from 3D label, a bounding box, and a text prompt). These modalities are embedded into the latent space by the encoding phase. Then, through the text attention stage, hand-related tokens from the given text prompt are attended to highlight hand-related regions of the latent embedding. After the highlighted embedding is fed to the visual attention stage, hand-related regions in the embedding are attended by conditioning global and local hand mesh images with the diffusion-based pipeline. In the decoding phase, the final feature is decoded to new hand images, which are well-aligned with the given hand mesh image and text prompt. As a result, AttentionHand achieved state-of-the-art among text-to-hand image generation models, and the performance of 3D hand mesh reconstruction was improved by additionally training with hand images generated by AttentionHand.","author":[{"family":"Park","given":"Junho"},{"family":"Kong","given":"Kyeongbo"},{"family":"Kang","given":"Suk-Ju"}],"citation-key":"parkAttentionHandTextdrivenControllable","language":"en","source":"Zotero","title":"AttentionHand: Text-driven Controllable Hand Image Generation for 3D Hand Reconstruction in the Wild","type":"article-journal"},
  {"id":"parkHandOccNetOcclusionRobust3D2022","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Park","given":"JoonKyu"},{"family":"Oh","given":"Yeonguk"},{"family":"Moon","given":"Gyeongsik"},{"family":"Choi","given":"Hongsuk"},{"family":"Lee","given":"Kyoung Mu"}],"citation-key":"parkHandOccNetOcclusionRobust3D2022","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2022"]]},"language":"en","page":"1496-1505","source":"openaccess.thecvf.com","title":"HandOccNet: Occlusion-Robust 3D Hand Mesh Estimation Network","title-short":"HandOccNet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2022/html/Park_HandOccNet_Occlusion-Robust_3D_Hand_Mesh_Estimation_Network_CVPR_2022_paper.html"},
  {"id":"pavlakosReconstructingHands3D2024","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Pavlakos","given":"Georgios"},{"family":"Shan","given":"Dandan"},{"family":"Radosavovic","given":"Ilija"},{"family":"Kanazawa","given":"Angjoo"},{"family":"Fouhey","given":"David"},{"family":"Malik","given":"Jitendra"}],"citation-key":"pavlakosReconstructingHands3D2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"9826-9836","source":"openaccess.thecvf.com","title":"Reconstructing Hands in 3D with Transformers","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Pavlakos_Reconstructing_Hands_in_3D_with_Transformers_CVPR_2024_paper.html"},
  {"id":"pooleDreamFusionText3DUsing2022","abstract":"Recent breakthroughs in text-to-image synthesis have been driven by diffusion models trained on billions of image-text pairs. Adapting this approach to 3D synthesis would require large-scale datasets of labeled 3D data and efficient architectures for denoising 3D data, neither of which currently exist. In this work, we circumvent these limitations by using a pretrained 2D text-to-image diffusion model to perform text-to-3D synthesis. We introduce a loss based on probability density distillation that enables the use of a 2D diffusion model as a prior for optimization of a parametric image generator. Using this loss in a DeepDream-like procedure, we optimize a randomly-initialized 3D model (a Neural Radiance Field, or NeRF) via gradient descent such that its 2D renderings from random angles achieve a low loss. The resulting 3D model of the given text can be viewed from any angle, relit by arbitrary illumination, or composited into any 3D environment. Our approach requires no 3D training data and no modifications to the image diffusion model, demonstrating the effectiveness of pretrained image diffusion models as priors.","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Poole","given":"Ben"},{"family":"Jain","given":"Ajay"},{"family":"Barron","given":"Jonathan T."},{"family":"Mildenhall","given":"Ben"}],"citation-key":"pooleDreamFusionText3DUsing2022","DOI":"10.48550/arXiv.2209.14988","issued":{"date-parts":[["2022",9,29]]},"number":"arXiv:2209.14988","publisher":"arXiv","source":"arXiv.org","title":"DreamFusion: Text-to-3D using 2D Diffusion","title-short":"DreamFusion","type":"article","URL":"http://arxiv.org/abs/2209.14988"},
  {"id":"potamiasHandyHighFidelity2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Potamias","given":"Rolandos Alexandros"},{"family":"Ploumpis","given":"Stylianos"},{"family":"Moschoglou","given":"Stylianos"},{"family":"Triantafyllou","given":"Vasileios"},{"family":"Zafeiriou","given":"Stefanos"}],"citation-key":"potamiasHandyHighFidelity2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"4670-4680","source":"openaccess.thecvf.com","title":"Handy: Towards a High Fidelity 3D Hand Shape and Appearance Model","title-short":"Handy","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Potamias_Handy_Towards_a_High_Fidelity_3D_Hand_Shape_and_Appearance_CVPR_2023_paper.html"},
  {"id":"prakash3DHandPose","author":[{"family":"Prakash","given":"Aditya"},{"family":"Tu","given":"Ruisen"},{"family":"Chang","given":"Matthew"},{"family":"Gupta","given":"Saurabh"}],"citation-key":"prakash3DHandPose","language":"en","source":"Zotero","title":"3D Hand Pose Estimation in Everyday Egocentric Images","type":"article-journal"},
  {"id":"prakash3DReconstructionObjects2024","abstract":"Prior works for reconstructing hand-held objects from a single image train models on images paired with 3D shapes. Such data is challenging to gather in the real world at scale. Consequently, these approaches do not generalize well when presented with novel objects in in-the-wild settings. While 3D supervision is a major bottleneck, there is an abundance of a) in-the-wild raw video data showing hand-object interactions and b) synthetic 3D shape collections. In this paper, we propose modules to leverage 3D supervision from these sources to scale up the learning of models for reconstructing hand-held objects. Specifically, we extract multiview 2D mask supervision from videos and 3D shape priors from shape collections. We use these indirect 3D cues to train occupancy networks that predict the 3D shape of objects from a single RGB image. Our experiments in the challenging object generalization setting on in-the-wild MOW dataset show 11.6% relative improvement over models trained with 3D supervision on existing datasets.","accessed":{"date-parts":[["2024",10,11]]},"author":[{"family":"Prakash","given":"Aditya"},{"family":"Chang","given":"Matthew"},{"family":"Jin","given":"Matthew"},{"family":"Tu","given":"Ruisen"},{"family":"Gupta","given":"Saurabh"}],"citation-key":"prakash3DReconstructionObjects2024","issued":{"date-parts":[["2024",9,23]]},"number":"arXiv:2305.03036","publisher":"arXiv","source":"arXiv.org","title":"3D Reconstruction of Objects in Hands without Real World 3D Supervision","type":"article","URL":"http://arxiv.org/abs/2305.03036","version":"2"},
  {"id":"prakashMitigatingPerspectiveDistortioninduced2024","abstract":"Objects undergo varying amounts of perspective distortion as they move across a camera's field of view. Models for predicting 3D from a single image often work with crops around the object of interest and ignore the location of the object in the camera's field of view. We note that ignoring this location information further exaggerates the inherent ambiguity in making 3D inferences from 2D images and can prevent models from even fitting to the training data. To mitigate this ambiguity, we propose Intrinsics-Aware Positional Encoding (KPE), which incorporates information about the location of crops in the image and camera intrinsics. Experiments on three popular 3D-from-a-single-image benchmarks: depth prediction on NYU, 3D object detection on KITTI & nuScenes, and predicting 3D shapes of articulated objects on ARCTIC, show the benefits of KPE.","accessed":{"date-parts":[["2024",10,4]]},"author":[{"family":"Prakash","given":"Aditya"},{"family":"Gupta","given":"Arjun"},{"family":"Gupta","given":"Saurabh"}],"citation-key":"prakashMitigatingPerspectiveDistortioninduced2024","DOI":"10.48550/arXiv.2312.06594","issued":{"date-parts":[["2024",9,23]]},"number":"arXiv:2312.06594","publisher":"arXiv","source":"arXiv.org","title":"Mitigating Perspective Distortion-induced Shape Ambiguity in Image Crops","type":"article","URL":"http://arxiv.org/abs/2312.06594"},
  {"id":"qiDiverse3DHand2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Qi","given":"Xingqun"},{"family":"Liu","given":"Chen"},{"family":"Sun","given":"Muyi"},{"family":"Li","given":"Lincheng"},{"family":"Fan","given":"Changjie"},{"family":"Yu","given":"Xin"}],"citation-key":"qiDiverse3DHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"4616-4626","source":"openaccess.thecvf.com","title":"Diverse 3D Hand Gesture Prediction From Body Dynamics by Bilateral Hand Disentanglement","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Qi_Diverse_3D_Hand_Gesture_Prediction_From_Body_Dynamics_by_Bilateral_CVPR_2023_paper.html"},
  {"id":"qiHOISDFConstraining3D2024","abstract":"Human hands are highly articulated and versatile at handling objects. Jointly estimating the 3D poses of a hand and the object it manipulates from a monocular camera is challenging due to frequent occlusions. Thus, existing methods often rely on intermediate 3D shape representations to increase performance. These representations are typically explicit, such as 3D point clouds or meshes, and thus provide information in the direct surroundings of the intermediate hand pose estimate. To address this, we introduce HOISDF, a Signed Distance Field (SDF) guided hand-object pose estimation network, which jointly exploits hand and object SDFs to provide a global, implicit representation over the complete reconstruction volume. Specifically, the role of the SDFs is threefold: equip the visual encoder with implicit shape information, help to encode hand-object interactions, and guide the hand and object pose regression via SDF-based sampling and by augmenting the feature representations. We show that HOISDF achieves state-of-the-art results on hand-object pose estimation benchmarks (DexYCB and HO3Dv2). Code is available at https://github.com/amathislab/HOISDF","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Qi","given":"Haozhe"},{"family":"Zhao","given":"Chen"},{"family":"Salzmann","given":"Mathieu"},{"family":"Mathis","given":"Alexander"}],"citation-key":"qiHOISDFConstraining3D2024","DOI":"10.48550/arXiv.2402.17062","issued":{"date-parts":[["2024",2,26]]},"number":"arXiv:2402.17062","publisher":"arXiv","source":"arXiv.org","title":"HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields","title-short":"HOISDF","type":"article","URL":"http://arxiv.org/abs/2402.17062"},
  {"id":"qiHOISDFConstraining3D2024a","abstract":"Human hands are highly articulated and versatile at handling objects. Jointly estimating the 3D poses of a hand and the object it manipulates from a monocular camera is challenging due to frequent occlusions. Thus, existing methods often rely on intermediate 3D shape representations to increase performance. These representations are typically explicit, such as 3D point clouds or meshes, and thus provide information in the direct surroundings of the intermediate hand pose estimate. To address this, we introduce HOISDF, a Signed Distance Field (SDF) guided hand-object pose estimation network, which jointly exploits hand and object SDFs to provide a global, implicit representation over the complete reconstruction volume. Specifically, the role of the SDFs is threefold: equip the visual encoder with implicit shape information, help to encode hand-object interactions, and guide the hand and object pose regression via SDF-based sampling and by augmenting the feature representations. We show that HOISDF achieves state-of-the-art results on hand-object pose estimation benchmarks (DexYCB and HO3Dv2). Code is available at https://github.com/amathislab/HOISDF","accessed":{"date-parts":[["2024",7,23]]},"author":[{"family":"Qi","given":"Haozhe"},{"family":"Zhao","given":"Chen"},{"family":"Salzmann","given":"Mathieu"},{"family":"Mathis","given":"Alexander"}],"citation-key":"qiHOISDFConstraining3D2024a","DOI":"10.48550/arXiv.2402.17062","issued":{"date-parts":[["2024",2,26]]},"number":"arXiv:2402.17062","publisher":"arXiv","source":"arXiv.org","title":"HOISDF: Constraining 3D Hand-Object Pose Estimation with Global Signed Distance Fields","title-short":"HOISDF","type":"article","URL":"http://arxiv.org/abs/2402.17062"},
  {"id":"quNovelViewSynthesisPose2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Qu","given":"Wentian"},{"family":"Cui","given":"Zhaopeng"},{"family":"Zhang","given":"Yinda"},{"family":"Meng","given":"Chenyu"},{"family":"Ma","given":"Cuixia"},{"family":"Deng","given":"Xiaoming"},{"family":"Wang","given":"Hongan"}],"citation-key":"quNovelViewSynthesisPose2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"15100-15111","source":"openaccess.thecvf.com","title":"Novel-View Synthesis and Pose Estimation for Hand-Object Interaction from Sparse Views","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Qu_Novel-View_Synthesis_and_Pose_Estimation_for_Hand-Object_Interaction_from_Sparse_ICCV_2023_paper.html"},
  {"id":"regmiCrossViewImageSynthesis2018","accessed":{"date-parts":[["2024",10,7]]},"author":[{"family":"Regmi","given":"Krishna"},{"family":"Borji","given":"Ali"}],"citation-key":"regmiCrossViewImageSynthesis2018","event-title":"Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2018"]]},"page":"3501-3510","source":"openaccess.thecvf.com","title":"Cross-View Image Synthesis Using Conditional GANs","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_cvpr_2018/html/Regmi_Cross-View_Image_Synthesis_CVPR_2018_paper.html"},
  {"id":"romeroEmbodiedHandsModeling2017","abstract":"Humans move their hands and bodies together to communicate and solve tasks. Capturing and replicating such coordinated activity is critical for virtual characters that behave realistically. Surprisingly, most methods treat the 3D modeling and tracking of bodies and hands separately. Here we formulate a model of hands and bodies interacting together and fit it to full-body 4D sequences. When scanning or capturing the full body in 3D, hands are small and often partially occluded, making their shape and pose hard to recover. To cope with low-resolution, occlusion, and noise, we develop a new model called MANO (hand Model with Articulated and Non-rigid defOrmations). MANO is learned from around 1000 high-resolution 3D scans of hands of 31 subjects in a wide variety of hand poses. The model is realistic, low-dimensional, captures non-rigid shape changes with pose, is compatible with standard graphics packages, and can fit any human hand. MANO provides a compact mapping from hand poses to pose blend shape corrections and a linear manifold of pose synergies. We attach MANO to a standard parameterized 3D body shape model (SMPL), resulting in a fully articulated body and hand model (SMPL+H). We illustrate SMPL+H by fitting complex, natural, activities of subjects captured with a 4D scanner. The fitting is fully automatic and results in full body models that move naturally with detailed hand motions and a realism not seen before in full body performance capture. The models and data are freely available for research purposes in our website (http://mano.is.tue.mpg.de).","accessed":{"date-parts":[["2024",8,17]]},"author":[{"family":"Romero","given":"Javier"},{"family":"Tzionas","given":"Dimitrios"},{"family":"Black","given":"Michael J."}],"citation-key":"romeroEmbodiedHandsModeling2017","container-title":"ACM Transactions on Graphics","container-title-short":"ACM Trans. Graph.","DOI":"10.1145/3130800.3130883","ISSN":"0730-0301, 1557-7368","issue":"6","issued":{"date-parts":[["2017",12,31]]},"page":"1-17","source":"arXiv.org","title":"Embodied Hands: Modeling and Capturing Hands and Bodies Together","title-short":"Embodied Hands","type":"article-journal","URL":"http://arxiv.org/abs/2201.02610","volume":"36"},
  {"id":"sartoriHowObjectsAre2011","abstract":"Background Substantial literature has demonstrated that how the hand approaches an object depends on the manipulative action that will follow object contact. Little is known about how the placement of individual fingers on objects is affected by the end-goal of the action. Methodology/Principal Findings Hand movement kinematics were measured during reaching for and grasping movements towards two objects (stimuli): a bottle with an ordinary cylindrical shape and a bottle with a concave constriction. The effects of the stimuli's weight (half full or completely full of water) and the end-goals (pouring, moving) of the action were also assessed. Analysis of key kinematic landmarks measured during reaching movements indicate that object affordance facilitates the end-goal of the action regardless of accuracy constraints. Furthermore, the placement of individual digits at contact is modulated by the shape of the object and the end-goal of the action. Conclusions/Significance These findings offer a substantial contribution to the current debate about the role played by affordances and end-goals in determining the structure of reach-to-grasp movements.","accessed":{"date-parts":[["2024",9,25]]},"author":[{"family":"Sartori","given":"Luisa"},{"family":"Straulino","given":"Elisa"},{"family":"Castiello","given":"Umberto"}],"citation-key":"sartoriHowObjectsAre2011","container-title":"PLOS ONE","container-title-short":"PLOS ONE","DOI":"10.1371/journal.pone.0025203","ISSN":"1932-6203","issue":"9","issued":{"date-parts":[["2011",9,28]]},"language":"en","page":"e25203","publisher":"Public Library of Science","source":"PLoS Journals","title":"How Objects Are Grasped: The Interplay between Affordances and End-Goals","title-short":"How Objects Are Grasped","type":"article-journal","URL":"https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0025203","volume":"6"},
  {"id":"shamilUtility3DHand","author":[{"family":"Shamil","given":"Salman"},{"family":"Chatterjee","given":"Dibyadip"},{"family":"Sener","given":"Fadime"},{"family":"Ma","given":"Shugao"},{"family":"Yao","given":"Angela"}],"citation-key":"shamilUtility3DHand","language":"en","source":"Zotero","title":"On the Utility of 3D Hand Poses for Action Recognition","type":"article-journal"},
  {"id":"sheLearningCrosshandPolicies","abstract":"Reaching-and-grasping is a fundamental skill for robotic manipulation, but existing methods usually train models on a specific gripper and cannot be reused on another gripper. In this paper, we propose a novel method that can learn a unified policy model that can be easily transferred to different dexterous grippers. Our method consists of two stages: a gripper-agnostic policy model that predicts the displacements of pre-defined key points on the gripper, and a gripper-specific adaptation model that translates these displacements into adjustments for controlling the grippers’ joints. The gripper state and interactions with objects are captured at the finger level using robust geometric representations, integrated with a transformer-based network to address variations in gripper morphology and geometry. In the experiments, we evaluate our method on several dexterous grippers and diverse objects, and the result shows that our method significantly outperforms the baseline methods. Pioneering the transfer of grasp policies across dexterous grippers, our method effectively demonstrates its potential for learning generalizable and transferable manipulation skills for various robotic hands.","author":[{"family":"She","given":"Qijin"},{"family":"Zhang","given":"Shishun"},{"family":"Ye","given":"Yunfan"},{"family":"Hu","given":"Ruizhen"},{"family":"Xu","given":"Kai"}],"citation-key":"sheLearningCrosshandPolicies","language":"en","source":"Zotero","title":"Learning Cross-hand Policies of High-DOF Reaching and Grasping","type":"article-journal"},
  {"id":"spurrWeaklySupervised3D2020","abstract":"Estimating 3D hand pose from 2D images is a difficult, inverse problem due to the inherent scale and depth ambiguities. Current state-of-the-art methods train fully supervised deep neural networks with 3D ground-truth data. However, acquiring 3D annotations is expensive, typically requiring calibrated multi-view setups or labor intensive manual annotations. While annotations of 2D keypoints are much easier to obtain, how to efficiently leverage such weakly-supervised data to improve the task of 3D hand pose prediction remains an important open question. The key difficulty stems from the fact that direct application of additional 2D supervision mostly benefits the 2D proxy objective but does little to alleviate the depth and scale ambiguities. Embracing this challenge we propose a set of novel losses. We show by extensive experiments that our proposed constraints significantly reduce the depth ambiguity and allow the network to more effectively leverage additional 2D annotated images. For example, on the challenging freiHAND dataset using additional 2D annotation without our proposed biomechanical constraints reduces the depth error by only $15\\%$, whereas the error is reduced significantly by $50\\%$ when the proposed biomechanical constraints are used.","accessed":{"date-parts":[["2024",10,15]]},"author":[{"family":"Spurr","given":"Adrian"},{"family":"Iqbal","given":"Umar"},{"family":"Molchanov","given":"Pavlo"},{"family":"Hilliges","given":"Otmar"},{"family":"Kautz","given":"Jan"}],"citation-key":"spurrWeaklySupervised3D2020","DOI":"10.48550/arXiv.2003.09282","issued":{"date-parts":[["2020",8,4]]},"number":"arXiv:2003.09282","publisher":"arXiv","source":"arXiv.org","title":"Weakly Supervised 3D Hand Pose Estimation via Biomechanical Constraints","type":"article","URL":"http://arxiv.org/abs/2003.09282"},
  {"id":"sudhakarControllingWorldSleight","abstract":"Humans naturally build mental models of object interactions and dynamics, allowing them to imagine how their surroundings will change if they take a certain action. While generative models today have shown impressive results on generating/editing images unconditionally or conditioned on text, current methods do not provide the ability to perform object manipulation conditioned on actions, an important tool for world modeling and action planning. Therefore, we propose to learn an action-conditional generative models by learning from unlabeled videos of human hands interacting with objects. The vast quantity of such data on the internet allows for efficient scaling which can enable high-performing action-conditional models. Given an image, and the shape/location of a desired hand interaction, CoSHAND, synthesizes an image of a future after the interaction has occurred. Experiments show that the resulting model can predict the effects of hand-object interactions well, with strong generalization particularly to translation, stretching, and squeezing interactions of unseen objects in unseen environments. Further, CoSHAND can be sampled many times to predict multiple possible effects, modeling the uncertainty of forces in the interaction/environment. Finally, method generalizes to different embodiments, including non-human hands, i.e. robot hands, suggesting that generative video models can be powerful models for robotics.","author":[{"family":"Sudhakar","given":"Sruthi"},{"family":"Liu","given":"Ruoshi"},{"family":"Hoorick","given":"Basile Van"},{"family":"Vondrick","given":"Carl"}],"citation-key":"sudhakarControllingWorldSleight","language":"en","source":"Zotero","title":"Controlling the World by Sleight of Hand","type":"article-journal"},
  {"id":"swamySHOWMeBenchmarkingObjectagnostic2023","abstract":"Recent hand-object interaction datasets show limited real object variability and rely on fitting the MANO parametric model to obtain groundtruth hand shapes. To go beyond these limitations and spur further research, we introduce the SHOWMe dataset which consists of 96 videos, annotated with real and detailed hand-object 3D textured meshes. Following recent work, we consider a rigid hand-object scenario, in which the pose of the hand with respect to the object remains constant during the whole video sequence. This assumption allows us to register sub-millimetre-precise groundtruth 3D scans to the image sequences in SHOWMe. Although simpler, this hypothesis makes sense in terms of applications where the required accuracy and level of detail is important eg., object hand-over in human-robot collaboration, object scanning, or manipulation and contact point analysis. Importantly, the rigidity of the hand-object systems allows to tackle video-based 3D reconstruction of unknown hand-held objects using a 2-stage pipeline consisting of a rigid registration step followed by a multi-view reconstruction (MVR) part. We carefully evaluate a set of non-trivial baselines for these two stages and show that it is possible to achieve promising object-agnostic 3D hand-object reconstructions employing an SfM toolbox or a hand pose estimator to recover the rigid transforms and off-the-shelf MVR algorithms. However, these methods remain sensitive to the initial camera pose estimates which might be imprecise due to lack of textures on the objects or heavy occlusions of the hands, leaving room for improvements in the reconstruction. Code and dataset are available at https://europe.naverlabs.com/research/showme","accessed":{"date-parts":[["2024",10,31]]},"author":[{"family":"Swamy","given":"Anilkumar"},{"family":"Leroy","given":"Vincent"},{"family":"Weinzaepfel","given":"Philippe"},{"family":"Baradel","given":"Fabien"},{"family":"Galaaoui","given":"Salma"},{"family":"Bregier","given":"Romain"},{"family":"Armando","given":"Matthieu"},{"family":"Franco","given":"Jean-Sebastien"},{"family":"Rogez","given":"Gregory"}],"citation-key":"swamySHOWMeBenchmarkingObjectagnostic2023","issued":{"date-parts":[["2023",9,19]]},"number":"arXiv:2309.10748","publisher":"arXiv","source":"arXiv.org","title":"SHOWMe: Benchmarking Object-agnostic Hand-Object 3D Reconstruction","title-short":"SHOWMe","type":"article","URL":"http://arxiv.org/abs/2309.10748"},
  {"id":"taheriGRABDatasetWholeBody2020","abstract":"Training computers to understand, model, and synthesize human grasping requires a rich dataset containing complex 3D object shapes, detailed contact information, hand pose and shape, and the 3D body motion over time. While “grasping” is commonly thought of as a single hand stably lifting an object, we capture the motion of the entire body and adopt the generalized notion of “whole-body grasps”. Thus, we collect a new dataset, called GRAB (GRasping Actions with Bodies), of whole-body grasps, containing full 3D shape and pose sequences of 10 subjects interacting with 51 everyday objects of varying shape and size. Given MoCap markers, we fit the full 3D body shape and pose, including the articulated face and hands, as well as the 3D object pose. This gives detailed 3D meshes over time, from which we compute contact between the body and object. This is a unique dataset, that goes well beyond existing ones for modeling and understanding how humans grasp and manipulate objects, how their full body is involved, and how interaction varies with the task. We illustrate the practical value of GRAB with an example application; we train GrabNet, a conditional generative network, to predict 3D hand grasps for unseen 3D object shapes. The dataset and code are available for research purposes at https://grab.is.tue.mpg.de.","author":[{"family":"Taheri","given":"Omid"},{"family":"Ghorbani","given":"Nima"},{"family":"Black","given":"Michael J."},{"family":"Tzionas","given":"Dimitrios"}],"citation-key":"taheriGRABDatasetWholeBody2020","container-title":"Computer Vision – ECCV 2020","DOI":"10.1007/978-3-030-58548-8_34","editor":[{"family":"Vedaldi","given":"Andrea"},{"family":"Bischof","given":"Horst"},{"family":"Brox","given":"Thomas"},{"family":"Frahm","given":"Jan-Michael"}],"event-place":"Cham","ISBN":"978-3-030-58548-8","issued":{"date-parts":[["2020"]]},"language":"en","page":"581-600","publisher":"Springer International Publishing","publisher-place":"Cham","source":"Springer Link","title":"GRAB: A Dataset of Whole-Body Human Grasping of Objects","title-short":"GRAB","type":"paper-conference"},
  {"id":"tangPromptingFutureDriven","abstract":"Hand motion prediction from both first- and third-person perspectives is vital for enhancing user experience in AR/VR and ensuring safe remote robotic arm control. Previous works typically focus on predicting hand motion trajectories or human body motion, with direct hand motion prediction remaining largely unexplored - despite the additional challenges posed by compact skeleton size. To address this, we propose a prompt-based Future Driven Diffusion Model (PromptFDDM) for predicting hand motion with guidance and prompts. Specifically, we develop a Spatial-Temporal Extractor Network (STEN) to predict hand motion with guidance, a Ground Truth Extractor Network (GTEN), and a Reference Data Generator Network (RDGN), which extract ground truth and substitute future data with generated reference data, respectively, to guide STEN. Additionally, interactive prompts generated from observed motions further enhance model performance. Experimental results on the FPHA and HO3D datasets demonstrate that the proposed PromptFDDM achieves state-of-the-art performance in both first- and third-person perspectives.","author":[{"family":"Tang","given":"Bowen"},{"family":"Zhang","given":"Kaihao"},{"family":"Luo","given":"Wenhan"},{"family":"Liu","given":"Wei"},{"family":"Li","given":"Hongdong"}],"citation-key":"tangPromptingFutureDriven","language":"en","source":"Zotero","title":"Prompting Future Driven Diffusion Model for Hand Motion Prediction","type":"article-journal"},
  {"id":"tevetHumanMotionDiffusion2022","abstract":"Natural and expressive human motion generation is the holy grail of computer animation. It is a challenging task, due to the diversity of possible motion, human perceptual sensitivity to it, and the difficulty of accurately describing it. Therefore, current generative solutions are either low-quality or limited in expressiveness. Diffusion models, which have already shown remarkable generative capabilities in other domains, are promising candidates for human motion due to their many-to-many nature, but they tend to be resource hungry and hard to control. In this paper, we introduce Motion Diffusion Model (MDM), a carefully adapted classifier-free diffusion-based generative model for the human motion domain. MDM is transformer-based, combining insights from motion generation literature. A notable design-choice is the prediction of the sample, rather than the noise, in each diffusion step. This facilitates the use of established geometric losses on the locations and velocities of the motion, such as the foot contact loss. As we demonstrate, MDM is a generic approach, enabling different modes of conditioning, and different generation tasks. We show that our model is trained with lightweight resources and yet achieves state-of-the-art results on leading benchmarks for text-to-motion and action-to-motion. https://guytevet.github.io/mdm-page/ .","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Tevet","given":"Guy"},{"family":"Raab","given":"Sigal"},{"family":"Gordon","given":"Brian"},{"family":"Shafir","given":"Yonatan"},{"family":"Cohen-Or","given":"Daniel"},{"family":"Bermano","given":"Amit H."}],"citation-key":"tevetHumanMotionDiffusion2022","DOI":"10.48550/arXiv.2209.14916","issued":{"date-parts":[["2022",10,3]]},"number":"arXiv:2209.14916","publisher":"arXiv","source":"arXiv.org","title":"Human Motion Diffusion Model","type":"article","URL":"http://arxiv.org/abs/2209.14916"},
  {"id":"tseSpectralGraphormerSpectral2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Tse","given":"Tze Ho Elden"},{"family":"Mueller","given":"Franziska"},{"family":"Shen","given":"Zhengyang"},{"family":"Tang","given":"Danhang"},{"family":"Beeler","given":"Thabo"},{"family":"Dou","given":"Mingsong"},{"family":"Zhang","given":"Yinda"},{"family":"Petrovic","given":"Sasa"},{"family":"Chang","given":"Hyung Jin"},{"family":"Taylor","given":"Jonathan"},{"family":"Doosti","given":"Bardia"}],"citation-key":"tseSpectralGraphormerSpectral2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"14666-14677","source":"openaccess.thecvf.com","title":"Spectral Graphormer: Spectral Graph-Based Transformer for Egocentric Two-Hand Reconstruction using Multi-View Color Images","title-short":"Spectral Graphormer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Tse_Spectral_Graphormer_Spectral_Graph-Based_Transformer_for_Egocentric_Two-Hand_Reconstruction_using_ICCV_2023_paper.html"},
  {"id":"valassakisHandDGPCameraSpaceHand","abstract":"Predicting camera-space hand meshes from single RGB images is crucial for enabling realistic hand interactions in 3D virtual and augmented worlds. Previous work typically divided the task into two stages: given a cropped image of the hand, predict meshes in relative coordinates, followed by lifting these predictions into camera space in a separate and independent stage, often resulting in the loss of valuable contextual and scale information. To prevent the loss of these cues, we propose unifying these two stages into an end-to-end solution that addresses the 2D-3D correspondence problem. This solution enables backpropagation from camera space outputs to the rest of the network through a new differentiable global positioning module. We also introduce an image rectification step that harmonizes both the training dataset and the input image as if they were acquired with the same camera, helping to alleviate the inherent scale-depth ambiguity of the problem. We validate the effectiveness of our framework in evaluations against several baselines and state-of-the-art approaches across three public benchmarks.","author":[{"family":"Valassakis","given":"Eugene"},{"family":"Garcia-Hernando","given":"Guillermo"}],"citation-key":"valassakisHandDGPCameraSpaceHand","language":"en","source":"Zotero","title":"HandDGP: Camera-Space Hand Mesh Prediction with Differentiable Global Positioning","type":"article-journal"},
  {"id":"wangDeepSimHOStablePose2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Wang","given":"Rong"},{"family":"Mao","given":"Wei"},{"family":"Li","given":"Hongdong"}],"citation-key":"wangDeepSimHOStablePose2023","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2023",12,15]]},"language":"en","page":"79685-79697","source":"proceedings.neurips.cc","title":"DeepSimHO: Stable Pose Estimation for Hand-Object Interaction via Physics Simulation","title-short":"DeepSimHO","type":"article-journal","URL":"https://proceedings.neurips.cc/paper_files/paper/2023/hash/fbdaea4878318e214c0577dae4b8bc43-Abstract-Conference.html","volume":"36"},
  {"id":"wangMeMaHandExploitingMeshMano2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Wang","given":"Congyi"},{"family":"Zhu","given":"Feida"},{"family":"Wen","given":"Shilei"}],"citation-key":"wangMeMaHandExploitingMeshMano2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"564-573","source":"openaccess.thecvf.com","title":"MeMaHand: Exploiting Mesh-Mano Interaction for Single Image Two-Hand Reconstruction","title-short":"MeMaHand","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Wang_MeMaHand_Exploiting_Mesh-Mano_Interaction_for_Single_Image_Two-Hand_Reconstruction_CVPR_2023_paper.html"},
  {"id":"wangSurvey3DHuman2024","abstract":"3D modeling has long been an important area in computer vision and computer graphics. Recently, thanks to the breakthroughs in neural representations and generative models, we witnessed a rapid development of 3D modeling. 3D human modeling, lying at the core of many real-world applications, such as gaming and animation, has attracted significant attention. Over the past few years, a large body of work on creating 3D human avatars has been introduced, forming a new and abundant knowledge base for 3D human modeling. The scale of the literature makes it difficult for individuals to keep track of all the works. This survey aims to provide a comprehensive overview of these emerging techniques for 3D human avatar modeling, from both reconstruction and generation perspectives. Firstly, we review representative methods for 3D human reconstruction, including methods based on pixel-aligned implicit function, neural radiance field, and 3D Gaussian Splatting, etc. We then summarize representative methods for 3D human generation, especially those using large language models like CLIP, diffusion models, and various 3D representations, which demonstrate state-of-the-art performance. Finally, we discuss our reflection on existing methods and open challenges for 3D human avatar modeling, shedding light on future research.","accessed":{"date-parts":[["2024",11,4]]},"author":[{"family":"Wang","given":"Ruihe"},{"family":"Cao","given":"Yukang"},{"family":"Han","given":"Kai"},{"family":"Wong","given":"Kwan-Yee K."}],"citation-key":"wangSurvey3DHuman2024","issued":{"date-parts":[["2024",6,6]]},"number":"arXiv:2406.04253","publisher":"arXiv","source":"arXiv.org","title":"A Survey on 3D Human Avatar Modeling -- From Reconstruction to Generation","type":"article","URL":"http://arxiv.org/abs/2406.04253"},
  {"id":"wangSurveyDeepLearningbased2022","abstract":"Hand pose estimation is one of the representative tasks in computer vision. Solving the hand pose estimation problem is essential for various fields such as virtual reality, augmented reality, mixed reality, and human-computer interaction. Due to the significant development of deep learning techniques, the hand pose estimation task has reached significant performance on many hand pose estimation datasets. However, the hand pose estimation task still faces many challenges due to the lack of large-scale labeled data, severe occlusion, low hand resolution, and background clutter. To better understand the hand pose estimation task, this paper presents a comprehensive survey of outstanding papers over the last five years. The paper first introduces 19 common hand pose estimation datasets, then extensively discusses some of the mainstream approaches in hand pose estimation, including fully supervised, semi-supervised, weakly supervised, and self-supervised learning methods. Finally, we extensively discuss the future evolution trends of hand pose estimation.","accessed":{"date-parts":[["2024",8,16]]},"author":[{"family":"Wang","given":"Shuaibing"},{"family":"Wang","given":"Shunli"},{"family":"Kuang","given":"HaoPeng"},{"family":"Li","given":"Fang"},{"family":"Qian","given":"Ziyun"},{"family":"Li","given":"Mingcheng"},{"family":"Zhang","given":"Lihua"}],"citation-key":"wangSurveyDeepLearningbased2022","container-title":"2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS)","DOI":"10.1109/CCIS57298.2022.10016310","event-title":"2022 IEEE 8th International Conference on Cloud Computing and Intelligent Systems (CCIS)","issued":{"date-parts":[["2022",11]]},"page":"331-340","source":"IEEE Xplore","title":"A Survey of Deep Learning-based Hand Pose Estimation","type":"paper-conference","URL":"https://ieeexplore.ieee.org/document/10016310/?arnumber=10016310"},
  {"id":"wenHierarchicalTemporalTransformer2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Wen","given":"Yilin"},{"family":"Pan","given":"Hao"},{"family":"Yang","given":"Lei"},{"family":"Pan","given":"Jia"},{"family":"Komura","given":"Taku"},{"family":"Wang","given":"Wenping"}],"citation-key":"wenHierarchicalTemporalTransformer2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"21243-21253","source":"openaccess.thecvf.com","title":"Hierarchical Temporal Transformer for 3D Hand Pose Estimation and Action Recognition From Egocentric RGB Videos","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Wen_Hierarchical_Temporal_Transformer_for_3D_Hand_Pose_Estimation_and_Action_CVPR_2023_paper.html"},
  {"id":"wiederholdHOHMarkerlessMultimodal2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Wiederhold","given":"Noah"},{"family":"Megyeri","given":"Ava"},{"family":"Paris","given":"DiMaggio"},{"family":"Banerjee","given":"Sean"},{"family":"Banerjee","given":"Natasha"}],"citation-key":"wiederholdHOHMarkerlessMultimodal2023","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2023",12,15]]},"language":"en","page":"68736-68748","source":"proceedings.neurips.cc","title":"HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count","title-short":"HOH","type":"article-journal","URL":"https://proceedings.neurips.cc/paper_files/paper/2023/hash/d8c6a37c4c94e9a63e53d296f1f668ae-Abstract-Datasets_and_Benchmarks.html","volume":"36"},
  {"id":"wilesSynSinEndEndView2020","accessed":{"date-parts":[["2024",10,7]]},"author":[{"family":"Wiles","given":"Olivia"},{"family":"Gkioxari","given":"Georgia"},{"family":"Szeliski","given":"Richard"},{"family":"Johnson","given":"Justin"}],"citation-key":"wilesSynSinEndEndView2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"7467-7477","source":"openaccess.thecvf.com","title":"SynSin: End-to-End View Synthesis From a Single Image","title-short":"SynSin","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Wiles_SynSin_End-to-End_View_Synthesis_From_a_Single_Image_CVPR_2020_paper.html"},
  {"id":"xieMSMANOEnablingHand2024","abstract":"This work proposes a novel learning framework for visual hand dynamics analysis that takes into account the physiological aspects of hand motion. The existing models, which are simplified joint-actuated systems, often produce unnatural motions. To address this, we integrate a musculoskeletal system with a learnable parametric hand model, MANO, to create a new model, MS-MANO. This model emulates the dynamics of muscles and tendons to drive the skeletal system, imposing physiologically realistic constraints on the resulting torque trajectories. We further propose a simulation-in-the-loop pose refinement framework, BioPR, that refines the initial estimated pose through a multi-layer perceptron (MLP) network. Our evaluation of the accuracy of MS-MANO and the efficacy of the BioPR is conducted in two separate parts. The accuracy of MS-MANO is compared with MyoSuite, while the efficacy of BioPR is benchmarked against two large-scale public datasets and two recent state-of-the-art methods. The results demonstrate that our approach consistently improves the baseline methods both quantitatively and qualitatively.","accessed":{"date-parts":[["2024",10,10]]},"author":[{"family":"Xie","given":"Pengfei"},{"family":"Xu","given":"Wenqiang"},{"family":"Tang","given":"Tutian"},{"family":"Yu","given":"Zhenjun"},{"family":"Lu","given":"Cewu"}],"citation-key":"xieMSMANOEnablingHand2024","DOI":"10.48550/arXiv.2404.10227","issued":{"date-parts":[["2024",4,16]]},"number":"arXiv:2404.10227","publisher":"arXiv","source":"arXiv.org","title":"MS-MANO: Enabling Hand Pose Tracking with Biomechanical Constraints","title-short":"MS-MANO","type":"article","URL":"http://arxiv.org/abs/2404.10227"},
  {"id":"xieNonrigidObjectContact2023","accessed":{"date-parts":[["2024",9,27]]},"author":[{"family":"Xie","given":"Wei"},{"family":"Zhao","given":"Zimeng"},{"family":"Li","given":"Shiying"},{"family":"Zuo","given":"Binghui"},{"family":"Wang","given":"Yangang"}],"citation-key":"xieNonrigidObjectContact2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"9342-9351","source":"openaccess.thecvf.com","title":"Nonrigid Object Contact Estimation With Regional Unwrapping Transformer","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Xie_Nonrigid_Object_Contact_Estimation_With_Regional_Unwrapping_Transformer_ICCV_2023_paper.html"},
  {"id":"xuEgoPCANewFramework2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Xu","given":"Yue"},{"family":"Li","given":"Yong-Lu"},{"family":"Huang","given":"Zhemin"},{"family":"Liu","given":"Michael Xu"},{"family":"Lu","given":"Cewu"},{"family":"Tai","given":"Yu-Wing"},{"family":"Tang","given":"Chi-Keung"}],"citation-key":"xuEgoPCANewFramework2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"5273-5284","source":"openaccess.thecvf.com","title":"EgoPCA: A New Framework for Egocentric Hand-Object Interaction Understanding","title-short":"EgoPCA","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Xu_EgoPCA_A_New_Framework_for_Egocentric_Hand-Object_Interaction_Understanding_ICCV_2023_paper.html"},
  {"id":"xuH2ONetHandOcclusionOrientationAwareNetwork2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Xu","given":"Hao"},{"family":"Wang","given":"Tianyu"},{"family":"Tang","given":"Xiao"},{"family":"Fu","given":"Chi-Wing"}],"citation-key":"xuH2ONetHandOcclusionOrientationAwareNetwork2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"17048-17058","source":"openaccess.thecvf.com","title":"H2ONet: Hand-Occlusion-and-Orientation-Aware Network for Real-Time 3D Hand Mesh Reconstruction","title-short":"H2ONet","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Xu_H2ONet_Hand-Occlusion-and-Orientation-Aware_Network_for_Real-Time_3D_Hand_Mesh_Reconstruction_CVPR_2023_paper.html"},
  {"id":"xuHandBoosterBoosting3D2024","abstract":"Reconstructing 3D hand mesh robustly from a single image is very challenging, due to the lack of diversity in existing real-world datasets. While data synthesis helps relieve the issue, the syn-to-real gap still hinders its usage. In this work, we present HandBooster, a new approach to uplift the data diversity and boost the 3D hand-mesh reconstruction performance by training a conditional generative space on hand-object interactions and purposely sampling the space to synthesize effective data samples. First, we construct versatile content-aware conditions to guide a diffusion model to produce realistic images with diverse hand appearances, poses, views, and backgrounds; favorably, accurate 3D annotations are obtained for free. Then, we design a novel condition creator based on our similarity-aware distribution sampling strategies to deliberately find novel and realistic interaction poses that are distinctive from the training set. Equipped with our method, several baselines can be significantly improved beyond the SOTA on the HO3D and DexYCB benchmarks. Our code will be released on https://github.com/hxwork/HandBooster_Pytorch.","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Xu","given":"Hao"},{"family":"Li","given":"Haipeng"},{"family":"Wang","given":"Yinqiao"},{"family":"Liu","given":"Shuaicheng"},{"family":"Fu","given":"Chi-Wing"}],"citation-key":"xuHandBoosterBoosting3D2024","DOI":"10.48550/arXiv.2403.18575","issued":{"date-parts":[["2024",3,27]]},"number":"arXiv:2403.18575","publisher":"arXiv","source":"arXiv.org","title":"HandBooster: Boosting 3D Hand-Mesh Reconstruction by Conditional Synthesis and Sampling of Hand-Object Interactions","title-short":"HandBooster","type":"article","URL":"http://arxiv.org/abs/2403.18575"},
  {"id":"xuVisualTactileSensingHand2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Xu","given":"Wenqiang"},{"family":"Yu","given":"Zhenjun"},{"family":"Xue","given":"Han"},{"family":"Ye","given":"Ruolin"},{"family":"Yao","given":"Siqiong"},{"family":"Lu","given":"Cewu"}],"citation-key":"xuVisualTactileSensingHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"8803-8812","source":"openaccess.thecvf.com","title":"Visual-Tactile Sensing for In-Hand Object Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Xu_Visual-Tactile_Sensing_for_In-Hand_Object_Reconstruction_CVPR_2023_paper.html"},
  {"id":"yangCPFLearningContact2021","accessed":{"date-parts":[["2024",9,27]]},"author":[{"family":"Yang","given":"Lixin"},{"family":"Zhan","given":"Xinyu"},{"family":"Li","given":"Kailin"},{"family":"Xu","given":"Wenqiang"},{"family":"Li","given":"Jiefeng"},{"family":"Lu","given":"Cewu"}],"citation-key":"yangCPFLearningContact2021","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2021"]]},"language":"en","page":"11097-11106","source":"openaccess.thecvf.com","title":"CPF: Learning a Contact Potential Field To Model the Hand-Object Interaction","title-short":"CPF","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2021/html/Yang_CPF_Learning_a_Contact_Potential_Field_To_Model_the_Hand-Object_ICCV_2021_paper.html"},
  {"id":"yangMLPHandRealTime","author":[{"family":"Yang","given":"Jian"},{"family":"Li","given":"Jiakun"},{"family":"Li","given":"Guoming"},{"family":"Wu","given":"Huai-Yu"},{"family":"Shen","given":"Zhen"},{"family":"Fan","given":"Zhaoxin"}],"citation-key":"yangMLPHandRealTime","language":"en","source":"Zotero","title":"MLPHand: Real Time Multi-View 3D Hand Reconstruction via MLP Modeling","type":"article-journal"},
  {"id":"yangPOEMReconstructingHand2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Yang","given":"Lixin"},{"family":"Xu","given":"Jian"},{"family":"Zhong","given":"Licheng"},{"family":"Zhan","given":"Xinyu"},{"family":"Wang","given":"Zhicheng"},{"family":"Wu","given":"Kejian"},{"family":"Lu","given":"Cewu"}],"citation-key":"yangPOEMReconstructingHand2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"21108-21117","source":"openaccess.thecvf.com","title":"POEM: Reconstructing Hand in a Point Embedded Multi-View Stereo","title-short":"POEM","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Yang_POEM_Reconstructing_Hand_in_a_Point_Embedded_Multi-View_Stereo_CVPR_2023_paper.html"},
  {"id":"yeAffordanceDiffusionSynthesizing2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Ye","given":"Yufei"},{"family":"Li","given":"Xueting"},{"family":"Gupta","given":"Abhinav"},{"family":"De Mello","given":"Shalini"},{"family":"Birchfield","given":"Stan"},{"family":"Song","given":"Jiaming"},{"family":"Tulsiani","given":"Shubham"},{"family":"Liu","given":"Sifei"}],"citation-key":"yeAffordanceDiffusionSynthesizing2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"22479-22489","source":"openaccess.thecvf.com","title":"Affordance Diffusion: Synthesizing Hand-Object Interactions","title-short":"Affordance Diffusion","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Ye_Affordance_Diffusion_Synthesizing_Hand-Object_Interactions_CVPR_2023_paper.html"},
  {"id":"yeDiffusionGuidedReconstructionEveryday2023","abstract":"We tackle the task of reconstructing hand-object interactions from short video clips. Given an input video, our approach casts 3D inference as a per-video optimization and recovers a neural 3D representation of the object shape, as well as the time-varying motion and hand articulation. While the input video naturally provides some multi-view cues to guide 3D inference, these are insufficient on their own due to occlusions and limited viewpoint variations. To obtain accurate 3D, we augment the multi-view signals with generic data-driven priors to guide reconstruction. Specifically, we learn a diffusion network to model the conditional distribution of (geometric) renderings of objects conditioned on hand configuration and category label, and leverage it as a prior to guide the novel-view renderings of the reconstructed scene. We empirically evaluate our approach on egocentric videos across 6 object categories, and observe significant improvements over prior single-view and multi-view methods. Finally, we demonstrate our system's ability to reconstruct arbitrary clips from YouTube, showing both 1st and 3rd person interactions.","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Ye","given":"Yufei"},{"family":"Hebbar","given":"Poorvi"},{"family":"Gupta","given":"Abhinav"},{"family":"Tulsiani","given":"Shubham"}],"citation-key":"yeDiffusionGuidedReconstructionEveryday2023","DOI":"10.48550/arXiv.2309.05663","issued":{"date-parts":[["2023",9,11]]},"number":"arXiv:2309.05663","publisher":"arXiv","source":"arXiv.org","title":"Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips","type":"article","URL":"http://arxiv.org/abs/2309.05663"},
  {"id":"yeDiffusionGuidedReconstructionEveryday2023a","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Ye","given":"Yufei"},{"family":"Hebbar","given":"Poorvi"},{"family":"Gupta","given":"Abhinav"},{"family":"Tulsiani","given":"Shubham"}],"citation-key":"yeDiffusionGuidedReconstructionEveryday2023a","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"19717-19728","source":"openaccess.thecvf.com","title":"Diffusion-Guided Reconstruction of Everyday Hand-Object Interaction Clips","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Ye_Diffusion-Guided_Reconstruction_of_Everyday_Hand-Object_Interaction_Clips_ICCV_2023_paper.html"},
  {"id":"yeGHOPGenerativeHandObject2024a","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Ye","given":"Yufei"},{"family":"Gupta","given":"Abhinav"},{"family":"Kitani","given":"Kris"},{"family":"Tulsiani","given":"Shubham"}],"citation-key":"yeGHOPGenerativeHandObject2024a","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"1911-1920","source":"openaccess.thecvf.com","title":"G-HOP: Generative Hand-Object Prior for Interaction Reconstruction and Grasp Synthesis","title-short":"G-HOP","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Ye_G-HOP_Generative_Hand-Object_Prior_for_Interaction_Reconstruction_and_Grasp_Synthesis_CVPR_2024_paper.html"},
  {"id":"yuACRAttentionCollaborationBased2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Yu","given":"Zhengdi"},{"family":"Huang","given":"Shaoli"},{"family":"Fang","given":"Chen"},{"family":"Breckon","given":"Toby P."},{"family":"Wang","given":"Jue"}],"citation-key":"yuACRAttentionCollaborationBased2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12955-12964","source":"openaccess.thecvf.com","title":"ACR: Attention Collaboration-Based Regressor for Arbitrary Two-Hand Reconstruction","title-short":"ACR","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Yu_ACR_Attention_Collaboration-Based_Regressor_for_Arbitrary_Two-Hand_Reconstruction_CVPR_2023_paper.html"},
  {"id":"yuOvercomingTradeAccuracyPlausibility2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Yu","given":"Ziwei"},{"family":"Li","given":"Chen"},{"family":"Yang","given":"Linlin"},{"family":"Zheng","given":"Xiaoxu"},{"family":"Mi","given":"Michael Bi"},{"family":"Lee","given":"Gim Hee"},{"family":"Yao","given":"Angela"}],"citation-key":"yuOvercomingTradeAccuracyPlausibility2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"544-553","source":"openaccess.thecvf.com","title":"Overcoming the Trade-Off Between Accuracy and Plausibility in 3D Hand Shape Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Yu_Overcoming_the_Trade-Off_Between_Accuracy_and_Plausibility_in_3D_Hand_CVPR_2023_paper.html"},
  {"id":"zhangBOTH2HandsInferring3D2024","accessed":{"date-parts":[["2024",10,25]]},"author":[{"family":"Zhang","given":"Wenqian"},{"family":"Huang","given":"Molin"},{"family":"Zhou","given":"Yuxuan"},{"family":"Zhang","given":"Juze"},{"family":"Yu","given":"Jingyi"},{"family":"Wang","given":"Jingya"},{"family":"Xu","given":"Lan"}],"citation-key":"zhangBOTH2HandsInferring3D2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"2393-2404","source":"openaccess.thecvf.com","title":"BOTH2Hands: Inferring 3D Hands from Both Text Prompts and Body Dynamics","title-short":"BOTH2Hands","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_BOTH2Hands_Inferring_3D_Hands_from_Both_Text_Prompts_and_Body_CVPR_2024_paper.html"},
  {"id":"zhangDDFHOHandHeldObject2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Zhang","given":"Chenyangguang"},{"family":"Di","given":"Yan"},{"family":"Zhang","given":"Ruida"},{"family":"Zhai","given":"Guangyao"},{"family":"Manhardt","given":"Fabian"},{"family":"Tombari","given":"Federico"},{"family":"Ji","given":"Xiangyang"}],"citation-key":"zhangDDFHOHandHeldObject2023","container-title":"Advances in Neural Information Processing Systems","issued":{"date-parts":[["2023",12,15]]},"language":"en","page":"56871-56884","source":"proceedings.neurips.cc","title":"DDF-HO: Hand-Held Object Reconstruction via Conditional Directed Distance Field","title-short":"DDF-HO","type":"article-journal","URL":"https://proceedings.neurips.cc/paper_files/paper/2023/hash/b2876deb92cbd098219a10da25671577-Abstract-Conference.html","volume":"36"},
  {"id":"zhangHOIDiffusionGeneratingRealistic2024","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Zhang","given":"Mengqi"},{"family":"Fu","given":"Yang"},{"family":"Ding","given":"Zheng"},{"family":"Liu","given":"Sifei"},{"family":"Tu","given":"Zhuowen"},{"family":"Wang","given":"Xiaolong"}],"citation-key":"zhangHOIDiffusionGeneratingRealistic2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"8521-8531","source":"openaccess.thecvf.com","title":"HOIDiffusion: Generating Realistic 3D Hand-Object Interaction Data","title-short":"HOIDiffusion","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Zhang_HOIDiffusion_Generating_Realistic_3D_Hand-Object_Interaction_Data_CVPR_2024_paper.html"},
  {"id":"zhangNL2ContactNaturalLanguage","abstract":"Modeling the physical contacts between the hand and object is standard for refining inaccurate hand poses and generating novel human grasp in 3D hand-object reconstruction. However, existing methods rely on geometric constraints that cannot be specified or controlled. This paper introduces a novel task of controllable 3D hand-object contact modeling with natural language descriptions. Challenges include i) the complexity of cross-modal modeling from language to contact, and ii) a lack of descriptive text for contact patterns. To address these issues, we propose NL2Contact, a model that generates controllable contacts by leveraging staged diffusion models. Given a language description of the hand and contact, NL2Contact generates realistic and faithful 3D hand-object contacts. To train the model, we build ContactDescribe, the first dataset with hand-centered contact descriptions. It contains multilevel and diverse descriptions generated by large language models based on carefully designed prompts (e.g., grasp action, grasp type, contact location, free finger status). We show applications of our model to grasp pose optimization and novel human grasp generation, both based on a textual contact description.","author":[{"family":"Zhang","given":"Zhongqun"},{"family":"Wang","given":"Hengfei"},{"family":"Yu","given":"Ziwei"},{"family":"Cheng","given":"Yihua"},{"family":"Yao","given":"Angela"},{"family":"Chang","given":"Hyung Jin"}],"citation-key":"zhangNL2ContactNaturalLanguage","language":"en","source":"Zotero","title":"NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model","type":"article-journal"},
  {"id":"zhangNL2ContactNaturalLanguage2024","abstract":"Modeling the physical contacts between the hand and object is standard for refining inaccurate hand poses and generating novel human grasp in 3D hand-object reconstruction. However, existing methods rely on geometric constraints that cannot be specified or controlled. This paper introduces a novel task of controllable 3D hand-object contact modeling with natural language descriptions. Challenges include i) the complexity of cross-modal modeling from language to contact, and ii) a lack of descriptive text for contact patterns. To address these issues, we propose NL2Contact, a model that generates controllable contacts by leveraging staged diffusion models. Given a language description of the hand and contact, NL2Contact generates realistic and faithful 3D hand-object contacts. To train the model, we build ContactDescribe, the first dataset with hand-centered contact descriptions. It contains multilevel and diverse descriptions generated by large language models based on carefully designed prompts (e.g., grasp action, grasp type, contact location, free finger status). We show applications of our model to grasp pose optimization and novel human grasp generation, both based on a textual contact description.","accessed":{"date-parts":[["2024",7,23]]},"author":[{"family":"Zhang","given":"Zhongqun"},{"family":"Wang","given":"Hengfei"},{"family":"Yu","given":"Ziwei"},{"family":"Cheng","given":"Yihua"},{"family":"Yao","given":"Angela"},{"family":"Chang","given":"Hyung Jin"}],"citation-key":"zhangNL2ContactNaturalLanguage2024","issued":{"date-parts":[["2024",7,17]]},"language":"en","number":"arXiv:2407.12727","publisher":"arXiv","source":"arXiv.org","title":"NL2Contact: Natural Language Guided 3D Hand-Object Contact Modeling with Diffusion Model","title-short":"NL2Contact","type":"article","URL":"http://arxiv.org/abs/2407.12727"},
  {"id":"zhangOCHIDFiOcclusionRobustHand2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Zhang","given":"Shujie"},{"family":"Zheng","given":"Tianyue"},{"family":"Chen","given":"Zhe"},{"family":"Hu","given":"Jingzhi"},{"family":"Khamis","given":"Abdelwahed"},{"family":"Liu","given":"Jiajun"},{"family":"Luo","given":"Jun"}],"citation-key":"zhangOCHIDFiOcclusionRobustHand2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"15112-15121","source":"openaccess.thecvf.com","title":"OCHID-Fi: Occlusion-Robust Hand Pose Estimation in 3D via RF-Vision","title-short":"OCHID-Fi","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Zhang_OCHID-Fi_Occlusion-Robust_Hand_Pose_Estimation_in_3D_via_RF-Vision_ICCV_2023_paper.html"},
  {"id":"zhangWeaklySupervised3DHand2024","abstract":"Fully-supervised monocular 3D hand reconstruction is often difficult because capturing the requisite 3D data entails deploying specialized equipment in a controlled environment. We introduce a weakly-supervised method that avoids such requirements by leveraging fundamental principles well-established in the understanding of the human hand's unique structure and functionality. Specifically, we systematically study hand knowledge from different sources, including biomechanics, functional anatomy, and physics. We effectively incorporate these valuable foundational insights into 3D hand reconstruction models through an appropriate set of differentiable training losses. This enables training solely with readily-obtainable 2D hand landmark annotations and eliminates the need for expensive 3D supervision. Moreover, we explicitly model the uncertainty that is inherent in image observations. We enhance the training process by exploiting a simple yet effective Negative Log Likelihood (NLL) loss that incorporates uncertainty into the loss function. Through extensive experiments, we demonstrate that our method significantly outperforms state-of-the-art weakly-supervised methods. For example, our method achieves nearly a 21\\% performance improvement on the widely adopted FreiHAND dataset.","accessed":{"date-parts":[["2024",10,16]]},"author":[{"family":"Zhang","given":"Yufei"},{"family":"Kephart","given":"Jeffrey O."},{"family":"Ji","given":"Qiang"}],"citation-key":"zhangWeaklySupervised3DHand2024","issued":{"date-parts":[["2024",7,17]]},"number":"arXiv:2407.12307","publisher":"arXiv","source":"arXiv.org","title":"Weakly-Supervised 3D Hand Reconstruction with Knowledge Prior and Uncertainty Guidance","type":"article","URL":"http://arxiv.org/abs/2407.12307"},
  {"id":"zhaoSemiSupervisedHandAppearance2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Zhao","given":"Zimeng"},{"family":"Zuo","given":"Binghui"},{"family":"Long","given":"Zhiyu"},{"family":"Wang","given":"Yangang"}],"citation-key":"zhaoSemiSupervisedHandAppearance2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"12125-12136","source":"openaccess.thecvf.com","title":"Semi-Supervised Hand Appearance Recovery via Structure Disentanglement and Dual Adversarial Discrimination","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Zhao_Semi-Supervised_Hand_Appearance_Recovery_via_Structure_Disentanglement_and_Dual_Adversarial_CVPR_2023_paper.html"},
  {"id":"zhengCAMSCAnonicalizedManipulation2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Zheng","given":"Juntian"},{"family":"Zheng","given":"Qingyuan"},{"family":"Fang","given":"Lixing"},{"family":"Liu","given":"Yun"},{"family":"Yi","given":"Li"}],"citation-key":"zhengCAMSCAnonicalizedManipulation2023","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2023"]]},"language":"en","page":"585-594","source":"openaccess.thecvf.com","title":"CAMS: CAnonicalized Manipulation Spaces for Category-Level Functional Hand-Object Manipulation Synthesis","title-short":"CAMS","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2023/html/Zheng_CAMS_CAnonicalized_Manipulation_Spaces_for_Category-Level_Functional_Hand-Object_Manipulation_Synthesis_CVPR_2023_paper.html"},
  {"id":"zhengHaMuCoHandPose2023","accessed":{"date-parts":[["2024",10,28]]},"author":[{"family":"Zheng","given":"Xiaozheng"},{"family":"Wen","given":"Chao"},{"family":"Xue","given":"Zhou"},{"family":"Ren","given":"Pengfei"},{"family":"Wang","given":"Jingyu"}],"citation-key":"zhengHaMuCoHandPose2023","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2023"]]},"language":"en","page":"20763-20773","source":"openaccess.thecvf.com","title":"HaMuCo: Hand Pose Estimation via Multiview Collaborative Self-Supervised Learning","title-short":"HaMuCo","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/ICCV2023/html/Zheng_HaMuCo_Hand_Pose_Estimation_via_Multiview_Collaborative_Self-Supervised_Learning_ICCV_2023_paper.html"},
  {"id":"zhouGEARSLocalGeometryaware2024","accessed":{"date-parts":[["2024",9,26]]},"author":[{"family":"Zhou","given":"Keyang"},{"family":"Bhatnagar","given":"Bharat Lal"},{"family":"Lenssen","given":"Jan Eric"},{"family":"Pons-Moll","given":"Gerard"}],"citation-key":"zhouGEARSLocalGeometryaware2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"20634-20643","source":"openaccess.thecvf.com","title":"GEARS: Local Geometry-aware Hand-object Interaction Synthesis","title-short":"GEARS","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_GEARS_Local_Geometry-aware_Hand-object_Interaction_Synthesis_CVPR_2024_paper.html"},
  {"id":"zhouMonocularRealTimeHand2020","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Zhou","given":"Yuxiao"},{"family":"Habermann","given":"Marc"},{"family":"Xu","given":"Weipeng"},{"family":"Habibie","given":"Ikhsanul"},{"family":"Theobalt","given":"Christian"},{"family":"Xu","given":"Feng"}],"citation-key":"zhouMonocularRealTimeHand2020","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2020"]]},"page":"5346-5355","source":"openaccess.thecvf.com","title":"Monocular Real-Time Hand Shape and Motion Capture Using Multi-Modal Data","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_CVPR_2020/html/Zhou_Monocular_Real-Time_Hand_Shape_and_Motion_Capture_Using_Multi-Modal_Data_CVPR_2020_paper.html"},
  {"id":"zhouSimpleBaselineEfficient2024","accessed":{"date-parts":[["2024",9,29]]},"author":[{"family":"Zhou","given":"Zhishan"},{"family":"Zhou","given":"Shihao"},{"family":"Lv","given":"Zhi"},{"family":"Zou","given":"Minqiang"},{"family":"Tang","given":"Yao"},{"family":"Liang","given":"Jiajun"}],"citation-key":"zhouSimpleBaselineEfficient2024","event-title":"Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition","issued":{"date-parts":[["2024"]]},"language":"en","page":"1367-1376","source":"openaccess.thecvf.com","title":"A Simple Baseline for Efficient Hand Mesh Reconstruction","type":"paper-conference","URL":"https://openaccess.thecvf.com/content/CVPR2024/html/Zhou_A_Simple_Baseline_for_Efficient_Hand_Mesh_Reconstruction_CVPR_2024_paper.html"},
  {"id":"zimmermannFreiHANDDatasetMarkerless2019","accessed":{"date-parts":[["2024",9,30]]},"author":[{"family":"Zimmermann","given":"Christian"},{"family":"Ceylan","given":"Duygu"},{"family":"Yang","given":"Jimei"},{"family":"Russell","given":"Bryan"},{"family":"Argus","given":"Max"},{"family":"Brox","given":"Thomas"}],"citation-key":"zimmermannFreiHANDDatasetMarkerless2019","event-title":"Proceedings of the IEEE/CVF International Conference on Computer Vision","issued":{"date-parts":[["2019"]]},"page":"813-822","source":"openaccess.thecvf.com","title":"FreiHAND: A Dataset for Markerless Capture of Hand Pose and Shape From Single RGB Images","title-short":"FreiHAND","type":"paper-conference","URL":"https://openaccess.thecvf.com/content_ICCV_2019/html/Zimmermann_FreiHAND_A_Dataset_for_Markerless_Capture_of_Hand_Pose_and_ICCV_2019_paper.html"}
]
